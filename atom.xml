<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>老司机种菜</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wodekouwei.com/"/>
  <updated>2017-07-03T15:02:03.000Z</updated>
  <id>http://wodekouwei.com/</id>
  
  <author>
    <name>轻口味</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Android应用性能优化</title>
    <link href="http://wodekouwei.com/2017/07/03/tips-android-performance/"/>
    <id>http://wodekouwei.com/2017/07/03/tips-android-performance/</id>
    <published>2017-07-03T14:53:59.000Z</published>
    <updated>2017-07-03T15:02:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>Android手机由于其本身的后台机制和硬件特点，性能上一直被诟病，所以软件开发者对软件本身的性能优化就显得尤为重要；本文将对Android开发过程中性能优化的各个方面做一个回顾与总结。</p>
<h3 id="Cache优化"><a href="#Cache优化" class="headerlink" title="Cache优化"></a>Cache优化</h3><ul>
<li><p>ListView缓存：</p>
<ul>
<li>ListView中有一个回收器，Item滑出界面的时候View会回收到这里，需要显示新的Item的时候，就尽量重用回收器里面的View；每次在getView函数中inflate新的item之前会先判断回收器中是否有缓存的view，即判断convertView是否为null，是则inflate一个新的item View，否则重用回收器中的item。</li>
<li>此外，ListView还使用静态的ViewHolder减少findViewById的次数</li>
<li>ListView中有getViewTypeCount()函数用于获取列表有几种布局类型，getItemViewType(int position)用于获取在position位置上的布局类型; 我们可以利用ViewType来给不同类型的item创建不同的View，这样可以利于ListView的回收</li>
<li>对Item中图片进行适当压缩, 并进行异步加载；如果快速滑动，不加载图片；实现数据的分页加载</li>
</ul>
</li>
<li><p>IO缓存：在文件和网络IO中使用具有缓存策略的输入流，BufferedInputStream替代InputStream，BufferedReader替代Reader，BufferedReader替代BufferedInputStream</p>
</li>
<li><p>data缓存(空间换时间)：①缓存数据库的查询结果，比如缓存数据库表中的数据条数，这样就可以避免多次的select count查询 ②缓存磁盘文件中需要频繁访问的数据到内存中 ③缓存耗时计算的结果</p>
</li>
</ul>
<h3 id="Battery优化"><a href="#Battery优化" class="headerlink" title="Battery优化"></a>Battery优化</h3><ul>
<li>cpu的使用率和使用频率将直接或间接的影响电量的分配和使用，cpu降频可以节约电量</li>
<li><p>service优化</p>
<ul>
<li><p>service作为一个运行在主线程中的后台服务，应该尽量避免耗时动作，而应该尽量新开线程去处理耗时动作
监听系统广播看service是否存活，否则kill掉；降低service优先级使得系统内存吃紧时会被自动kill掉</p>
</li>
<li><p>使用Android提供的IntentService代替service，因为IntentService会在运行完成之后自动停止，而service需要手动调用stopService()才能停止运行</p>
</li>
<li><p>定时执行任务的Alarm机制：Android的定时任务有两种实现方式，Timer类和Alarm机制；Timer不适合长期后台运行的定时任务。因为每种手机都会有自己的休眠策略，Android手机就会在长时间不操作的情况下自动让CPU进入到睡眠状态，这就有可能导致Timer中的定时任务无法正常运行。而Alarm机制则不存在这种情况，它具有唤醒CPU的功能，即可以保证每次需要执行定时任务的时候CPU能正常工作。然而从Android4.4之后，Alarm任务的触发时间将会变得不准确，有可能会延迟一段时间后任务才能得到执行。这不是bug，而是系统在耗电性方面进行的优化。系统会自动检测目前有多少Alarm任务存在，然后将触发时间将近的几个任务放在一起执行，这就可以大幅度的减少CPU被唤醒的次数，从而有效延长电池的使用时间</p>
</li>
</ul>
</li>
</ul>
<h3 id="渲染层优化"><a href="#渲染层优化" class="headerlink" title="渲染层优化"></a>渲染层优化</h3><ul>
<li><p>Android 界面卡顿的原因？</p>
<ul>
<li>UI线程中做耗时操作，比如进行网络请求,磁盘读取，位图修改，更新UI等耗时操作，从而导致UI线程卡顿</li>
<li>布局Layout过于复杂，无法在16ms内完成渲染，或者嵌套层次过深</li>
<li>View过度绘制或者频繁的触发measure、layout，同一时间动画执行的次数过多，导致CPU或GPU负载过重</li>
<li>冗余资源及逻辑等导致加载和执行缓慢</li>
</ul>
</li>
<li>Android 界面卡顿怎么处理？<ul>
<li>xml布局优化：尽量使用include、merge、ViewStub标签，尽量不存在冗余嵌套及过于复杂布局（譬如10层就会直接异常），例如使用RelativeLayout代替LinearLayout可以减少布局层次和复杂性，View的嵌套层次不能过深，尽量使用GONE替换INVISIBLE，使用weight后尽量将width和heigh设置为0dp，减少运算，Item存在非常复杂的嵌套时考虑使用自定义Item View来取代，减少measure与layout次数等。</li>
<li>ListView及Adapter优化；尽量复用getView方法中的相关View，不重复获取实例导致卡顿，列表尽量在滑动过程中不进行UI元素刷新等。</li>
<li>背景和图片等内存分配优化；尽量减少不必要的背景设置，图片尽量压缩处理显示，尽量避免频繁内存抖动等问题出现；尽可能为不同分辨率创建资源，以减少不必要的硬件缩放</li>
<li>自定义View等绘图与布局优化；尽量避免在draw、measure、layout中做过于耗时及耗内存操作，尤其是draw方法中，尽量减少draw、measure、layout等执行次数，避免过度渲染和绘制；减少不必要的inflate，尽量使用全局变量缓存View</li>
<li>避免ANR，不要在UI线程中做耗时操作，譬如多次数据库操作等</li>
</ul>
</li>
<li><p>Layout常用的标签</p>
<ul>
<li><p>include标签：该标签可以用于将布局文件中的公共部分提取出来给其它布局文件复用，从而使得布局模块化，代码轻量化; 注意点: ①如果标签已经定义了id，而嵌入布局文件的root布局文件也定义了id，标签的id会覆盖掉嵌入布局文件root的id，如果include标签没有定义id则会使用嵌入文件root的id ②如果想使用标签覆盖嵌入布局root布局属性，必须同时覆盖layout_height和layout_width属性，否则会直接报编译时语法错误</p>
</li>
<li><p>viewstub标签：该标签与include一样用于引入布局模块，只是引入的布局默认不会扩张，既不会占用显示也不会占用位置，从而在解析layout时节省cpu和内存，只有通过调用setVisibility函数或者Inflate函数才会将其要装载的目标布局给加载出来，从而达到延迟加载的效果；例如条目详情、进度条标识或者未读消息等，这些情况如果在一开始初始化，虽然设置可见性View.GONE,但是在Inflate的时候View仍然会被Inflate，仍然会创建对象。</p>
</li>
<li><p>merge标签：该标签在layout中会被自动忽略，从而减少一层布局嵌套，其主要用处是当一个布局作为子布局被其他布局include时，使用merge当作该布局的顶节点来代替layout顶节点就可以减少一层嵌套</p>
</li>
<li><p>hierarchy viewer：该工具可以方便的查看Activity的布局，各个View的属性、measure、layout、draw的时间，如果耗时较多会用红色标记，否则显示绿色</p>
</li>
</ul>
</li>
</ul>
<h3 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h3><ul>
<li>异步请求网络数据，避免频繁请求数据（例如如果某个页面内请求过多，可以考虑做一定的请求合并），尽可能的减少网络请求次数和减小网络请求时间间隔</li>
<li>网络应用传输中使用高效率的数据格式，譬如使用JSON代替XML，使用WebP代替其他图片格式,并对数据进行Gzip压缩数据，比如post请求的body和header内容</li>
<li>及时缓存数据到内存/文件/数据库</li>
<li><p>执行某些操作前尽量先进行网络状态判断，比如wifi传输数据比蜂窝数据更省电，所以尽量在wifi下进行数据的预加载</p>
</li>
<li><p>httpClient和httpUrlConnection对比：</p>
<ul>
<li>httpClient是apache的开源实现，API数量多，非常稳定</li>
<li>httpUrlConnection是java自带的模块: ①可以直接支持GZIP压缩,而HttpClient虽然也支持GZIP，但要自己写代码处理 ②httpUrlConnection直接在系统层面做了缓存策略处理，加快重复请求的速度 ③API简单，体积较小,而且直接支持系统级连接池，即打开的连接不会直接关闭，在一段时间内所有程序可共用</li>
<li>HttpURLConnection在Android2.2之前有个重大Bug，调用close()函数会影响连接池，导致连接复用失效，需要关闭keepAlive;因此在2.2之前http请求都是用httpClient，2.2之后则是使用HttpURLConnection</li>
<li>但是!!!现在!!!Android不再推荐这两种方式！二是直接使用OKHttp这种成熟方案！支持Android 2.3及其以上版本</li>
</ul>
</li>
</ul>
<h3 id="数据结构优化"><a href="#数据结构优化" class="headerlink" title="数据结构优化"></a>数据结构优化</h3><ul>
<li><p>ArrayList和LinkedList的选择：ArrayList根据index取值更快，LinkedList更占内存、随机插入删除更快速、扩容效率更高</p>
</li>
<li><p>ArrayList、HashMap、LinkedHashMap、HashSet的选择：hash系列数据结构查询速度更优，ArrayList存储有序元素，HashMap为键值对数据结构，LinkedHashMap可以记住加入次序的hashMap，HashSet不允许重复元素</p>
</li>
<li><p>HashMap、WeakHashMap选择：WeakHashMap中元素可在适当时候被系统垃圾回收器自动回收，所以适合在内存紧张时使用</p>
</li>
<li><p>Collections.synchronizedMap和ConcurrentHashMap的选择：ConcurrentHashMap为细分锁，锁粒度更小，并发性能更优；Collections.synchronizedMap为对象锁，自己添加函数进行锁控制更方便</p>
</li>
<li><p>Android中性能更优的数据类型：如SparseArray、SparseBooleanArray、SparseIntArray、Pair；Sparse系列的数据结构是为key为int情况的特殊处理，采用二分查找及简单的数组存储，加上不需要泛型转换的开销，相对Map来说性能更优</p>
</li>
</ul>
<h3 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h3><ul>
<li>Android应用内存溢出OOM<ul>
<li>内存溢出的主要导致原因有如下几类：①应用代码存在内存泄露，长时间积累无法释放导致OOM；②应用的某些逻辑操作疯狂的消耗掉大量内存（譬如加载一张不经过处理的超大超高清图片等）导致超过阈值OOM</li>
<li>解决思路①在内存引用上做些处理，常用的有软引用、弱引用 ②在内存中加载图片时直接在内存中做处理，如：边界压缩 ③动态回收内存，手动recyle bitmap，回收对象 ④优化Dalvik虚拟机的堆内存分配 ⑤自定义堆内存大小</li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Android手机由于其本身的后台机制和硬件特点，性能上一直被诟病，所以软件开发者对软件本身的性能优化就显得尤为重要；本文将对Android开发过程中性能优化的各个方面做一个回顾与总结。&lt;/p&gt;
&lt;h3 id=&quot;Cache优化&quot;&gt;&lt;a href=&quot;#Cache优化&quot; cla
    
    </summary>
    
      <category term="Android" scheme="http://wodekouwei.com/categories/Android/"/>
    
    
      <category term="Android" scheme="http://wodekouwei.com/tags/Android/"/>
    
      <category term="性能" scheme="http://wodekouwei.com/tags/%E6%80%A7%E8%83%BD/"/>
    
      <category term="tips" scheme="http://wodekouwei.com/tags/tips/"/>
    
  </entry>
  
  <entry>
    <title>多媒体之术语</title>
    <link href="http://wodekouwei.com/2017/06/27/media-terminology/"/>
    <id>http://wodekouwei.com/2017/06/27/media-terminology/</id>
    <published>2017-06-27T14:29:35.000Z</published>
    <updated>2017-07-01T01:45:17.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-编解码术语"><a href="#一-编解码术语" class="headerlink" title="一.编解码术语"></a>一.编解码术语</h2><h3 id="1-GOP-码流-比特率-帧速率-分辨率"><a href="#1-GOP-码流-比特率-帧速率-分辨率" class="headerlink" title="1.GOP/码流/比特率/帧速率/分辨率"></a>1.GOP/码流/比特率/帧速率/分辨率</h3><h4 id="1-1GOP（Group-of-picture）"><a href="#1-1GOP（Group-of-picture）" class="headerlink" title="1.1GOP（Group of picture）"></a>1.1GOP（Group of picture）</h4><p>关键帧的周期，也就是两个IDR帧之间的距离，一个帧组的最大帧数，一般而言，每一秒视频至少需要使用 1 个关键帧。增加关键帧个数可改善质量，但是同时增加带宽和网络负载。</p>
<p>需要说明的是，通过提高GOP值来提高图像质量是有限度的，在遇到场景切换的情况时，H.264编码器会自动强制插入一个I帧，此时实际的GOP值被缩短了。另一方面，在一个GOP中，P、B帧是由I帧预测得到的，当I帧的图像质量比较差时，会影响到一个GOP中后续P、B帧的图像质量，直到下一个GOP开始才有可能得以恢复，所以GOP值也不宜设置过大。</p>
<p>同时，由于P、B帧的复杂度大于I帧，所以过多的P、B帧会影响编码效率，使编码效率降低。另外，过长的GOP还会影响Seek操作的响应速度，由于P、B帧是由前面的I或P帧预测得到的，所以Seek操作需要直接定位，解码某一个P或B帧时，需要先解码得到本GOP内的I帧及之前的N个预测帧才可以，GOP值越长，需要解码的预测帧就越多，seek响应的时间也越长。</p>
<h5 id="1-12CABAC-CAVLC"><a href="#1-12CABAC-CAVLC" class="headerlink" title="1.12CABAC/CAVLC"></a>1.12CABAC/CAVLC</h5><p>H.264/AVC标准中两种熵编码方法，CABAC叫自适应二进制算数编码，CAVLC叫前后自适应可变长度编码，</p>
<ol>
<li>CABAC：是一种无损编码方式，画质好，X264就会舍弃一些较小的DCT系数，码率降低，可以将码率再降低10-15%（特别是在高码率情况下），会降低编码和解码的速速。</li>
<li>CAVLC将占用更少的CPU资源，但会影响压缩性能。</li>
</ol>
<p>其他:</p>
<ul>
<li>帧：当采样视频信号时，如果是通过逐行扫描，那么得到的信号就是一帧图像，通常帧频为25帧每秒（PAL制）、30帧每秒（NTSC制）；</li>
<li>场：当采样视频信号时，如果是通过隔行扫描（奇、偶数行），那么一帧图像就被分成了两场，通常场频为50Hz（PAL制）、60Hz（NTSC制）；</li>
<li>帧频、场频的由来：最早由于抗干扰和滤波技术的限制，电视图像的场频通常与电网频率（交流电）相一致，于是根据各地交流电频率不同就有了欧洲和中国等PAL制的50Hz和北美等NTSC制的60Hz，但是现在并没有这样的限制了，帧频可以和场频一样，或者场频可以更高。</li>
<li>帧编码、场编码方式：逐行视频帧内邻近行空间相关性较强，因此当活动量非常小或者静止的图像比较适宜采用帧编码方式；而场内相邻行之间的时间相关性较强，对运动量较大的运动图像则适宜采用场编码方式。</li>
</ul>
<h5 id="1-1-3Deblocking"><a href="#1-1-3Deblocking" class="headerlink" title="1.1.3Deblocking"></a>1.1.3Deblocking</h5><p>开启会减少块效应.</p>
<h5 id="1-1-4FORCE-IDR"><a href="#1-1-4FORCE-IDR" class="headerlink" title="1.1.4FORCE_IDR"></a>1.1.4FORCE_IDR</h5><p>是否让每个I帧变成IDR帧，如果是IDR帧，支持随机访问。</p>
<h5 id="1-1-5frame-tff-bff"><a href="#1-1-5frame-tff-bff" class="headerlink" title="1.1.5frame,tff,bff"></a>1.1.5frame,tff,bff</h5><p>–frame 将两场合并作为一帧进行编码,–tff Enable interlaced mode (开启隔行编码并设置上半场在前),–bff Enable interlaced mode。</p>
<p>PAFF 和MBAFF：当对隔行扫描图像进行编码时，每帧包括两个场，由于两个场之间存在较大的扫描间隔，这样，对运动图像来说，帧中相邻两行之间的空间相关性相对于逐行扫描时就会减小，因此这时对两个场分别进行编码会更节省码流。</p>
<p>对帧来说，存在三种可选的编码方式：将两场合并作为一帧进行编码(frame 方式)或将两场分别编码(field 方式)或将两场合并起来作为一帧，但不同的是将帧中垂直相邻的两个宏块合并为宏块对进行编码；前两种称为PAFF 编码，对运动区域进行编码时field 方式有效，对非运区域编码时，由于相邻两行有较大的相关性，因而frame 方式会更有效。当图像同时存在运动区域和非运动区域时，在MB 层次上，对运动区域采取field 方式，对非运动区域采取frame 方式会更加有效，这种方式就称为MBAFF，预测的单位是宏块对。</p>
<h4 id="1-2码流-码率"><a href="#1-2码流-码率" class="headerlink" title="1.2码流/码率"></a>1.2码流/码率</h4><p>码流(Data Rate)是指视频文件在单位时间内使用的数据流量，也叫码率或码流率，通俗一点的理解就是取样率,是视频编码中画面质量控制中最重要的部分，一般我们用的单位是kb/s或者Mb/s。一般来说同样分辨率下，视频文件的码流越大，压缩比就越小，画面质量就越高。码流越大，说明单位时间内取样率越大，数据流，精度就越高，处理出来的文件就越接近原始文件，图像质量越好，画质越清晰，要求播放设备的解码能力也越高。</p>
<p>当然，码流越大，文件体积也越大，其计算公式是文件体积=时间X码率/8。例如，网络上常见的一部90分钟1Mbps码流的720P RMVB文件，其体积就=5400秒×1Mb/8=675MB。</p>
<p>通常来说，一个视频文件包括了画面及声音，例如一个RMVB的视频文件，里面包含了视频信息和音频信息，音频及视频都有各自不同的采样方式和比特率，也就是说，同一个视频文件音频和视频的比特率并不是一样的。而我们所说的一个视频文件码流率大小，一般是指视频文件中音频及视频信息码流率的总和。</p>
<p>以以国内最流行，大家最熟悉的RMVB视频文件为例，RMVB中的VB，指的是VBR，即Variable Bit Rate的缩写，中文含义是可变比特率，它表示RMVB采用的是动态编码的方式，把较高的采样率用于复杂的动态画面(歌舞、飞车、战争、动作等)，而把较低的采样率用于静态画面，合理利用资源，达到画质与体积可兼得的效果。</p>
<p>码率和取样率最根本的差别就是码率是针对源文件来讲的。</p>
<h4 id="1-3采样率"><a href="#1-3采样率" class="headerlink" title="1.3采样率"></a>1.3采样率</h4><p>采样率（也称为采样速度或者采样频率）定义了每秒从连续信号中提取并组成离散信号的采样个数，它用赫兹（Hz）来表示。采样率是指将模拟信号转换成数字信号时的采样频率，也就是单位时间内采样多少点。一个采样点数据有多少个比特。比特率是指每秒传送的比特(bit)数。单位为 bps(Bit Per Second)，比特率越高，传送的数据越大，音质越好.比特率 =采样率 x 采用位数 x声道数.</p>
<p>采样率类似于动态影像的帧数，比如电影的采样率是24赫兹，PAL制式的采样率是25赫兹，NTSC制式的采样率是30赫兹。当我们把采样到的一个个静止画面再以采样率同样的速度回放时，看到的就是连续的画面。同样的道理，把以44.1kHZ采样率记录的CD以同样的速率播放时，就能听到连续的声音。显然，这个采样率越高，听到的声音和看到的图像就越连贯。当然，人的听觉和视觉器官能分辨的采样率是有限的，基本上高于44.1kHZ采样的声音，绝大部分人已经觉察不到其中的分别了。</p>
<p>而声音的位数就相当于画面的颜色数，表示每个取样的数据量，当然数据量越大，回放的声音越准确，不至于把开水壶的叫声和火车的鸣笛混淆。同样的道理，对于画面来说就是更清晰和准确，不至于把血和西红柿酱混淆。不过受人的器官的机能限制，16位的声音和24位的画面基本已经是普通人类的极限了，更高位数就只能靠仪器才能分辨出来了。比如电话就是3kHZ取样的7位声音，而CD是44.1kHZ取样的16位声音，所以CD就比电话更清楚。</p>
<p>当你理解了以上这两个概念，比特率就很容易理解了。以电话为例，每秒3000次取样，每个取样是7比特，那么电话的比特率是21000。 而CD是每秒 44100次取样，两个声道，每个取样是13位PCM编码，所以CD的比特率是44100<em>2</em>13=1146600，也就是说CD每秒的数据量大约是 144KB，而一张CD的容量是74分等于4440秒，就是639360KB＝640MB。</p>
<p>码率和取样率最根本的差别就是码率是针对源文件来讲的</p>
<h4 id="1-4比特率"><a href="#1-4比特率" class="headerlink" title="1.4比特率"></a>1.4比特率</h4><p>比特率是指每秒传送的比特(bit)数。单位为bps(Bit Per Second)，比特率越高，传送的数据越大。在视频领域,比特率常翻译为码率 !!!</p>
<p>比特率表示经过编码（压缩）后的音、视频数据每秒钟需要用多少个比特来表示，而比特就是二进制里面最小的单位，要么是0，要么是1。比特率与音、视频压缩的关系，简单的说就是比特率越高，音、视频的质量就越好，但编码后的文件就越大；如果比特率越少则情况刚好相反。</p>
<p>比特率是指将数字声音、视频由模拟格式转化成数字格式的采样率，采样率越高，还原后的音质、画质就越好。</p>
<p>常见编码模式：</p>
<ul>
<li>VBR（Variable Bitrate）动态比特率 也就是没有固定的比特率，压缩软件在压缩时根据音频数据即时确定使用什么比特率，这是以质量为前提兼顾文件大小的方式，推荐编码模式；</li>
<li>ABR（Average Bitrate）平均比特率 是VBR的一种插值参数。LAME针对CBR不佳的文件体积比和VBR生成文件大小不定的特点独创了这种编码模式。ABR在指定的文件大小内，以每50帧（30帧约1秒）为一段，低频和不敏感频率使用相对低的流量，高频和大动态表现时使用高流量，可以做为VBR和CBR的一种折衷选择。</li>
<li>CBR（Constant Bitrate），常数比特率 指文件从头到尾都是一种位速率。相对于VBR和ABR来讲，它压缩出来的文件体积很大，而且音质相对于VBR和ABR不会有明显的提高。</li>
</ul>
<h4 id="1-5帧速率"><a href="#1-5帧速率" class="headerlink" title="1.5帧速率"></a>1.5帧速率</h4><p>帧速率也称为FPS(Frames PerSecond)的缩写——帧/秒。是指每秒钟刷新的图片的帧数，也可以理解为图形处理器每秒钟能够刷新几次。越高的帧速率可以得到更流畅、更逼真的动画。每秒钟帧数(FPS)越多，所显示的动作就会越流畅。</p>
<h4 id="1-6分辨率"><a href="#1-6分辨率" class="headerlink" title="1.6分辨率"></a>1.6分辨率</h4><p>就是帧大小每一帧就是一副图像。</p>
<p>640*480分辨率的视频，建议视频的码速率设置在700以上，音频采样率44100就行了</p>
<p>一个音频编码率为128Kbps，视频编码率为800Kbps的文件，其总编码率为928Kbps，意思是经过编码后的数据每秒钟需要用928K比特来表示。</p>
<p>视频分辨率是指视频成像产品所成图像的大小或尺寸。常见的视像分辨率有352×288，176×144，640×480，1024×768。在成像的两组数字中，前者为图片长度，后者为图片的宽度，两者相乘得出的是图片的像素，长宽比一般为4：3. 　目前监控行业中主要使用Qcif(176×144）、CIF(352×288）、HALF D1(704×288）、D1(704×576）等几种分辨率。
D1是数字电视系统显示格式的标准，共分为以下5种规格：</p>
<ol>
<li>D1：480i格式（525i）：720×480（水平480线，隔行扫描），和NTSC模拟电视清晰度相同，行频为15.25kHz，相当于我们所说的4CIF(720×576)</li>
<li>D2：480P格式（525p）：720×480（水平480线，逐行扫描），较D1隔行扫描要清晰不少，和逐行扫描DVD规格相同，行频为31.5kHz</li>
<li>D3：1080i格式（1125i）：1920×1080（水平1080线，隔行扫描），高清方式采用最多的一种分辨率，分辨率为1920×1080i/60Hz，行频为33.75kHz</li>
<li>D4：720p格式（750p）：1280×720（水平720线，逐行扫描），虽然分辨率较D3要低，但是因为逐行扫描，市面上更多人感觉相对于1080I（实际逐次540线）视觉效果更加清晰。不过个人感觉来说，在最大分辨率达到1920×1080的情况下，D3要比D4感觉更加清晰，尤其是文字表现力上，分辨率为1280×720p/60Hz，行频为45kHz</li>
<li>D5：1080p格式（1125p）：1920×1080（水平1080线，逐行扫描），目前民用高清视频的最高标准，分辨率为1920×1080P/60Hz，行频为67.5KHZ。</li>
</ol>
<p>其中D1 和D2标准是我们一般模拟电视的最高标准，并不能称的上高清晰，D3的1080i标准是高清晰电视的基本标准，它可以兼容720p格式，而D5的1080P只是专业上的标准。</p>
<p>计算输出文件大小公式：
（音频编码率（KBit为单位）/8 +视频编码率（KBit为单位）/8）×影片总长度（秒为单位）=文件大小（MB为单位）</p>
<h3 id="2-高清视频"><a href="#2-高清视频" class="headerlink" title="2.高清视频"></a>2.高清视频</h3><p>目前的720P以及1080P采用了很多种编码，例如主流的MPEG2，VC-1以及H.264，还有Divx以及Xvid，至于封装格式更多到令人发指，ts、mkv、wmv以及蓝光专用等等。</p>
<p>720和1080代表视频流的分辨率，前者1280<em>720，后者1920</em>1080，不同的编码需要不同的系统资源，大概可以认为是H.264&gt;VC-1&gt;MPEG2。 　</p>
<p>VC-1是最后被认可的高清编码格式，不过因为有微软的后台，所以这种编码格式不能小窥。相对于MPEG2，VC-1的压缩比更高，但相对于H.264而言，编码解码的计算则要稍小一些，目前来看，VC-1可能是一个比较好的平衡，辅以微软的支持，应该是一只不可忽视的力量。一般来说，VC-1多为 “.wmv”后缀，但这都不是绝对的，具体的编码格式还是要通过软件来查询。</p>
<p>总的来说，从压缩比上来看，H.264的压缩比率更高一些，也就是同样的视频，通过H.264编码算法压出来的视频容量要比VC-1的更小，但是VC-1 格式的视频在解码计算方面则更小一些，一般通过高性能的CPU就可以很流畅的观看高清视频。相信这也是目前NVIDIA Geforce 8系列显卡不能完全解码VC-1视频的主要原因。</p>
<p>PS&amp;TS是两种视频或影片封装格式，常用于高清片。扩展名分别为VOB/EVO和TS等；其文件编码一般用MPEG2/VC-1/H.264</p>
<p> 高清，英文为“High Definition”，即指“高分辨率”。 高清电视(HDTV)，是由美国电影电视工程师协会确定的高清晰度电视标准格式。现在的大屏幕液晶电视机，一般都支持1080i和720P，而一些俗称的“全高清”(Full HD)，则是指支持1080P输出的电视机。</p>
<p>目前的高清视频编码格式主要有H.264、VC-1、MPEG-2、MPEG-4、DivX、XviD、WMA-HD以及X264。事实上，现在网络上流传的高清视频主要以两类文件的方式存在：一类是经过MPEG-2标准压缩，以tp和ts为后缀的视频流文件;一类是经过WMV-HD(Windows Media Video HighDefinition)标准压缩过的wmv文件，还有少数文件后缀为avi或mpg，其性质与wmv是一样的。真正效果好的高清视频更多地以H.264与VC-1这两种主流的编码格式流传。</p>
<p>一般来说，H.264格式以“.avi”、“.mkv”以及“.ts”封装比较常见。</p>
<h3 id="3-位率（定码率，变码率）"><a href="#3-位率（定码率，变码率）" class="headerlink" title="3.位率（定码率，变码率）"></a>3.位率（定码率，变码率）</h3><p>位率又称为“码率”。指单位时间内，单个录像通道所产生的数据量，其单位通常是bps、Kbps或Mbps。可以根据录像的时间与位率估算出一定时间内的录像文件大小。 　位率是一个可调参数，不同的分辨率模式下和监控场景下，合适的位率大小是不同的。在设置时，要综合考虑三个因素： 　　</p>
<ol>
<li>分辨率:分辨率是决定位率（码率）的主要因素，不同的分辨率要采用不同的位率。总体而言，录像的分辨率越高，所要求的位率（码率）也越大，但并不总是如此，图1说明了不同分辨率的合理的码率选择范围。所谓“合理的范围”指的是，如果低于这个范围，图像质量看起来会变得不可接受；如果高于这个范围，则显得没有必要，对于网络资源以及存储资源来说是一种浪费。 　　</li>
<li>场景:监控的场景是设置码率时要考虑的第二个因素。在视频监控中，图像的运动剧烈程度还与位率有一定的关系，运动越剧烈，编码所要求的码率就越高。反之则越低。因此在同样的图像分辨率条件下，监控人多的场景和人少的场景，所要求的位率也是不同的。 　　</li>
<li>存储空间:最后需要考量的因素是存储空间，这个因素主要是决定了录像系统的成本。位率设置得越高，画质相对会越好，但所要求的存储空间就越大。所以在工程实施中，设置合适的位率即可以保证良好的回放图像质量，又可以避免不必要的资源浪费。
　　
QP(quantizer parameter)
介于0~31之间，值越小，量化越精细，图像质量就越高，而产生的码流也越长。</li>
</ol>
<p>PSNR
允许计算峰值信噪比(PSNR,Peak signal-to-noise ratio),编码结束后在屏幕上显示PSNR计算结果。开启与否与输出的视频质量无关，关闭后会带来微小的速度提升。</p>
<p>profile level
分别是BP、EP、MP、HP：</p>
<ol>
<li>BP-Baseline Profile：基本画质。支持I/P 帧，只支持无交错（Progressive）和CAVLC；</li>
<li>EP-Extended profile：进阶画质。支持I/P/B/SP/SI 帧，只支持无交错（Progressive）和CAVLC；</li>
<li>MP-Main profile：主流画质。提供I/P/B 帧，支持无交错（Progressive）和交错（Interlaced），也支持CAVLC 和CABAC 的支持；</li>
<li>HP-High profile：高级画质。在main Profile 的基础上增加了8x8内部预测、自定义量化、无损视频编码和更多的YUV 格式；</li>
</ol>
<p>H.264规定了三种档次，每个档次支持一组特定的编码功能，并支持一类特定的应用。</p>
<ol>
<li>基本档次：利用I片和P片支持帧内和帧间编码，支持利用基于上下文的自适应的变长编码进行的熵编码（CAVLC）。主要用于可视电话、会议电视、无线通信等实时视频通信；</li>
<li>主要档次：支持隔行视频，采用B片的帧间编码和采用加权预测的帧内编码；支持利用基于上下文的自适应的算术编码（CABAC）。主要用于数字广播电视与数字视频存储；</li>
<li>扩展档次：支持码流之间有效的切换（SP和SI片）、改进误码性能（数据分割），但不支持隔行视频和CABAC。主要用于网络的视频流，如视频点播。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一-编解码术语&quot;&gt;&lt;a href=&quot;#一-编解码术语&quot; class=&quot;headerlink&quot; title=&quot;一.编解码术语&quot;&gt;&lt;/a&gt;一.编解码术语&lt;/h2&gt;&lt;h3 id=&quot;1-GOP-码流-比特率-帧速率-分辨率&quot;&gt;&lt;a href=&quot;#1-GOP-码流-比特率-
    
    </summary>
    
      <category term="media" scheme="http://wodekouwei.com/categories/media/"/>
    
    
      <category term="多媒体" scheme="http://wodekouwei.com/tags/%E5%A4%9A%E5%AA%92%E4%BD%93/"/>
    
  </entry>
  
  <entry>
    <title>Image Stride(内存图像行跨度)</title>
    <link href="http://wodekouwei.com/2017/06/27/media-graphic-image-stride/"/>
    <id>http://wodekouwei.com/2017/06/27/media-graphic-image-stride/</id>
    <published>2017-06-27T07:00:14.000Z</published>
    <updated>2017-06-27T07:24:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>When a video image is stored in memory, the memory buffer might contain extra padding bytes after each row of pixels. The padding bytes affect how the image is store in memory, but do not affect how the image is displayed.</p>
<p>当视频图像存储在内存时，图像的每一行末尾也许包含一些扩展的内容，这些扩展的内容只影响图像如何存储在内存中，但是不影响图像如何显示出来；</p>
<p>The stride is the number of bytes from one row of pixels in memory to the next row of pixels in memory. Stride is also called pitch. If padding bytes are present, the stride is wider than the width of the image, as shown in the following illustration.</p>
<p>Stride 就是这些扩展内容的名称，Stride 也被称作 Pitch，如果图像的每一行像素末尾拥有扩展内容，Stride 的值一定大于图像的宽度值，就像下图所示：
<img src="http://images.wodekouwei.com/media/imagestride1.png" alt="image"></p>
<p>Two buffers that contain video frames with equal dimensions can have two different strides. If you process a video image, you must take into the stride into account.</p>
<p>两个缓冲区包含同样大小（宽度和高度）的视频帧，却不一定拥有同样的 Stride 值，如果你处理一个视频帧，你必须在计算的时候把 Stride 考虑进去；</p>
<p>In addition, there are two ways that an image can be arranged in memory. In a top-down image, the top row of pixels in the image appears first in memory. In a bottom-up image, the last row of pixels appears first in memory. The following illustration shows the difference between a top-down image and a bottom-up image.</p>
<p>另外，一张图像在内存中有两种不同的存储序列（arranged），对于一个从上而下存储（Top-Down） 的图像，最顶行的像素保存在内存中最开头的部分，对于一张从下而上存储（Bottom-Up）的图像，最后一行的像素保存在内存中最开头的部分，下面图示展示了这两种情况：
<img src="http://images.wodekouwei.com/media/imagestride2.png" alt="image"></p>
<p>A bottom-up image has a negative stride, because stride is defined as the number of bytes need to move down a row of pixels, relative to the displayed image. YUV images should always be top-down, and any image that is contained in a Direct3D surface must be top-down. RGB images in system memory are usually bottom-up.</p>
<p>一张从下而上的图像拥有一个负的 Stride 值，因为 Stride 被定义为[从一行像素移动到下一行像素时需要跨过多少个像素]，仅相对于被显示出来的图像而言；而 YUV 图像永远都是从上而下表示的，以及任何包含在 Direct3D Surface 中的图像必须是从上而下，RGB 图像保存在系统内存时通常是从下而上；</p>
<p>Video transforms in particular need to handle buffers with mismatched strides, because the input buffer might not match the output buffer. For example, suppose that you want to convert a source image and write the result to a destination image. Assume that both images have the same width and height, but might not have the same pixel format or the same image stride.</p>
<p>尤其是视频变换，特别需要处理不同 Stride 值的图像，因为输入缓冲也许与输出缓冲不匹配，举个例子，假设你想要将源图像转换并且将结果写入到目标图像，假设两个图像拥有相同的宽度和高度，但是其像素格式与 Stride 值也许不同；</p>
<p>The following example code shows a generalized approach for writing this kind of function. This is not a complete working example, because it abstracts many of the specific details.</p>
<p>下面代码演示了一种通用方法来编写这种功能，这段代码并不完整，因为这只是一个抽象的算法，没有完全考虑到真实需求中的所有细节；
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">void ProcessVideoImage(</div><div class="line">    BYTE*       pDestScanLine0,    </div><div class="line">    LONG        lDestStride,       </div><div class="line">    const BYTE* pSrcScanLine0,     </div><div class="line">    LONG        lSrcStride,        </div><div class="line">    DWORD       dwWidthInPixels,    </div><div class="line">    DWORD       dwHeightInPixels</div><div class="line">    )</div><div class="line">&#123;</div><div class="line">    for (DWORD y = 0; y &lt; dwHeightInPixels; y++)</div><div class="line">    &#123;</div><div class="line">        SOURCE_PIXEL_TYPE *pSrcPixel = (SOURCE_PIXEL_TYPE*)pDestScanLine0;</div><div class="line">        DEST_PIXEL_TYPE *pDestPixel = (DEST_PIXEL_TYPE*)pSrcScanLine0;</div><div class="line"></div><div class="line">        for (DWORD x = 0; x &lt; dwWidthInPixels; x +=2)</div><div class="line">        &#123;</div><div class="line">            pDestPixel[x] = TransformPixelValue(pSrcPixel[x]);</div><div class="line">        &#125;</div><div class="line">        pDestScanLine0 += lDestStride;</div><div class="line">        pSrcScanLine0 += lSrcStride;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>This function takes six parameters:</p>
<ul>
<li>A pointer to the start of scan line 0 in the destination image.</li>
<li>The stride of the destination image.</li>
<li>A pointer to the start of scan line 0 in the source image.</li>
<li>The stride of the source image.</li>
<li>The width of the image in pixels.</li>
<li>The height of the image in pixels.</li>
</ul>
<p>这个函数需要六个参数：</p>
<ul>
<li>目标图像的起始扫描行的内存指针</li>
<li>目标图像的 Stride 值</li>
<li>源图像的起始扫描行的内存指针</li>
<li>源图像的 Stride 值</li>
<li>图像的宽度值（以像素为单位）</li>
<li>图像的高度值（以像素为单位）
The general idea is to process one row at a time, iterating over each pixel in the row. Assume that SOURCE_PIXEL_TYPE and DEST_PIXEL_TYPE are structures representing the pixel layout for the source and destination images, respectively. (For example, 32-bit RGB uses the RGBQUAD structure. Not every pixel format has a pre-defined structure.) Casting the array pointer to the structure type enables you to access the RGB or YUV components of each pixel. At the start of each row, the function stores a pointer to the row. At the end of the row, it increments the pointer by the width of the image stride, which advances the pointer to the next row.</li>
</ul>
<p>这里的要点是如何一次处理一行像素，遍历一行里面的每一个像素，假设源像素类型与目标像素类型各自在像素的层面上已经结构化来表示一个源图像与目标图像的像素，（举个例子，32 位 RGB 像素使用 RGBQUAD 结构体，并不是每一种像素类型都有预定义结构体的）强制转换数组指针到这样的结构体指针，可以方便你直接读写每一个像素的 RGB 或者 YUV 值，在每一行的开头，这个函数保存了一个指向这行像素的指针，函数的最后一行，通过图像的 Stride 值直接将指针跳转到图像的下一行像素的起始点；</p>
<p>This example calls a hypothetical function named TransformPixelValue for each pixel. This could be any function that calculates a target pixel from a source pixel. Of course, the exact details will depend on the particular task. For example, if you have a planar YUV format, you must access the chroma planes independently from the luma plane; with interlaced video, you might need to process the fields separately; and so forth.</p>
<p>To give a more concrete example, the following code converts a 32-bit RGB image into an AYUV image. The RGB pixels are accessed using an RGBQUAD structure, and the AYUV pixels are accessed using aDXVA2_AYUVSample8 Structure structure.</p>
<blockquote>
<p>引用:
如果你用的是 MSDN Library For Visual Studio 2008 SP1，那么你应该能够在下面地址中找到这篇文章的原文：
ms-help://MS.MSDNQTR.v90.chs/medfound/html/13cd1106-48b3-4522-ac09-8efbaab5c31d.htm</p>
<p><a href="http://blog.csdn.net/g0ose/article/details/52116453" target="_blank" rel="external">http://blog.csdn.net/g0ose/article/details/52116453</a></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When a video image is stored in memory, the memory buffer might contain extra padding bytes after each row of pixels. The padding bytes a
    
    </summary>
    
      <category term="media" scheme="http://wodekouwei.com/categories/media/"/>
    
    
      <category term="多媒体" scheme="http://wodekouwei.com/tags/%E5%A4%9A%E5%AA%92%E4%BD%93/"/>
    
      <category term="翻译" scheme="http://wodekouwei.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>OpenGL混色</title>
    <link href="http://wodekouwei.com/2017/06/27/gl-glBlendFunc/"/>
    <id>http://wodekouwei.com/2017/06/27/gl-glBlendFunc/</id>
    <published>2017-06-27T02:44:39.000Z</published>
    <updated>2017-06-27T08:03:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>混合就是把两种颜色混在一起。具体一点，就是把某一像素位置原来的颜色和将要画上去的颜色，通过某种方式混在一起，从而实现特殊的效果。
假设我们需要绘制这样一个场景：透过红色的玻璃去看绿色的物体，那么可以先绘制绿色的物体，再绘制红色玻璃。在绘制红色玻璃的时候，利用“混合”功能，把将要绘制上去的红色和原来的绿色进行混合，于是得到一种新的颜色，看上去就好像玻璃是半透明的。
要使用OpenGL的混合功能，只需要调用：<code>glEnable(GL_BLEND);</code>即可。要关闭OpenGL的混合功能，只需要调用：<code>glDisable(GL_BLEND);</code>即可。
<strong>注意：</strong> 只有在RGBA模式下，才可以使用混合功能，颜色索引模式下是无法使用混合功能的。</p>
<h3 id="1-源因子和目标因子"><a href="#1-源因子和目标因子" class="headerlink" title="1.源因子和目标因子"></a>1.源因子和目标因子</h3><p>混合需要把原来的颜色和将要画上去的颜色找出来，经过某种方式处理后得到一种新的颜色。这里把将要画上去的颜色称为“源颜色”，把原来的颜色称为“目标颜色”。
OpenGL 会把源颜色和目标颜色各自取出，并乘以一个系数（源颜色乘以的系数称为“源因子”，目标颜色乘以的系数称为“目标因子”），然后相加，这样就得到了新的颜 色。（也可以不是相加，新版本的OpenGL可以设置运算方式，包括加、减、取两者中较大的、取两者中较小的、逻辑运算等）
下面用数学公式来表达一下这个运算方式。假设源颜色的四个分量（指红色，绿色，蓝色，alpha值）是(Rs, Gs, Bs,  As)，目标颜色的四个分量是(Rd, Gd, Bd, Ad)，又设源因子为(Sr, Sg, Sb, Sa)，目标因子为(Dr, Dg, Db,  Da)。则混合产生的新颜色可以表示为：
<code>(Rs*Sr+Rd*Dr, Gs*Sg+Gd*Dg, Bs*Sb+Bd*Db, As*Sa+Ad*Da)</code>
如果颜色的某一分量超过了1.0，则它会被自动截取为1.0，不需要考虑越界的问题。</p>
<p>源因子和目标因子是可以通过<code>glBlendFunc</code>函数来进行设置的。glBlendFunc有两个参数，前者表示源因子，后者表示目标因子。这两个参数可以是多种值，下面介绍比较常用的几种。</p>
<ul>
<li>GL_ZERO：表示使用0.0作为因子，实际上相当于不使用这种颜色参与混合运算。</li>
<li>GL_ONE：表示使用1.0作为因子，实际上相当于完全的使用了这种颜色参与混合运算。</li>
<li>GL_SRC_ALPHA：表示使用源颜色的alpha值来作为因子。</li>
<li>GL_DST_ALPHA：表示使用目标颜色的alpha值来作为因子。</li>
<li>GL_ONE_MINUS_SRC_ALPHA：表示用1.0减去源颜色的alpha值来作为因子。</li>
<li>GL_ONE_MINUS_DST_ALPHA：表示用1.0减去目标颜色的alpha值来作为因子。</li>
<li>GL_SRC_COLOR: 把源颜色的四个分量分别作为因子的四个分量</li>
<li>GL_ONE_MINUS_SRC_COLOR</li>
<li>GL_DST_COLOR</li>
<li>GL_ONE_MINUS_DST_COLOR
GL_SRC_COLOR与GL_ONE_MINUS_SRC_COLOR在OpenGL旧版本中只能用于设置目标因子，GL_DST_COLOR与GL_ONE_MINUS_DST_COLOR在OpenGL 旧版本中只能用于设置源因子。新版本的OpenGL则没有这个限制，并且支持新的GL_CONST_COLOR（设定一种常数颜色，将其四个分量分别作为 因子的四个分量）、GL_ONE_MINUS_CONST_COLOR、GL_CONST_ALPHA、 GL_ONE_MINUS_CONST_ALPHA。另外还有GL_SRC_ALPHA_SATURATE。新版本的OpenGL还允许颜色的alpha 值和RGB值采用不同的混合因子。</li>
</ul>
<h3 id="2-模式示例"><a href="#2-模式示例" class="headerlink" title="2.模式示例"></a>2.模式示例</h3><ul>
<li>如果设置了glBlendFunc(GL_ONE, GL_ZERO);，则表示完全使用源颜色，完全不使用目标颜色，因此画面效果和不使用混合的时候一致（当然效率可能会低一点点）。如果没有设置源因子和目标因子，则默认情况就是这样的设置。</li>
<li>如果设置了glBlendFunc(GL_ZERO, GL_ONE);，则表示完全不使用源颜色，因此无论你想画什么，最后都不会被画上去了。（但这并不是说这样设置就没有用，有些时候可能有特殊用途）</li>
<li>如果设置了glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);，则表示源颜色乘以自身的alpha 值，目标颜色乘以1.0减去源颜色的alpha值，这样一来，源颜色的alpha值越大，则产生的新颜色中源颜色所占比例就越大，而目标颜色所占比例则减 小。这种情况下，我们可以简单的将源颜色的alpha值理解为“不透明度”。这也是混合时最常用的方式。</li>
<li>如果设置了glBlendFunc(GL_ONE, GL_ONE);，则表示完全使用源颜色和目标颜色，最终的颜色实际上就是两种颜色的简单相加。例如红色(1, 0, 0)和绿色(0, 1, 0)相加得到(1, 1, 0)，结果为黄色。
注意：
所谓源颜色和目标颜色，是跟绘制的顺序有关的。假如先绘制了一个红色的物体，再在其上绘制绿色的物体。则绿色是源颜色，红色是目标颜色。如果顺序反过来，则 红色就是源颜色，绿色才是目标颜色。在绘制时，应该注意顺序，使得绘制的源颜色与设置的源因子对应，目标颜色与设置的目标因子对应。</li>
</ul>
<h3 id="3-对两种示例模式的具体解释"><a href="#3-对两种示例模式的具体解释" class="headerlink" title="3.对两种示例模式的具体解释:"></a>3.对两种示例模式的具体解释:</h3><p>模式一:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">GLES20.glEnable(GLES20.GL_BLEND);  </div><div class="line">GLES20.glBlendFunc(GLES20.GL_SRC_ALPHA, GLES20.GL_ONE_MINUS_SRC_ALPHA);</div></pre></td></tr></table></figure></p>
<p>模式二:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">GLES20.glEnable(GLES20.GL_BLEND);  </div><div class="line">GLES20.glBlendFunc(GLES20.GL_ONE, GLES20.GL_ONE_MINUS_SRC_ALPHA);</div></pre></td></tr></table></figure></p>
<p>模式一是传统的alpha通道混合，这种模式下颜色和alpha值是分立的，rgb决定颜色，alpha决定..（英文是决定how solid it is 水平有限找不到准确的中文来表达）
在数学上表达式是：<code>blend(source, dest) = (source.rgb * source.a) + (dest.rgb * (1 – source.a)).</code>要注意的是这种模式下，透明只跟alpha有关，跟rgb值无关，一个透明的颜色，不透明的颜色有相同的rgb值，只要alpha=0即可。</p>
<p>模式二是alpha预乘的混合（Premultiplied Alpha Blending），这种模式下rgb与alpha是联系在一起的，数学上的表达式是
<code>blend(source, dest) = source.rgb + (dest.rgb * (1 – source.a))</code>,在这种模式下，透明的表示是rgb值都为0.</p>
<h3 id="4-EGLSurface背景透明设置"><a href="#4-EGLSurface背景透明设置" class="headerlink" title="4.EGLSurface背景透明设置"></a>4.EGLSurface背景透明设置</h3><p>在OpenGL绘制中,除了设置混色外,还要设置EGLSurface配置支持Alpha,如果不设置EGL相关的EGLSurface支持透明,就算OpenGL函数中开启混色,绘制完成后仍是有黑色背景.</p>
<h4 id="4-1GLSurfaceView"><a href="#4-1GLSurfaceView" class="headerlink" title="4.1GLSurfaceView"></a>4.1GLSurfaceView</h4><p>在onSurfaceCreated里，调用GLES20.glClearColor(0f, 0f, 0f, 0f);alpha为0，即透明。</p>
<p>然后，对surfaceview要作一定处理：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">mGLSurfaceView.setEGLConfigChooser(8, 8, 8, 8, 16, 0);</div><div class="line">TestRenderer renderer = new TestRenderer();</div><div class="line">mGLSurfaceView.setRender(renderer);</div><div class="line">mGLSurfaceView.getHolder().setFormat(PixelFormat.TRANSLUCENT);</div><div class="line">mGLSurfaceView.setZOrderOnTop(true);</div></pre></td></tr></table></figure></p>
<h4 id="4-2SurfaceTexture或Surface构造的Surface"><a href="#4-2SurfaceTexture或Surface构造的Surface" class="headerlink" title="4.2SurfaceTexture或Surface构造的Surface"></a>4.2SurfaceTexture或Surface构造的Surface</h4><p>EGL14.eglChooseConfig中config数组增加EGL10.EGL_ALPHA_SIZE配置:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">int[] CONFIG_RGBA = &#123;</div><div class="line">          EGL10.EGL_RED_SIZE, 8,</div><div class="line">          EGL10.EGL_GREEN_SIZE, 8,</div><div class="line">          EGL10.EGL_BLUE_SIZE, 8,</div><div class="line">          EGL10.EGL_ALPHA_SIZE, 8,</div><div class="line">          EGL10.EGL_RENDERABLE_TYPE, EGL_OPENGL_ES2_BIT,</div><div class="line">          EGL10.EGL_NONE</div><div class="line">  &#125;;</div></pre></td></tr></table></figure></p>
<h3 id="5-实现三维混合"><a href="#5-实现三维混合" class="headerlink" title="5.实现三维混合"></a>5.实现三维混合</h3><p>在进行三维场景的混合时必须注意的是深度缓冲。</p>
<p>深度缓冲是这样一段数据，它记录了每一个像素距离观察者有多近。在启用深度缓冲测试的情况下，如果将要绘制的像素比原来的像素更近，则像素将被绘制。否则,像素就会被忽略掉，不进行绘制。这在绘制不透明的物体时非常有用——不管是先绘制近的物体再绘制远的物体，还是先绘制远的物体再绘制近的物体，或者干脆以 混乱的顺序进行绘制，最后的显示结果总是近的物体遮住远的物体。
然而在你需要实现半透明效果时，发现一切都不是那么美好了。如果你绘制了一个近距离的半透明物体，则它在深度缓冲区内保留了一些信息，使得远处的物体将无法再被绘制出来。虽然半透明的物体仍然半透明，但透过它看到的却不是正确的内容了。
要 解决以上问题，需要在绘制半透明物体时将深度缓冲区设置为只读，这样一来，虽然半透明物体被绘制上去了，深度缓冲区还保持在原来的状态。如果再有一个物体 出现在半透明物体之后，在不透明物体之前，则它也可以被绘制（因为此时深度缓冲区中记录的是那个不透明物体的深度）。以后再要绘制不透明物体时，只需要再 将深度缓冲区设置为可读可写的形式即可。怎么绘制一个一部分半透明一部分不透明的物体？这个好办，只需要把物体分为两个部分，一部分全是半透明 的，一部分全是不透明的，分别绘制就可以了。
即使使用了以上技巧，我们仍然不能随心所欲的按照混乱顺序来进行绘制。必须是先绘制不透明的物体，然 后绘制透明的物体。否则，假设背景为蓝色，近处一块红色玻璃，中间一个绿色物体。如果先绘制红色半透明玻璃的话，它先和蓝色背景进行混合，则以后绘制中间 的绿色物体时，想单独与红色玻璃混合已经不能实现了。
总结起来，绘制顺序就是：首先绘制所有不透明的物体。如果两个物体都是不透明的，则谁先谁后 都没有关系。然后，将深度缓冲区设置为只读。接下来，绘制所有半透明的物体。如果两个物体都是半透明的，则谁先谁后只需要根据自己的意愿（注意了，先绘制 的将成为“目标颜色”，后绘制的将成为“源颜色”，所以绘制的顺序将会对结果造成一些影响）。最后，将深度缓冲区设置为可读可写形式。
调用glDepthMask(GL_FALSE);可将深度缓冲区设置为只读形式。调用glDepthMask(GL_TRUE);可将深度缓冲区设置为可读可写形式。</p>
<blockquote>
<p><a href="https://www.khronos.org/registry/OpenGL-Refpages/es2.0/xhtml/glBlendFunc.xml" target="_blank" rel="external">glBlendFunc函数官网文档</a></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;混合就是把两种颜色混在一起。具体一点，就是把某一像素位置原来的颜色和将要画上去的颜色，通过某种方式混在一起，从而实现特殊的效果。
假设我们需要绘制这样一个场景：透过红色的玻璃去看绿色的物体，那么可以先绘制绿色的物体，再绘制红色玻璃。在绘制红色玻璃的时候，利用“混合”功能，把
    
    </summary>
    
      <category term="OpenGL" scheme="http://wodekouwei.com/categories/OpenGL/"/>
    
    
      <category term="OpenGL" scheme="http://wodekouwei.com/tags/OpenGL/"/>
    
  </entry>
  
  <entry>
    <title>android系统编码MediaCodec</title>
    <link href="http://wodekouwei.com/2017/06/25/tips-android-mediacodec/"/>
    <id>http://wodekouwei.com/2017/06/25/tips-android-mediacodec/</id>
    <published>2017-06-25T08:03:24.000Z</published>
    <updated>2017-06-25T08:20:18.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="android编码器支持参数"><a href="#android编码器支持参数" class="headerlink" title="android编码器支持参数"></a>android编码器支持参数</h3><h4 id="Supported-Media-Formats"><a href="#Supported-Media-Formats" class="headerlink" title="Supported Media Formats"></a><a href="https://developer.android.com/guide/topics/media/media-formats.html" target="_blank" rel="external">Supported Media Formats</a></h4><p><strong>Video encoding recommendations</strong>
The table below lists the Android media framework video encoding profiles and parameters recommended for playback using the H.264 Baseline Profile codec. The same recommendations apply to the Main Profile codec, which is only available in Android 6.0 and later.</p>
<table>
<thead>
<tr>
<th></th>
<th>SD (Low quality)</th>
<th>SD (High quality)</th>
<th>HD 720p (N/A on all devices)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Video resolution</td>
<td>176 x 144 px</td>
<td>480 x 360 px</td>
<td>1280 x 720 px</td>
</tr>
<tr>
<td>Video frame rate</td>
<td>12 fps</td>
<td>30 fps</td>
<td>30 fps</td>
</tr>
<tr>
<td>Video bitrate</td>
<td>56 Kbps</td>
<td>500 Kbps</td>
<td>2 Mbps</td>
</tr>
<tr>
<td>Audio codec</td>
<td>AAC-LC</td>
<td>AAC-LC</td>
<td>AAC-LC</td>
</tr>
<tr>
<td>Audio channels</td>
<td>1 (mono)</td>
<td>2 (stereo)</td>
<td>2 (stereo)</td>
</tr>
<tr>
<td>Audio bitrate</td>
<td>24 Kbps</td>
<td>128 Kbps</td>
<td>192 Kbps</td>
</tr>
</tbody>
</table>
<p>The table below lists the Android media framework video encoding profiles and parameters recommended for playback using the VP8 media codec.</p>
<table>
<thead>
<tr>
<th></th>
<th>SD (Low quality)</th>
<th>SD (High quality)</th>
<th>HD 720p (N/A on all devices)</th>
<th>HD 1080p (N/A on all devices)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Video resolution</td>
<td>320 x 180 px</td>
<td>640 x 360 px</td>
<td>1280 x 720 px</td>
<td>1920 x 1080 px</td>
</tr>
<tr>
<td>Video frame rate</td>
<td>30 fps</td>
<td>30 fps</td>
<td>30 fps</td>
<td>30 fps</td>
</tr>
<tr>
<td>Video bitrate</td>
<td>800 Kbps</td>
<td>2 Mbps</td>
<td>4 Mbps</td>
<td>10 Mbps</td>
</tr>
</tbody>
</table>
<h4 id="CamcorderProfile"><a href="#CamcorderProfile" class="headerlink" title="CamcorderProfile"></a><a href="https://developer.android.com/reference/android/media/CamcorderProfile.html" target="_blank" rel="external">CamcorderProfile</a></h4><p>Retrieves the predefined camcorder profile settings for camcorder applications. These settings are read-only.</p>
<p>The compressed output from a recording session with a given CamcorderProfile contains two tracks: one for audio and one for video.</p>
<p>Each profile specifies the following set of parameters:</p>
<ul>
<li>The file output format</li>
<li>Video codec format</li>
<li>Video bit rate in bits per second</li>
<li>Video frame rate in frames per second</li>
<li>Video frame width and height,</li>
<li>Audio codec format</li>
<li>Audio bit rate in bits per second,</li>
<li>Audio sample rate</li>
<li>Number of audio channels for recording.</li>
</ul>
<h3 id="Android编码器常见问题"><a href="#Android编码器常见问题" class="headerlink" title="Android编码器常见问题"></a>Android编码器常见问题</h3><h4 id="MediaCodec-KEY-FRAME-RATE-seems-to-be-ignored"><a href="#MediaCodec-KEY-FRAME-RATE-seems-to-be-ignored" class="headerlink" title="MediaCodec KEY_FRAME_RATE seems to be ignored"></a><a href="https://stackoverflow.com/questions/22336604/mediacodec-key-frame-rate-seems-to-be-ignored" target="_blank" rel="external">MediaCodec KEY_FRAME_RATE seems to be ignored</a></h4><p><strong>总结起来就是和输入编码器的帧率有关系</strong></p>
<p>I am trying to modify the source for screenrecord in android 4.4 and lower the captured frame rate, but no matter what value I put in:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">format-&gt;setFloat(&quot;frame-rate&quot;, 5);</div></pre></td></tr></table></figure></p>
<p>the result is always the same ( a very high frame rate )
Is the encoder ignoring this property ? how can I control the frame rate ?</p>
<p>The frame-rate value is not ignored, but it doesn’t do what you want.</p>
<p>The combination of frame-rate and i-frame-interval determines how often I-frames (also called “sync frames”) appear in the encoded output. The frame rate value might also play a role in meeting the bitrate target on some devices, but I’m not sure about that (see e.g. this post).</p>
<p>The MediaCodec encoder does not drop frames. If you want to reduce the frame rate, you have to do so by sending fewer frames to it.</p>
<p>The screenrecord command doesn’t “sample” the screen at a fixed frame rate. Instead, every frame it receives from the surface compositor (SurfaceFlinger) is sent to the encoder, with an appropriate time stamp. If screenrecord receives 60 frames per seconds, you’ll have 60fps output. If it receives 10 frames in quick succession, followed by nothing for 5 seconds, followed by a couple more, you’ll have exactly that in the output file.</p>
<p>You can modify screenrecord to drop frames, but you have to be a bit careful. If you try to reduce the maximum frame rate from 60fps to 30fps by dropping every-other frame, you run the risk that in a “frame0 - frame1 - long_pause - frame2” sequence you’ll drop frame1, and the video will hold on frame0 instead, showing a not-quite-complete animation. So you need to buffer up a frame, and then encode or drop frame N-1 if the difference in presentation times between that and frame N is ~17ms.</p>
<p>The tricky part is that screenrecord, in its default operating mode, directs the frames to the encoder without touching them, so all you see is the encoded output. You can’t arbitrarily drop individual frames of encoded data, so you really want to prevent the encoder from seeing them in the first place. If you use the screenrecord v1.1 sources you can tap into “overlay” mode, used for –bugreport, to have the frames pass through screenrecord on their way to the encoder.</p>
<p>In some respects it might be simpler to write a post-processor that reduces the frame rate. I don’t know how much quality would be lost by decoding and re-encoding the video.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;android编码器支持参数&quot;&gt;&lt;a href=&quot;#android编码器支持参数&quot; class=&quot;headerlink&quot; title=&quot;android编码器支持参数&quot;&gt;&lt;/a&gt;android编码器支持参数&lt;/h3&gt;&lt;h4 id=&quot;Supported-Media-F
    
    </summary>
    
      <category term="Android" scheme="http://wodekouwei.com/categories/Android/"/>
    
    
      <category term="多媒体" scheme="http://wodekouwei.com/tags/%E5%A4%9A%E5%AA%92%E4%BD%93/"/>
    
      <category term="Android" scheme="http://wodekouwei.com/tags/Android/"/>
    
      <category term="tips" scheme="http://wodekouwei.com/tags/tips/"/>
    
  </entry>
  
  <entry>
    <title>media-storage-and-transfer</title>
    <link href="http://wodekouwei.com/2017/06/20/media-storage-and-transfer/"/>
    <id>http://wodekouwei.com/2017/06/20/media-storage-and-transfer/</id>
    <published>2017-06-20T15:10:09.000Z</published>
    <updated>2017-06-20T15:10:09.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>media-compress</title>
    <link href="http://wodekouwei.com/2017/06/20/media-compress/"/>
    <id>http://wodekouwei.com/2017/06/20/media-compress/</id>
    <published>2017-06-20T15:09:04.000Z</published>
    <updated>2017-06-20T15:09:04.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>media-video</title>
    <link href="http://wodekouwei.com/2017/06/20/media-video/"/>
    <id>http://wodekouwei.com/2017/06/20/media-video/</id>
    <published>2017-06-20T15:08:35.000Z</published>
    <updated>2017-06-20T15:08:35.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>media-audio</title>
    <link href="http://wodekouwei.com/2017/06/20/media-audio/"/>
    <id>http://wodekouwei.com/2017/06/20/media-audio/</id>
    <published>2017-06-20T15:08:29.000Z</published>
    <updated>2017-06-20T15:08:29.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>多媒体技术(一)之图形图像</title>
    <link href="http://wodekouwei.com/2017/06/20/media-graphic/"/>
    <id>http://wodekouwei.com/2017/06/20/media-graphic/</id>
    <published>2017-06-20T15:07:54.000Z</published>
    <updated>2017-06-29T14:59:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-图形与图像的基本概念"><a href="#1-图形与图像的基本概念" class="headerlink" title="1.图形与图像的基本概念"></a>1.图形与图像的基本概念</h2><h3 id="1-1图形与图像的颜色模型"><a href="#1-1图形与图像的颜色模型" class="headerlink" title="1.1图形与图像的颜色模型"></a>1.1图形与图像的颜色模型</h3><h4 id="1-1-1颜色的基本概念"><a href="#1-1-1颜色的基本概念" class="headerlink" title="1.1.1颜色的基本概念"></a>1.1.1颜色的基本概念</h4><h5 id="1-1-1-1-物体的颜色"><a href="#1-1-1-1-物体的颜色" class="headerlink" title="1.1.1.1 物体的颜色"></a>1.1.1.1 物体的颜色</h5><p>物体的颜色不同是因为他们对光的吸收和反射的属性不同.物体的颜色是由该物体所反射的光的波长来决定的.</p>
<p>人眼看到的物体的颜色不仅取决于该物体所反射的光的波长,还与照射它的光源有关.如果用单一蓝色去照射绿色的树叶,
则此时的树叶只能是黑色的.因为蓝色光源中没有绿色成分,树叶吸收了全部蓝色而呈现黑色.</p>
<p>在彩色显示器中,为了使颜色具有较好的还原度和真实感,通常采用类似自然光作为照明光源.</p>
<h5 id="1-1-1-2-彩色三要素"><a href="#1-1-1-2-彩色三要素" class="headerlink" title="1.1.1.2 彩色三要素"></a>1.1.1.2 彩色三要素</h5><p>颜色信息对人的视觉反应,可通过色调,色饱和度和亮度这三个参量来表示:</p>
<ul>
<li>色调:用来描述颜色的不同类别的物理量称为色调,如红,橙,黄,绿,青,蓝,紫.色调取决于该种颜色的主要波长.</li>
<li>色饱和度:色饱和度则是描述颜色的深浅程度的物理量,它按该种颜色混入白光的比例来表示,当某色光的饱和度为100%时,就是表示该色光是完全没有混入白色光的单色光饱和度越高则颜色也越浓.如果大量混入白色光使饱和度降低,人视觉会感到颜色变淡.例如,在浓的的红色光中混入大量的白光,由于饱和度降低就变成了粉红色,但是因为红色是基本色,所以色调并不改变.在某颜色中混入白光与增强白光对某颜色物体的照射是不同的.前者是在摄入人眼的眸色光中混入白光,而后者的结果则是加强了某色物体的反射光的强度,在摄入人眼的反射光中并没有混入白光,因此它并没有改变该色的饱和度.</li>
<li>亮度:用来描述色光的明暗变化的强度的物理量成为亮度.亮度是色光能量的一种描述,是指色调和色饱和度已经固定的光,当它的全部能量增强时感觉明亮,否则感觉暗淡.</li>
</ul>
<p>色调和色饱和度统称为色度.</p>
<h5 id="1-1-1-3-三基色原理"><a href="#1-1-1-3-三基色原理" class="headerlink" title="1.1.1.3 三基色原理"></a>1.1.1.3 三基色原理</h5><p>三基色原理认为自然界中景物的绝大多数的彩色光,能分解为互相独立的红(R),绿(G),蓝(B)三种基色光;反之,用互相独立的红,绿,蓝3种基色光以不同的比例混合,可模拟出自然界中绝大多数景色的光.RGB三基色相互独立的含义是指,任一种基色都不能由另外两种基色混合而产生.三基色的选择并不是唯一的.</p>
<h5 id="1-1-1-4-像素-pixel"><a href="#1-1-1-4-像素-pixel" class="headerlink" title="1.1.1.4 像素(pixel)"></a>1.1.1.4 像素(pixel)</h5><p>像素是计算机图形与图像中能被单独处理的最小基本单元.
从像素的视觉属性看,它是一个最小可是单位.一幅彩色图像可以看成是由许多很小的可是点组成的,这个点就是像素.每个像素点都有确定的颜色和亮度,这个颜色就是有互相独立的红,绿,蓝三种基色光以不同的比例混合而成的.
从像素的量值属性看,它的数据结构应同时显示地址,色彩,亮度等数据信息,这些数据就称为像素值.</p>
<h4 id="1-1-2-颜色模型"><a href="#1-1-2-颜色模型" class="headerlink" title="1.1.2 颜色模型"></a>1.1.2 颜色模型</h4><p>颜色模型(或称色彩模型)就是定量颜色方法.在不同的领域应用于图像时,为了尽可能多地,有效地描述各种颜色,往往采用不同的颜色模型,例如,用显示器这类发光物体显示时采用的是RGB模型,用打印机这类吸光物体输出图像时用CMY模型,进行彩色电视信号的显示与传输时采用YUV模型,从事艺术绘画时习惯采用HSL模型.</p>
<h5 id="1-1-2-1-RGB模型"><a href="#1-1-2-1-RGB模型" class="headerlink" title="1.1.2.1 RGB模型"></a>1.1.2.1 RGB模型</h5><p>RGB模型也称为加色法混色模型.其混色规律是:以等量的红,绿,蓝基色光混合时:</p>
<ul>
<li>红 + 绿 = 黄色</li>
<li>红 + 蓝 = 品红色</li>
<li>绿 + 蓝 = 青色</li>
<li>红 + 绿 + 蓝 = 白色</li>
<li>3种基色光全无 = 黑色</li>
</ul>
<p>加色法的混色规律可以使用下图表示,3个圆分别红绿蓝3种基色,圆与圆的叠加区域表示以等量的基色相混合时所合成的颜色,其色调如该区域的文字表示.当3种基色等量相加时,就会得到白色.其中,又尝尝把品红色成为绿色的补色,青色称为红色的补色,黄色称为蓝色的补色.
<img src="http://images.wodekouwei.com/media/rgb.png" alt="image">
物体的颜色是丰富多彩的,任何一种颜色和这3种基色之间的关系可以用下面的配色方程式来描述:
F(物体颜色)=R(红色的百分比) + G(绿色的百分比) + B(蓝色的百分比)</p>
<p>由于人类的视觉特性,两种或3种基色产生混色效果,不一定要同时和同一空间位置混合.例如,在心理学实验中有一个色轮实验,它是在可旋转的圆盘上按扇形面积均等的分成3部分,并涂上3种基色,当圆盘慢慢旋转时能够分辨出3中基色,但当圆盘旋转的频率提高到闪光融合频率以上时,人眼不再能分辨出3种基色,而产生白颜色的感觉,以达到混色的效果.这就是 <strong>“时间混色法”</strong> .最初的顺序制彩色电视就应用了人的这一视觉效果.例如,很细小的红点和旅店均匀间置互相靠的很近,只有在近距离仔细观看才能区分出来,当观看距离很远时就只感觉到黄色的一篇了,这就是 <strong>“空间混色法”</strong> .目前广泛使用的彩色显像管以及大型LED真彩色广告屏就是利用了这一混色视觉效应.</p>
<p>在多媒体技术中,RGB颜色模型是最基本的模型,因为彩色显示器只有按RGB分量式输入,才能在显示屏幕上合成任意颜色.</p>
<h5 id="1-1-2-2-CMY模型"><a href="#1-1-2-2-CMY模型" class="headerlink" title="1.1.2.2 CMY模型"></a>1.1.2.2 CMY模型</h5><p>CMY(Cyan Magenta Yellow)模型是采用青,品红,黄色3种基本颜色按一定比例合成颜色的方法.CMY模型又称为减色法混色模型,因为色彩的显示不是直接来自于光线的色彩,而是光线被物体吸收掉一部分之后发射回来的剩余光线所产生的.光线都被吸收时称为黑色,当光线都被反射时成为白色.这种模式适合于彩色打印机等用彩色墨水或颜料进行混合显示的情况.</p>
<p>在相减混色中,当3种基本颜色等量相减时得到黑色;等量黄色(Y)和品红(M)相减而青色为0时,等到红色(R);等量青色(C)和品红(M)相减而黄色为0时,得到蓝色(B);等量黄色(Y)和青色(C)相减而品红(M)为0时,得到绿色(G).</p>
<p>由于颜料的化学特性,实际上等量的CMY混合后并不能产生真正的黑色,因此在印刷时通常再加上黑色(Black),这样又称为CMYK模式,四色印刷便是由此而来.</p>
<p>显然,RGB与CMY模型是颜色互补的模型,它们之间可以互相转换.如果按每个像素每种颜色用一位二进制数表示的话,RGB与CMY模型之间的颜色关系如下表所示.利用它们之间的关系,可以把显示器显示的颜色转换成打印的颜色.但实际上因为发射光与反射光的性质完全不同,显示器上的颜色不可能精确地在打印机上复制出来,因此实际的转换过程会利用一定的算法进行一定程度上的失真补偿.
|RGB|CMY|颜色|
|—|—|—|
|000|111|黑|
|001|110|蓝|
|010|101|绿|
|011|100|青|
|100|011|红|
|101|010|品红|
|110|001|黄|
|111|000|白|</p>
<h5 id="1-1-2-3-YUV与YIQ模型"><a href="#1-1-2-3-YUV与YIQ模型" class="headerlink" title="1.1.2.3 YUV与YIQ模型"></a>1.1.2.3 YUV与YIQ模型</h5><p>在彩色电视系统中不采用RGB颜色模型,而采用YUV或YIQ模型表示彩色图像.YUV适用于PAL(Phase Altermation Line,同行倒相制式)和SECAM(法文)(Sequential C欧了让Memo,顺序传送彩色与存储制式)彩色电视制式,而YIQ适用于美国国家电视标准委员会(NTSC,National Television System Committee)彩色电视制式.</p>
<p>Y是亮度信号,U和V则是两个色差信号,分别传送红基色分量和蓝基色分量与亮度分量的差值信号.</p>
<p>采用YUV颜色模型的好处是:其一,亮度信号Y解决了彩色电视与黑白电视的兼容性问题;其二,由于人眼对颜色细节的分辨率低于对亮度细节的分辨率,所以可以用一个通道来传送,Y,U,V这3个信号,给亮度信号较大的带宽(6MHz)来传送图像的细节,而给色差信号较小的带宽(1.3MHz)来进行大面积涂色.这样,总的传输数据量和RGB模型相比,要明显小一些,起到了一种数据压缩节省存储空间的作用,而对于这种数据压缩带来的画面变化人眼一般是感觉不到的.</p>
<p>电视系统通常采用摄像机把摄得的彩色图像信号经分色,放大和校正分成RGB这3个分量的信号,再经过矩阵变换电路将彩色信号分解成亮度信号Y和色差信号,U,V,而后对其进行编码,用同一信道发送出去.接收端再通过编码及矩阵逆变换还原成3个基色显示.</p>
<p>在NTSC彩色电视制式中使用YIQ模型,其特性与YUV模型相近.其中的Y表示亮度,I,Q也是两个色差分量,但它们在色度矢量图中与U,V的位置不同.Q,I正交坐标轴与U,V正交坐标轴之间有33度夹角,如图所示:</p>
<p><img src="http://images.wodekouwei.com/media/yuv_yiq.bmp" alt="image"></p>
<p>I,Q与U,V之间的关系如下:
I = Vcos33 - Usin33
Q = Vsin33 + Ucos33</p>
<p>人眼的彩色视觉特性表明,人眼分辨红与黄之间的颜色变化的能力最强,而分辨蓝,紫之间颜色变化的能力最弱.因此YIQ模型在色度矢量图中,选择I轴正好处于夹角123度处,即人眼具有最大彩色分辨率的红与黄之间的橙色和青色(相角为303度)处,选择与I轴正交的色度信号轴为Q轴(相角为33度),正是人眼最不敏感的色轴位置.因而YIQ模型传送分辨力较强的I信号时,用较宽的频带(1.3MHz~1.5MHz),传送分辨力弱的Q信号时,可用较窄的频带(0.5MHz),这就可以在保证较好的颜色传输特性情况下,最大限度地节省存储空间.</p>
<h5 id="1-1-2-4-HSI颜色模型"><a href="#1-1-2-4-HSI颜色模型" class="headerlink" title="1.1.2.4 HSI颜色模型"></a>1.1.2.4 HSI颜色模型</h5><p>HSI或HSL是Hue Saturation Intensity(Lightness)的英文缩写,颜色模型用H,S,I这三个参数描述颜色特性,其中H定义颜色的波长,称为色调;S表示颜色的深浅程度,称为饱和度;I表示强度或亮度,这正是颜色的三要素.</p>
<p>HSI模型更接近人对颜色更接近人对彩色的认识,符合人眼对颜色的感知方式,是一种从事艺术绘画的画家们习惯使用的描述色彩的方法.它比RGB模型使用更方便,从而能减少彩色图像处理的复杂性,增加快速性,因此一般的图像处理软件中,都提供了这种定量色彩的方式.</p>
<h4 id="1-1-3-颜色模型的转换"><a href="#1-1-3-颜色模型的转换" class="headerlink" title="1.1.3 颜色模型的转换"></a>1.1.3 颜色模型的转换</h4><p>无论采用什么颜色模型来表示彩色图形与图像,由于所有的显示器都需要RGB值来驱动,所以在显示每个像素之前,必须要把彩色分量值转换成RGB值.</p>
<h5 id="1-1-3-1-YUV与RGB颜色模型变换"><a href="#1-1-3-1-YUV与RGB颜色模型变换" class="headerlink" title="1.1.3.1 YUV与RGB颜色模型变换"></a>1.1.3.1 YUV与RGB颜色模型变换</h5><p>RGB与YUV的对应关系可以近似地用下面的方程表示:</p>
<ul>
<li>Y = 0.299R + 0.587G + 0.114B</li>
<li>U = -0.147R - 0.289G + 0.436B</li>
<li>V = 0.615R - 0.515G - 0.096B<h5 id="1-1-3-2-YIQ与RGB颜色模型变换"><a href="#1-1-3-2-YIQ与RGB颜色模型变换" class="headerlink" title="1.1.3.2 YIQ与RGB颜色模型变换"></a>1.1.3.2 YIQ与RGB颜色模型变换</h5>YIQ与RGB的对应关系可以近似地用下面的方程表示:</li>
<li>Y = 0.229R + 0.587G + 0.114B</li>
<li>I = 0.596R - 0.275G - 0.321B</li>
<li>Q = 0.212R - 0.523G + 0.311B</li>
</ul>
<h5 id="HSI-与-RGB颜色变换"><a href="#HSI-与-RGB颜色变换" class="headerlink" title="HSI 与 RGB颜色变换"></a>HSI 与 RGB颜色变换</h5><p>HSI与RGB空间的转换关系可以用下面的方程表示:</p>
<ul>
<li>H = [90 - arctan(F/sqr(3)) + [0, G&gt;B;180,G&lt;B]]/360</li>
<li>S = 1 - min(R,G,B)/I</li>
<li>I = (R+G+B)/3
其中,F=(2R-G-B)/(G-B),sqr为求平方根</li>
</ul>
<h3 id="1-2图形与图像的基本属性"><a href="#1-2图形与图像的基本属性" class="headerlink" title="1.2图形与图像的基本属性"></a>1.2图形与图像的基本属性</h3><p>一幅彩色图像可以看成二维连续函数f(x,y),其彩色幅度是位置(x,y)的函数.计算机多媒体技术从其图像的生成,显示,处理和存储的机制出发,需要对彩色图像数字化.数字化一幅彩色图像就是要把连续函数f(x,y)在空间的坐标和彩色幅度进行离散和量化.空间坐标x,y的离散化通常以分辨率来表征,而彩色幅度的离散化则由像素的颜色深度来表征.</p>
<h4 id="1-2-1分辨率"><a href="#1-2-1分辨率" class="headerlink" title="1.2.1分辨率"></a>1.2.1分辨率</h4><p>分辨率是一个统称,分为显示分辨率,图像分辨率,扫描分辨率和打印分辨率等.</p>
<h5 id="1-2-1-1显示分辨率"><a href="#1-2-1-1显示分辨率" class="headerlink" title="1.2.1.1显示分辨率"></a>1.2.1.1显示分辨率</h5><p>是指某一种显示方式下,显示屏上能够显示出的像素数目,以水平和垂直的像素表示.例如,显示分辨率为640*480表示显示屏分成480行,每行显示640个像素,整个显示屏就含有307200个显像点.屏幕上的像素越多,分辨率就越高,显示出来的图像也就越细腻,显示的图像质量也就约高.屏幕能够显示的最大像素数目越多,也说明显示设备的最大分辨率越高.显示屏上的每个彩色像素由代表R,G,B这3种模拟信号的相对强度决定,这些彩色像素点就构成一幅彩色图像.</p>
<h5 id="1-2-1-2图像分辨率"><a href="#1-2-1-2图像分辨率" class="headerlink" title="1.2.1.2图像分辨率"></a>1.2.1.2图像分辨率</h5><p>图像分辨率指数字化图像的大小,以水平和垂直的像素数表示.如果组成图像的像素数目越多,则说明图像的分辨率越高,看起来就越逼真,图像分辨率实际上决定了图像的显示质量,也就是说,即使提高了显示分辨率,也无法真正改善图像的质量.图像分辨率与显示分辨率是两个不同的概念.图像分辨率的确定组成一幅图像的像素数目,而显示分辨率是确定显示图像的区域大小.当图像分辨率与屏幕分辨率一致时,图像正好占据满屏;当图像分辨率小于屏幕分辨率时,图像占据屏幕的一部分;当图像分辨率大于屏幕分辨率时,则屏幕仅能显示图像的一部分.</p>
<h5 id="1-2-1-3扫描分辨率和打印分辨率"><a href="#1-2-1-3扫描分辨率和打印分辨率" class="headerlink" title="1.2.1.3扫描分辨率和打印分辨率"></a>1.2.1.3扫描分辨率和打印分辨率</h5><p>在用于扫描仪扫描图像时,通常要指定扫描的分辨率,用每英寸包含的点(d/i,dots per inch)表示.如果用300d/i来扫描一幅8<em>6的彩色图像,就得到一幅2400</em>1800个像素的图像.分辨率越高,像素就越多.</p>
<p>打印分辨率是指图像打印时每英寸可识别的点数,也使用d/i(dots per inch)为衡量单位.两种分辨率之间是有区别的,扫描分辨率反映了扫描后的图像与原始图像之间的差异程度,分辨率越高,差异越小.打印分辨率反映了打印的图像与原数字图像之间的差异程度,分辨率越接近原图像的分辨率,打印质量越高.两种分辨率的最高值都受到设备的限制.</p>
<h4 id="1-2-2颜色深度"><a href="#1-2-2颜色深度" class="headerlink" title="1.2.2颜色深度"></a>1.2.2颜色深度</h4><p>颜色深度是指图像中每个像素的颜色(或亮度)信息所占的二进制数位数,记做位/像素(b/p,bits per pixel).屏幕上的每一个像素都占有一个或多个位,用于存放与它相关的颜色信息.颜色深度决定了构成图像的每个像素可能出现的最大颜色数,因而颜色深度值越高,显示的图像色彩越丰富.反之,颜色深度太浅,会影响图像的质量,图像看起来让人觉得很粗糙和很不自然.常见颜色深度有一下5种:</p>
<ol>
<li>4bit:这是VGA标准支持的颜色深度,共2的四次方16种颜色;</li>
<li>8bit:这是多媒体应用中的最低颜色深度,共2的8次方256种颜色,称为索引彩色图(由颜色查找决定);</li>
<li>16bit:在16bit中,用其中的15bit表示RGB这3种颜色,每种颜色5bit,用剩余的以为表示图像的其他属性,如透明度.所以16bit的颜色深度实际可以表示为2的15次方32<em>32</em>32共32768种颜色.称为HI-Color(高彩色)图像.</li>
<li>24bit:用3个8bit分别表示RGB,可生成的颜色数2的24次16777216种,约16M种颜色,这已经成为真彩色;</li>
<li>32bit:同24bit颜色深度一样,也是用3个bit分别表示RGB这三种颜色,剩余的8bit用来表示图像的其他属性,如透明度等.</li>
</ol>
<p>虽然像素的颜色颜色深度值越大图像色彩越丰富,但由于设备的限制,人眼分辨率的限制,不一定要追求特别深的颜色深度,一般来说,32bit的颜色深度已经足够.此外,像素颜色深度越深,所占用的存储空间越大.</p>
<p>一个像素的颜色深度位数除R,G,B分量占用固定bit数表示颜色外,一般要腾出1bit或几bit作为属性(Attribute)位.属性位用来指定该像素应具有的性质.例如,像素的颜色深度为32bit时,R,G,B分别用8bit表示,那么余下的8bit常称为a通道(Alpha Channel)位,或称为覆盖(Overlay)位,中断位,属性位,它用来控制该像素点的透明度.假如定义一个像素值(A,R,G,B)的4个分量(其中A为Alpha属性位数值)都用归一化的数值表示,那么像素91,1,0,0)时显示红色.当像素为(0.5,1,0,0)时,预乘的结果就变成了(0.5.0.5,0,0),这表示现在显示的红色的强度减低一半.用这种定义像素属性的办法可以实现两幅彩色图像之间的透明叠加效果.当Alpha数值很小时,渲染出来的效果是几乎透明的,如玻璃;当Alpha数值处于中间的位置时,则可以得到一种半透明的效果;当Alpha数值接近255时,是几乎不透明的效果.这种属性位的加入为实现透明和半透明的显示掉过带来了方便.</p>
<h4 id="1-2-3文件的大小"><a href="#1-2-3文件的大小" class="headerlink" title="1.2.3文件的大小"></a>1.2.3文件的大小</h4><h3 id="1-3图形与图像的基本类型"><a href="#1-3图形与图像的基本类型" class="headerlink" title="1.3图形与图像的基本类型"></a>1.3图形与图像的基本类型</h3><h2 id="2-图形与图像的处理"><a href="#2-图形与图像的处理" class="headerlink" title="2.图形与图像的处理"></a>2.图形与图像的处理</h2><h3 id="2-1图形与图像的获取"><a href="#2-1图形与图像的获取" class="headerlink" title="2.1图形与图像的获取"></a>2.1图形与图像的获取</h3><h3 id="2-2图形与图像的存储"><a href="#2-2图形与图像的存储" class="headerlink" title="2.2图形与图像的存储"></a>2.2图形与图像的存储</h3><h3 id="2-3图形与图像的显示"><a href="#2-3图形与图像的显示" class="headerlink" title="2.3图形与图像的显示"></a>2.3图形与图像的显示</h3><h3 id="2-4图形与图像的处理"><a href="#2-4图形与图像的处理" class="headerlink" title="2.4图形与图像的处理"></a>2.4图形与图像的处理</h3><h2 id="3-计算机动画"><a href="#3-计算机动画" class="headerlink" title="3.计算机动画"></a>3.计算机动画</h2><h3 id="3-1计算机动画的原理"><a href="#3-1计算机动画的原理" class="headerlink" title="3.1计算机动画的原理"></a>3.1计算机动画的原理</h3><h3 id="3-2计算机动画的类型"><a href="#3-2计算机动画的类型" class="headerlink" title="3.2计算机动画的类型"></a>3.2计算机动画的类型</h3><h3 id="3-3计算机动画的制作"><a href="#3-3计算机动画的制作" class="headerlink" title="3.3计算机动画的制作"></a>3.3计算机动画的制作</h3><h3 id="3-4虚拟现实动画技术"><a href="#3-4虚拟现实动画技术" class="headerlink" title="3.4虚拟现实动画技术"></a>3.4虚拟现实动画技术</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-图形与图像的基本概念&quot;&gt;&lt;a href=&quot;#1-图形与图像的基本概念&quot; class=&quot;headerlink&quot; title=&quot;1.图形与图像的基本概念&quot;&gt;&lt;/a&gt;1.图形与图像的基本概念&lt;/h2&gt;&lt;h3 id=&quot;1-1图形与图像的颜色模型&quot;&gt;&lt;a href=&quot;#
    
    </summary>
    
      <category term="media" scheme="http://wodekouwei.com/categories/media/"/>
    
    
      <category term="多媒体" scheme="http://wodekouwei.com/tags/%E5%A4%9A%E5%AA%92%E4%BD%93/"/>
    
  </entry>
  
  <entry>
    <title>mac下打包qt程序成dmg</title>
    <link href="http://wodekouwei.com/2017/06/18/tips-qt-pack/"/>
    <id>http://wodekouwei.com/2017/06/18/tips-qt-pack/</id>
    <published>2017-06-18T11:06:08.000Z</published>
    <updated>2017-06-18T11:09:50.000Z</updated>
    
    <content type="html"><![CDATA[<ol>
<li>参考<a href="http://doc.qt.digia.com/4.7-snapshot/appicon.html" target="_blank" rel="external">http://doc.qt.digia.com/4.7-snapshot/appicon.html</a></li>
<li>图片格式在线转换<a href="http://iconverticons.com/，可以生成icns格式图片。" target="_blank" rel="external">http://iconverticons.com/，可以生成icns格式图片。</a></li>
<li>Test.pro中添加macx{ICON = Test.icns} （记得把Test.icns添加到工程中）</li>
<li>发布dmb包</li>
</ol>
<ul>
<li>QT在mac下有个发布命令：<code>macdeployqt</code></li>
<li>我的mac上macdeployqt目录如下：<code>/Users/duobianxing/QtSDK/Desktop/Qt/4.8.1/gcc/bin</code></li>
<li>将macdeployqt的路径添加到环境变量里面
终端里 <code>vim /etc/profile</code>（如果在保存时有问题，可以用 <code>sudo vim /etc/profile</code>）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#在最后添加一行，如下所示：</div><div class="line"># System-wide .profile for sh(1)</div><div class="line"></div><div class="line">if [ -x /usr/libexec/path_helper ]; then</div><div class="line"></div><div class="line">        eval `/usr/libexec/path_helper -s`</div><div class="line"></div><div class="line">fi</div><div class="line"></div><div class="line">if [ &quot;$&#123;BASH-no&#125;&quot; != &quot;no&quot; ]; then</div><div class="line"></div><div class="line">        [ -r /etc/bashrc ] &amp;&amp; . /etc/bashrc</div><div class="line"></div><div class="line">fi</div></pre></td></tr></table></figure>
</li>
</ul>
<p><code>export PATH=/Users/duobianxing/QtSDK/Desktop/Qt/4.8.1/gcc/bin:$PATH</code></p>
<ul>
<li>cd进入到Test.app所在目录，然后执行<code>macdeployqt extractor.app -verbose=1 -dmg</code>，即可生成Test.dmg包。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;参考&lt;a href=&quot;http://doc.qt.digia.com/4.7-snapshot/appicon.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://doc.qt.digia.com/4.7-snapshot/a
    
    </summary>
    
    
      <category term="QT" scheme="http://wodekouwei.com/tags/QT/"/>
    
      <category term="MAC" scheme="http://wodekouwei.com/tags/MAC/"/>
    
  </entry>
  
  <entry>
    <title>mpeg编码seekto方法精确定位到指定帧</title>
    <link href="http://wodekouwei.com/2017/06/18/tips-seekto/"/>
    <id>http://wodekouwei.com/2017/06/18/tips-seekto/</id>
    <published>2017-06-18T10:11:40.000Z</published>
    <updated>2017-06-18T10:28:23.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://developer.android.com/intl/es/reference/android/media/MediaExtractor.html" target="_blank" rel="external">MediaExtractor</a>有一个方法如下:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">//All selected tracks seek near the requested time according to the specified mode.</div><div class="line">public void seekTo (long timeUs, int mode)</div></pre></td></tr></table></figure></p>
<p>timeUs是要seek的时间戳，mode是seek的模式，可以是SEEK_TO_PREVIOUS_SYNC, SEEK_TO_CLOSEST_SYNC, SEEK_TO_NEXT_SYNC，分别是seek指定帧的上一帧，最近帧和下一帧。
此方法可用于视频播放时动态定位播放帧，用于动态改变视频播放进度，比如使用seekBar来跟踪视频播放进度，同时可拖动来动态改变播放进度。</p>
<p>mpeg编码决定seekTo方法无法精确定位到指定帧。即使使用的是某一帧精确的时间戳作为seekTo方法的输入参数也无法实现精确定位。</p>
<p>在google, stackoverlfow查询得出的结论是：在每次seekTo方法调用后，MediaCodec必须从关键帧开始解码。因此seekTo方法只会seek到最近的／上一个／下一个关键帧，也就是I-Frame(key frame = I frame = sync frame)。之所以要从关键帧开始解码，是因为每一帧不一定是单独编码的，只有I frame才是帧内编码，而P, B frame都是要参考别的帧来进行编码，因此单独拿出来是不完整的一帧。</p>
<p>stackoverflow上有人对此的做法是：seekTo的输入参数mode设置为SEEK_TO_PREVIOUS_SYNC,即seek的是指定帧的上一个关键帧。然后判断当前的时间戳是否小于定位关键帧的时间戳，如果是就调用MediaExtractor的advance方法，“快进”到指定帧。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">extractor.seekTo(expectedPts, MediaExtractor.SEEK_TO_PREVIOUS_SYNC);  </div><div class="line">while (currentPts &lt; expectedPts) &#123;  </div><div class="line">    extractor.advance();  </div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>但是这个方法仍不理想，如果seek的位置和当前位置比较远的话，会有一定延迟。而且视频内容偶尔会出现不完整的帧的闪现。
经过一段时间的研究，终于解决了这个问题，现在播放时可以根据seekBar随时拖动到视频任何一帧，不会有任何延迟，甚至可以实现倒播了。
因为之前播放视频的是自己用MediaCodec, MediaMuxer等编码合成的视频文件，在编码参数设置的时候，将关键帧间隔KEY_I_FRAME_INTERVAL设置为了1（因为要求参数为整数）。注意这个参数的单位是秒，而不是帧数！网上看到很多例子包括fadden的bigflake和Grafika上都将这里设置为了20几。搞得我一开始还以为是每隔二十几帧就有一个关键帧。如果设置为20几，那么就是说你用MediaCodec编码录制一段20多秒的视频，只有开头的一个关键帧！剩下的都是P或者B帧。
这显然是不合理的。一般来说是每隔1秒有一个关键帧，这样就可以seek到对应秒的关键帧。或者说1秒内如果有30帧，那么这30帧至少有一个关键帧。因此我将KEY_I_FRAME_INTERVAL设置为了1。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mediaFormat.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 1);</div></pre></td></tr></table></figure></p>
<p>这也是为什么我在播放用这种参数编码的视频的时候，使用seekTo方法不能准确定位帧了。之前有讲，seekTo是定位到关键帧的，如果不是关键帧，那么它会去找上一个／最近一个／下一个关键帧，这取决于你输入参数mode的设置。
因此如果想使用seekBar准确拖动定位到任何一帧播放，必须保证每一帧都是关键帧。
于是，我将KEY_I_FRAME_INTERVAL设置为了0：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mediaFormat.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 0);</div></pre></td></tr></table></figure></p>
<p>事实也证明这样可以保证录制的每一帧都是关键帧，因此在使用seekTo方法的时候终于可以准确定位任何一帧了。拖动seekBar的时候视频内容也会立刻改变，无论是往前还是往后，都不会有任何延迟和画面不完整的情况.
但是，把视频每一帧都设置为关键帧是否合理呢？是否会占太大空间呢？
带着这个疑问，我使用ffmpeg查看了我测试使用的手机（Lenovo X2)内置相机录制的视频。
只需一行代码：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ffprobe -show_frames video.mp4  &gt; frames.txt</div></pre></td></tr></table></figure></p>
<p>打开frames.txt可以看到每一帧的key_frame=1，表示是关键帧</p>
<p>这说明了手机本来录像就是把每一帧都作为关键帧的。
当然，不能以偏概全，于是我使用iPhone 6s录制的一段普通视频和慢动作视频。使用ffmpeg查看，发现每一帧也都是关键帧（慢动作视频1秒有240帧也都全部作为关键帧也是蛮拼的）。</p>
<p>目前我测试的两部手机都是如此，具体为什么手机录的视频每一帧都是关键帧我也不明白。而视频文件体积大小和是否将每一帧设为关键帧似乎不成线性关系，所以将KEY_I_FRAME_INTERVAL设置为0的方案是可行的。
因此只要保证视频每帧都是关键帧，那么seekTo方法就可以精确定位指定帧了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://developer.android.com/intl/es/reference/android/media/MediaExtractor.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;MediaExtractor&lt;
    
    </summary>
    
      <category term="media" scheme="http://wodekouwei.com/categories/media/"/>
    
    
      <category term="多媒体" scheme="http://wodekouwei.com/tags/%E5%A4%9A%E5%AA%92%E4%BD%93/"/>
    
      <category term="Android" scheme="http://wodekouwei.com/tags/Android/"/>
    
      <category term="Mpeg" scheme="http://wodekouwei.com/tags/Mpeg/"/>
    
  </entry>
  
  <entry>
    <title>多媒体技术简介</title>
    <link href="http://wodekouwei.com/2017/06/07/media-intro/"/>
    <id>http://wodekouwei.com/2017/06/07/media-intro/</id>
    <published>2017-06-07T14:28:19.000Z</published>
    <updated>2017-06-07T14:31:33.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="关键帧间隔"><a href="#关键帧间隔" class="headerlink" title="关键帧间隔"></a>关键帧间隔</h4><p>关键帧包含了显示帧需要的所有信息</p>
<p>所有的视频都至少包含一个关键帧，作为文件的第一个帧。其它的关键帧可用来改善视频的质量，不过它们也会增加文件大小。一般而言，每一秒视频至少需要使用 1 个关键帧。若使用此公式，在每秒播放 25个帧的视频中，每 25 个帧就会有 1 个关键帧。增加关键帧个数可改善质量，但是同时增加带宽和网络负载。</p>
<p>两种彩电视频制式：</p>
<ul>
<li>NTSC (525 lines @ 59.94 Hz)  29.97 fps</li>
<li>PAL (625 lines @ 50 Hz)  25 fps</li>
</ul>
<p>NTSC和PAL属于全球两大主要的电视广播制式，但是由于系统投射颜色影像的频率而有所不同。NTSC是National Television System Committee的缩写，其标准主要应用于日本、美国，加拿大、墨西哥等等，PAL 则是Phase Alternating Line的缩写，主要应用于中国，香港、中东地区和欧洲一带。</p>
<p>GOP最大可含帧数目：18 (NTSC) / 15 (PAL)</p>
<p>GOP是由固定模式的一系列I帧、P帧、B帧组成。
I帧编码是为了减少空间域冗余，P帧和B帧是为了减少时间域冗余。
常用的结构由15个帧组成，具有以下形式IBBPBBPBBPBBPBB。简称GOP(4,2)，指的是该图像组除了一个I帧外，包含了4个P帧，并且任何两个P帧或者I、P之间都有两个B帧。</p>
<p>GOP（Group of Pictures）策略影响编码质量：所谓GOP，意思是画面组，一个GOP就是一组连续的画面。MPEG编码将画面（即帧）分为I、P、B三种，I是内部编码帧，P是前向预测帧，B是双向内插帧。简单地讲，I帧是一个完整的画面，而P帧和B帧记录的是相对于I帧的变化。没有I帧，P帧和B帧就无法解码，这就是MPEG格式难以精确剪辑的原因，也是我们之所以要微调头和尾的原因。</p>
<p>MPEG-2 帧结构
　　MPEG-2压缩的帧结构有两个参数，一个是GOP（Group Of Picture）图像组的长度，一般可按编码方式从1－15；另一个是I帧和P帧之间B帧的数量，一般是1－2个。前者在理论上记录为N，即多少帧里面出现一次I帧；后者描述为多少帧里出现一次P帧，记录为M。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;关键帧间隔&quot;&gt;&lt;a href=&quot;#关键帧间隔&quot; class=&quot;headerlink&quot; title=&quot;关键帧间隔&quot;&gt;&lt;/a&gt;关键帧间隔&lt;/h4&gt;&lt;p&gt;关键帧包含了显示帧需要的所有信息&lt;/p&gt;
&lt;p&gt;所有的视频都至少包含一个关键帧，作为文件的第一个帧。其它的关键帧可用
    
    </summary>
    
      <category term="media" scheme="http://wodekouwei.com/categories/media/"/>
    
    
      <category term="多媒体" scheme="http://wodekouwei.com/tags/%E5%A4%9A%E5%AA%92%E4%BD%93/"/>
    
      <category term="音视频" scheme="http://wodekouwei.com/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"/>
    
  </entry>
  
  <entry>
    <title>jni介绍</title>
    <link href="http://wodekouwei.com/2017/06/07/jni-intro/"/>
    <id>http://wodekouwei.com/2017/06/07/jni-intro/</id>
    <published>2017-06-07T14:23:23.000Z</published>
    <updated>2017-06-07T14:27:53.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-C回调JAVA"><a href="#1-C回调JAVA" class="headerlink" title="1.C回调JAVA"></a>1.C回调JAVA</h3><ol>
<li><p>c中返回一个字符串</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">（*env）-&gt;NewStringUTF(env,&quot;Huazi 华仔&quot;);</div></pre></td></tr></table></figure>
</li>
<li><p>c中返回一个数组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">.....................  </div><div class="line">  int i = 0;  </div><div class="line">  jintArray array;  </div><div class="line">  array =(*env)-&gt;NewIntArray(env,8);  </div><div class="line">  for(;i&lt;8;i++)  </div><div class="line">  </div><div class="line">// 赋值成 0 ~ 7  </div><div class="line">      (*env)-&gt;SetObjectArrayElement(env,array,i,i);  </div><div class="line">  &#125;  </div><div class="line">  return array;</div></pre></td></tr></table></figure>
</li>
<li><p>c中使用调用传入的参数是数组array 是传入的数组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">.........  </div><div class="line"> int sum =0, i;  </div><div class="line"> int len = (*env)-&gt;GetArrayLength(env,array);  </div><div class="line"> jint *element =(*env)-&gt;GetIntArrayElement(env,array,0);  </div><div class="line"> for(i=0;i&lt;len;i++)  </div><div class="line"> &#123;  </div><div class="line">     sum+= *(element+i);  </div><div class="line"> &#125;  </div><div class="line"> return sum;</div></pre></td></tr></table></figure>
</li>
<li><p>c中调用java中类的方法 没有参数 只有返回值String</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">//()Ljava/lang/String;&quot; 表示参数为空 返回值是String类型  </div><div class="line">JNIEXPORT jstring JNICALLJava_com_huazi_Demo_getCallBack(JNIENV env,jobject object)&#123;  </div><div class="line">  jmethodID mid;  </div><div class="line">  jclass cls =(*env)-&gt;FindClass(env,&quot;com/huazi/Demo&quot;); //后面是包名+类名  </div><div class="line"> mid =(*env)-&gt;GetMethodID(env,cls,&quot;TestMethod&quot;,&quot;()Ljava/lang/String;&quot;);//TestMethod java中的方法名  </div><div class="line"> jstring msg =(*env)-&gt;CallObjectMethod(env,object,mid); //object 注意下是jni传过来的jobject  </div><div class="line"> return msg;</div></pre></td></tr></table></figure>
</li>
<li><p>c中调用java中类的静态方法 没有参数 只有返回值String</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//@&quot;()Ljava/lang/String;&quot; 表示参数为空 返回值是String类型</div><div class="line">JNIEXPORT jstring JNICALLJava_com_huazi_Demo_getCallBack(JNIENV env,jobject object)&#123;  </div><div class="line">    jmethodID mid;  </div><div class="line">    jclass cls =(*env)-&gt;FindClass(env,&quot;com/huazi/Demo&quot;); //后面是包名+类名  </div><div class="line">   mid =(*env)-&gt;GeStatictMethodID(env,cls,&quot;TestMethod&quot;,&quot;()Ljava/lang/String;&quot;);// TestMethod java中的方法名  </div><div class="line">   jstring msg =(*env)-&gt;CallStaticObjectMethod(env,cls,mid); //object 注意下是jni传过来的jobject  </div><div class="line">   return msg;  </div><div class="line">&#125;</div></pre></td></tr></table></figure></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-C回调JAVA&quot;&gt;&lt;a href=&quot;#1-C回调JAVA&quot; class=&quot;headerlink&quot; title=&quot;1.C回调JAVA&quot;&gt;&lt;/a&gt;1.C回调JAVA&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;c中返回一个字符串&lt;/p&gt;
&lt;figure class=&quot;high
    
    </summary>
    
    
      <category term="Android" scheme="http://wodekouwei.com/tags/Android/"/>
    
      <category term="JNI" scheme="http://wodekouwei.com/tags/JNI/"/>
    
  </entry>
  
  <entry>
    <title>FFMPEG编译之Mac</title>
    <link href="http://wodekouwei.com/2017/06/07/ffmpeg-compile-mac/"/>
    <id>http://wodekouwei.com/2017/06/07/ffmpeg-compile-mac/</id>
    <published>2017-06-07T13:28:32.000Z</published>
    <updated>2017-06-07T14:19:36.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Mac下FFMPEG使用"><a href="#Mac下FFMPEG使用" class="headerlink" title="Mac下FFMPEG使用"></a>Mac下FFMPEG使用</h3><p>There are a few ways to get FFmpeg on OS X.</p>
<ol>
<li>One is to build it yourself. Compiling on Mac OS X is as easy as any other *nix machine, there are just a few caveats(警告). The general procedure is get the source, then ./configure <flags>; make &amp;&amp; sudo make install, though specific configure flags are possible.</flags></li>
<li>Another is to use some “build helper” tool, to install it for you. For example, homebrew or macports, see the homebrew section in this document.</li>
<li>Alternatively, if you are unable to compile, or do not want to install homebrew, you can simply download a static build for OS X, but it may not contain the features you want. Typically this involves unzipping an FFmpeg distribution file [like .zip file], then running it from within the newly extracted files/directories.</li>
</ol>
<h3 id="手动编译FFMPEG"><a href="#手动编译FFMPEG" class="headerlink" title="手动编译FFMPEG"></a>手动编译FFMPEG</h3><h4 id="1-下载FFMPEG源码"><a href="#1-下载FFMPEG源码" class="headerlink" title="1.下载FFMPEG源码"></a>1.下载FFMPEG源码</h4><p>使用<code>git clone https://github.com/FFmpeg/FFmpeg</code>从github下载ffmpeg源码,切换到要使用的目标分支(这里使用release/3.3):<code>git checkout -b r3.3 origin/release/3.3</code>,或者直接从github下载分支<code>release/3.3</code>的压缩包,解压.</p>
<h4 id="2-准备Xcode"><a href="#2-准备Xcode" class="headerlink" title="2.准备Xcode"></a>2.准备Xcode</h4><p>Starting with Lion 10.7, Xcode is available for free from the Mac App Store and is required to compile anything on your Mac. Make sure you install the Command Line Tools from Preferences &gt; Downloads &gt; Components. Older versions are still available with an AppleID and free Developer account at ​developer.apple.com.</p>
<h4 id="3-准备HomeBrew工具"><a href="#3-准备HomeBrew工具" class="headerlink" title="3.准备HomeBrew工具"></a>3.准备HomeBrew工具</h4><p>To get ffmpeg for OS X, you first have to install ​Homebrew. If you don’t want to use Homebrew, see the section below.
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</div></pre></td></tr></table></figure></p>
<p>Then:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">brew install automake fdk-aac git lame libass libtool libvorbis libvpx \</div><div class="line">opus sdl shtool texi2html theora wget x264 x265 xvid yasm</div></pre></td></tr></table></figure></p>
<p>Mac OS X Lion comes with Freetype already installed (older versions may need ‘X11’ selected during installation), but in an atypical location: /usr/X11. Running freetype-config in Terminal can give the locations of the individual folders, like headers, and libraries, so be prepared to add lines like CFLAGS=<code>freetype-config --cflags</code> LDFLAGS=<code>freetype-config --libs</code> PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig:/usr/lib/pkgconfig:/usr/X11/lib/pkgconfig before ./configure or add them to your $HOME/.profile file.</p>
<h4 id="4-编译"><a href="#4-编译" class="headerlink" title="4.编译"></a>4.编译</h4><p>Once you have compiled all of the codecs/libraries you want, you can now download the FFmpeg source either with Git or the from release tarball links on the website. Study the output of ./configure –help and make sure you’ve enabled all the features you want, remembering that –enable-nonfree and –enable-gpl will be necessary for some of the dependencies above. A sample command is:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">git clone http://source.ffmpeg.org/git/ffmpeg.git ffmpeg</div><div class="line">cd ffmpeg</div><div class="line">./configure  --prefix=/usr/local/ffmpeg --enable-gpl --enable-nonfree --enable-libass \</div><div class="line">--enable-libfdk-aac --enable-libfreetype --enable-libmp3lame \</div><div class="line">--enable-libtheora --enable-libvorbis --enable-libvpx --enable-libx264 --enable-libx265 --enable-libopus --enable-libxvid</div><div class="line">make &amp;&amp; sudo make install</div></pre></td></tr></table></figure></p>
<p><code>--prefix</code>指定编译完成后安装路径,这里指定到<code>/usr/local/ffmpeg</code>,安装完成会在<code>/usr/local/ffmpeg</code>下生成:bin,include,lib,share四个目录</p>
<h3 id="安装环境介绍"><a href="#安装环境介绍" class="headerlink" title="安装环境介绍"></a>安装环境介绍</h3><p>A package consists of several related files which are installed in several directories. The configure step usually allows the user to specify the so-called install prefix, and is usually specified through the configure option configure –prefix=PREFIX, where PREFIX usually is by default /usr/local. The prefix specifies the common directory where all the components are installed.</p>
<p>The following directories are usually involved in the installation:</p>
<ul>
<li><code>PREFIX/bin</code>: contains the generated binaries (e.g. ffmpeg, ffplay, ffprobe etc. in the case of FFmpeg)</li>
<li><code>PREFIX/include</code>: contains the library headers (e.g. libavutil/avstring.h, libavcodec/avcodec.h, libavformat/avformat.h etc. in case of FFmpeg) required to compile applications linked against the package libraries</li>
<li><code>PREFIX/lib</code>: contains the generated libraries (e.g. libavutil, libavcodec, libavformat etc. in the case of FFmpeg)</li>
<li><code>PREFIX/share</code>: contains various system-independent components; especially documentation files and examples
By specifying the prefix it is possible to define the installation layout.</li>
</ul>
<p>By using a shared prefix like /usr/local/, different packages will be installed in the same directory, so in general it will be more difficult to revert the installation.</p>
<p>Using a prefix like /opt/PROJECT/, the project will be installed in a dedicated directory, and to remove from the system you can simply remove the /opt/PREFIX path. On the other hand, such installation will require to edit all the environment variables to point to the custom path.</p>
<h3 id="Environment-variables"><a href="#Environment-variables" class="headerlink" title="Environment variables"></a>Environment variables</h3><p>Several variables defined in the environment affect your package install. In particular, depending on your installation prefix, you may need to update some of these variables in order to make sure that the installed components can be found by the system tools.</p>
<p>The list of environment variables can be shown through the command env.</p>
<p>A list of the affected variables follows:</p>
<ul>
<li>PATH: defines the list of :-separated paths where the system looks for binaries. For example if you install your package in /usr/local/, you should update the PATH so that it will contain /usr/local/bin. This can be done for example through the command export PATH=/usr/local/bin:$PATH.</li>
<li>LD_LIBRARY_PATH: contains the :-separated paths where the system looks for libraries. For example if you install your package in /usr/local/, you should update the LD_LIBRARY_PATH so that it will contain /usr/local/lib. This can be done for example through the command export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH. This variable is sometimes deprecated in favor of the use of ldconfig.</li>
<li>CFLAGS: contains flags used by the C compiler, and usually includes preprocessing directives like -IPREFIX/include or compilation flags. Custom CFLAGS are usually prefixed to the source package compiler flags by the source package build system. Alternatively many build systems allow to specify the configure option -extra-cflags.</li>
<li>LDFLAGS: these are directives used by the linker, and usually include linking directives like -LPREFIX/lib needed to find libraries installed in custom paths. Custom LDFLAGS are usually prefixed to the source package linker flags by the source package build system. Alternatively, many build systems allow to specify the configure option -extra-ldflags.</li>
<li>PKG_CONFIG_PATH: contains the :-separated paths used by pkg-config to detect the pkg-config files used by many build systems to detect the custom CFLAGS/LDFLAGS used by a specific library.
In case you installed a package in a non standard path, you need to update these environment libraries so that system tools will be able to detect the package components. This is especially required when running a configure script for a package relying on other installed libraries/headers/tools.</li>
</ul>
<p>Environment variables are usually defined in the profile file, for example .profile defined in the user directory for sh/bash users, and in /etc/profile. This file can be edited to permanently set the custom environment. Alternatively, the variables can be set in a script or in a particular shell session.</p>
<p>Remember to export the variables to the child process, e.g. using the export command. Read the fine documentation of your shell for more detailed information.</p>
<h3 id="MAC下的动态链接库"><a href="#MAC下的动态链接库" class="headerlink" title="MAC下的动态链接库"></a>MAC下的动态链接库</h3><h4 id="扩展名"><a href="#扩展名" class="headerlink" title="扩展名"></a>扩展名</h4><p>Windows下.DLL,Linux下.so,Mac OS X下的扩展名是.dylib。
.dylib是Mach-O格式，也就是Mac OS X下的二进制文件格式。Mac OS X提供了一系列
工具，用于创建和访问动态链接库。</p>
<ul>
<li>编译器/usr/bin/cc，也就是gcc了，Apple改过的。这个主要还是一个壳，去调用其他
的一些部件。当然同时还有/usr/bin/c++，等等。</li>
<li>汇编器/usr/bin/as</li>
<li>链接器/usr/bin/ld</li>
</ul>
<h4 id="MAC下创建动态链接库步骤"><a href="#MAC下创建动态链接库步骤" class="headerlink" title="MAC下创建动态链接库步骤:"></a>MAC下创建动态链接库步骤:</h4><ol>
<li>首先是生成module文件，也就是.o文件。这跟一般的unix没什么区别。例如<code>cc -c a.c b.c</code>,就得到a.o和b.o</li>
<li>可以用ld来合并.o文件，比如<code>ld -r -o c.o a.o b.o</code></li>
<li>然后可以用libtool来创建动态链接库:<code>libtool -dynamic -o c.dylib a.o b.o</code>.（ 这里也可以用<code>libtool -static -o c.a a.o b.o</code>就创建静态库）</li>
</ol>
<p>如果用gcc直接编译，我记得linux下一般是可以:
<code>gcc -shared -o c.so a.c b.c</code>
而在Mac OS X下需要:
<code>gcc -dynamiclib -o c.dylib a.c b.c</code></p>
<h4 id="动态链接库的工具"><a href="#动态链接库的工具" class="headerlink" title="动态链接库的工具"></a>动态链接库的工具</h4><p>nm是最常用的，这个用法跟linux下差不多:<code>nm c.dylib</code>,可以看到导出符号表，等等。
另一个常用的工具是otool，这个是Mac OS X独有的。比如想看看c.dylib的依赖关系<code>otool -L c.dylib</code></p>
<h3 id="官网方法"><a href="#官网方法" class="headerlink" title="官网方法"></a>官网方法</h3><ul>
<li><a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Generic" target="_blank" rel="external">CompilationGuide-Generic</a></li>
<li><a href="https://trac.ffmpeg.org/wiki/CompilationGuide/MacOSX" target="_blank" rel="external">CompilationGuide-MacOSX</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Mac下FFMPEG使用&quot;&gt;&lt;a href=&quot;#Mac下FFMPEG使用&quot; class=&quot;headerlink&quot; title=&quot;Mac下FFMPEG使用&quot;&gt;&lt;/a&gt;Mac下FFMPEG使用&lt;/h3&gt;&lt;p&gt;There are a few ways to get FF
    
    </summary>
    
      <category term="FFMPEG" scheme="http://wodekouwei.com/categories/FFMPEG/"/>
    
    
      <category term="Mac" scheme="http://wodekouwei.com/tags/Mac/"/>
    
      <category term="FFMPEG" scheme="http://wodekouwei.com/tags/FFMPEG/"/>
    
      <category term="多媒体" scheme="http://wodekouwei.com/tags/%E5%A4%9A%E5%AA%92%E4%BD%93/"/>
    
      <category term="音视频" scheme="http://wodekouwei.com/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"/>
    
  </entry>
  
  <entry>
    <title>OpenGL Frame Buffer Object(FBO)</title>
    <link href="http://wodekouwei.com/2017/06/03/gl-fbo/"/>
    <id>http://wodekouwei.com/2017/06/03/gl-fbo/</id>
    <published>2017-06-03T10:57:30.000Z</published>
    <updated>2017-06-03T13:57:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>Update: Framebuffer object extension is promoted as a core feature of OpenGL version 3.0, and is approved by ARB combining the following extensions;</p>
<ul>
<li>EXT_framebuffer_object</li>
<li>EXT_framebuffer_blit</li>
<li>EXT_framebuffer_multisample</li>
<li>EXT_packed_depth_stencil</li>
</ul>
<hr>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>In <a href="http://www.songho.ca/opengl/gl_pipeline.html" target="_blank" rel="external">OpenGL rendering pipeline</a>, the geometry data and textures are transformed and passed several tests, and then finally rendered onto a screen as 2D pixels. The final rendering destination of the OpenGL pipeline is called framebuffer. Framebuffer is a collection of 2D arrays or storages utilized by OpenGL; colour buffers, depth buffer, stencil buffer and accumulation buffer. By default, OpenGL uses the framebuffer as a rendering destination that is created and managed entirely by the window system. This default framebuffer is called window-system-provided framebuffer.
在OpenGL渲染管线中，几何数据和纹理经过多次转化和多次测试，最后以二维像素的形式显示在屏幕上。OpenGL管线的最终渲染目的地被称作帧缓存（framebuffer）。帧缓冲是一些二维数组和OpenG所使用的存储区的集合：颜色缓存、深度缓存、模板缓存和累计缓存。一般情况下，帧缓存完全由window系统生成和管理，由OpenGL使用。这个默认的帧缓存被称作“window系统生成”（window-system-provided）的帧缓存。</p>
<p>The OpenGL extension, GL_ARB_framebuffer_object provides an interface to create additional non-displayable framebuffer objects (FBO). This framebuffer is called application-created framebuffer in order to distinguish from the default window-system-provided framebuffer. By using framebuffer object (FBO), an OpenGL application can redirect the rendering output to the application-created framebuffer object (FBO) other than the traditional window-system-provided framebuffer. And, it is fully controlled by OpenGL.
在OpenGL扩展中，GL_EXT_framebuffer_object提供了一种创建额外的不能显示的帧缓存对象的接口。为了和默认的“window系统生成”的帧缓存区别，这种帧缓冲成为应用程序帧缓存（application-createdframebuffer）。通过使用帧缓存对象（FBO），OpenGL可以将显示输出到引用程序帧缓存对象，而不是传统的“window系统生成”帧缓存。而且，它完全受OpenGL控制。</p>
<p>Similar to window-system-provided framebuffer, a FBO contains a collection of rendering destinations; color, depth and stencil buffer. (Note that accumulation buffer is not defined in FBO.) These logical buffers in a FBO are called framebuffer-attachable images, which are 2D arrays of pixels that can be attached to a framebuffer object.
相似于window系统提供的帧缓存，一个FBO也包含一些存储颜色、深度和模板数据的区域。（注意：没有累积缓存）我们把FBO中这些逻辑缓存称之为“帧缓存关联图像”，它们是一些能够和一个帧缓存对象关联起来的二维数组像素。</p>
<p>There are two types of framebuffer-attachable images; texture images and renderbuffer images. If an image of a texture object is attached to a framebuffer, OpenGL performs “render to texture”. And if an image of a renderbuffer object is attached to a framebuffer, then OpenGL performs “offscreen rendering”.
有两种类型的“帧缓存关联图像”：纹理图像（texture images）和渲染缓存图像（renderbuffer images）。如果纹理对象的图像数据关联到帧缓存，OpenGL执行的是“渲染到纹理”（render to texture）操作。如果渲染缓存的图像数据关联到帧缓存，OpenGL执行的是离线渲染（offscreen rendering）。</p>
<p>By the way, renderbuffer object is a new type of storage object defined in GL_ARB_framebuffer_object extension. It is used as a rendering destination for a single 2D image during rendering process.
这里要提到的是，渲染缓存对象是在GL_EXT_framebuffer_object扩展中定义的一种新的存储类型。在渲染过程中它被用作存储单幅二维图像。</p>
<p>The following diagram shows the connectivity among the framebuffer object, texture object and renderbuffer object. Multiple texture objects or renderbuffer objects can be attached to a framebuffer object through the attachment points.
下面这幅图显示了帧缓存对象、纹理对象和渲染缓存对象之间的联系。多多个纹理对象或者渲染缓存对象能够通过关联点关联到一个帧缓存对象上。</p>
<p><img src="http://images.wodekouwei.com/gl/gl_fbo01.png" alt="image">
There are multiple color attachment points (GL_COLOR_ATTACHMENT0,…, GL_COLOR_ATTACHMENTn), one depth attachment point (GL_DEPTH_ATTACHMENT), and one stencil attachment point (GL_STENCIL_ATTACHMENT) in a framebuffer object. The number of color attachment points is implementation dependent, but each FBO must have at least one color attachement point. You can query the maximum number of color attachement points with GL_MAX_COLOR_ATTACHMENTS, which are supported by a graphics card. The reason that a FBO has multiple color attachement points is to allow to render the color buffer to multiple destinations at the same time. This “multiple render targets” (MRT) can be accomplished by GL_ARB_draw_buffers extension. Notice that the framebuffer object itself does not have any image storage(array) in it, but, it has only multiple attachment points.
在一个帧缓存对象中有多个颜色关联点（GL_COLOR_ATTACHMENT0_EXT,…,GL_COLOR_ATTACHMENTn_EXT），一个深度关联点（GL_DEPTH_ATTACHMENT_EXT），和一个模板关联点（GL_STENCIL_ATTACHMENT_EXT）。每个FBO中至少有一个颜色关联点，其数目与实体显卡相关。可以通过GL_MAX_COLOR_ATTACHMENTS_EXT来查询颜色关联点的最大数目。FBO有多个颜色关联点的原因是这样可以同时将颜色而换成渲染到多个FBO关联区。这种“多渲染目标”（multiple rendertargets,MRT）可以通过GL_ARB_draw_buffers扩展实现。需要注意的是：FBO本身并没有任何图像存储区，只有多个关联点。</p>
<p>Framebuffer object (FBO) provides an efficient switching mechanism; detach the previous framebuffer-attachable image from a FBO, and attach a new framebuffer-attachable image to the FBO. Switching framebuffer-attachable images is much faster than switching between FBOs. FBO provides glFramebufferTexture2D() to switch 2D texture objects, and glFramebufferRenderbuffer() to switch renderbuffer objects.
FBO提供了一种高效的切换机制；将前面的帧缓存关联图像从FBO分离，然后把新的帧缓存关联图像关联到FBO。在帧缓存关联图像之间切换比在FBO之间切换要快得多。FBO提供了glFramebufferTexture2DEXT()来切换2D纹理对象和glFramebufferRenderbufferEXT()来切换渲染缓存对象。</p>
<hr>
<h3 id="Creating-Frame-Buffer-Object-FBO"><a href="#Creating-Frame-Buffer-Object-FBO" class="headerlink" title="Creating Frame Buffer Object (FBO)"></a>Creating Frame Buffer Object (FBO)</h3><p>Creating framebuffer objects is similar to generating <a href="http://www.songho.ca/opengl/gl_vbo.html" target="_blank" rel="external">vertex buffer objects (VBO)</a>.
创建FBO和产生VBO类似。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">void glGenFramebuffers(GLsizei n, GLuint* ids)</div><div class="line">void glDeleteFramebuffers(GLsizei n, const GLuint* ids)</div></pre></td></tr></table></figure></p>
<p>glGenFramebuffers() requires 2 parameters; the first one is the number of framebuffers to create, and the second parameter is the pointer to a GLuint variable or an array to store a single ID or multiple IDs. It returns the IDs of unused framebuffer objects. ID 0 means the default framebuffer, which is the window-system-provided framebuffer.
glGenFramebuffersEXT()需要两个参数：第一个是要创建的帧缓存的数目，第二个是指向存储一个或者多个ID的变量或数组的指针。它返回未使用的FBO的ID。ID为0表示默认帧缓存，即window系统提供的帧缓存。</p>
<p>And, FBO may be deleted by calling glDeleteFramebuffers() when it is not used anymore.
当FBO不再被使用时，FBO可以通过调用glDeleteFrameBuffersEXT()来删除。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">glBindFramebuffer()</div></pre></td></tr></table></figure>
<p>Once a FBO is created, it has to be bound before using it.
一旦一个FBO被创建，在使用它之前必须绑定。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">void glBindFramebuffer(GLenum target, GLuint id)</div></pre></td></tr></table></figure>
<p>The first parameter, target, should be GL_FRAMEBUFFER, and the second parameter is the ID of a framebuffer object. Once a FBO is bound, all OpenGL operations affect onto the current bound framebuffer object. The object ID 0 is reserved for the default window-system provided framebuffer. Therefore, in order to unbind the current framebuffer (FBO), use ID 0 in glBindFramebuffer().
第一个参数target应该是GL_FRAMEBUFFER_EXT，第二个参数是FBO的ID号。一旦FBO被绑定，之后的所有的OpenGL操作都会对当前所绑定的FBO造成影响。ID号为0表示缺省帧缓存，即默认的window提供的帧缓存。因此，在glBindFramebufferEXT()中将ID号设置为0可以解绑定当前FBO。</p>
<hr>
<h3 id="Renderbuffer-Object"><a href="#Renderbuffer-Object" class="headerlink" title="Renderbuffer Object"></a>Renderbuffer Object</h3><p>In addition, renderbuffer object is newly introduced for offscreen rendering. It allows to render a scene directly to a renderbuffer object, instead of rendering to a texture object. Renderbuffer is simply a data storage object containing a single image of a renderable internal format. It is used to store OpenGL logical buffers that do not have corresponding texture format, such as stencil or depth buffer.</p>
<p>另外，渲染缓存是为离线渲染而新引进的。它允许将一个场景直接渲染到一个渲染缓存对象中，而不是渲染到纹理对象中。渲染缓存对象是用于存储单幅图像的数据存储区域。该图像按照一种可渲染的内部格式存储。它用于存储没有相关纹理格式的OpenGL逻辑缓存，比如模板缓存或者深度缓存。</p>
<h4 id="glGenRenderbuffers"><a href="#glGenRenderbuffers" class="headerlink" title="glGenRenderbuffers()"></a>glGenRenderbuffers()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">void glGenRenderbuffers(GLsizei n, GLuint* ids)</div><div class="line">void glDeleteRenderbuffers(GLsizei n, const Gluint* ids)</div></pre></td></tr></table></figure>
<p>Once a renderbuffer is created, it returns non-zero positive integer. ID 0 is reserved for OpenGL.
一旦一个渲染缓存被创建，它返回一个非零的正整数。ID为0是OpenGL保留值。</p>
<h4 id="glBindRenderbuffer"><a href="#glBindRenderbuffer" class="headerlink" title="glBindRenderbuffer()"></a>glBindRenderbuffer()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">void glBindRenderbuffer(GLenum target, GLuint id)</div></pre></td></tr></table></figure>
<p>Same as other OpenGL objects, you have to bind the current renderbuffer object before referencing it. The target parameter should be GL_RENDERBUFFER for renderbuffer object.
和OpenGL中其他对象一样，在引用渲染缓存之前必须绑定当前渲染缓存对象。他target参数应该是GL_RENDERBUFFER_EXT。</p>
<h4 id="glRenderbufferStorage"><a href="#glRenderbufferStorage" class="headerlink" title="glRenderbufferStorage()"></a>glRenderbufferStorage()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">void glRenderbufferStorage(GLenum  target,</div><div class="line">                           GLenum  internalFormat,</div><div class="line">                           GLsizei width,</div><div class="line">                           GLsizei height)</div></pre></td></tr></table></figure>
<p>When a renderbuffer object is created, it does not have any data storage, so we have to allocate a memory space for it. This can be done by using glRenderbufferStorage(). The first parameter must be GL_RENDERBUFFER. The second parameter would be color-renderable (GL_RGB, GL_RGBA, etc.), depth-renderable (GL_DEPTH_COMPONENT), or stencil-renderable formats (GL_STENCIL_INDEX). The width and height are the dimension of the renderbuffer image in pixels.
当一个渲染缓存被创建，它没有任何数据存储区域，所以我们还要为他分配空间。这可以通过用glRenderbufferStorageEXT()实现。第一个参数必须是GL_RENDERBUFFER_EXT。第二个参数可以是用于颜色的（GL_RGB，GL_RGBA，etc.），用于深度的（GL_DEPTH_COMPONENT），或者是用于模板的格式（GL_STENCIL_INDEX）。Width和height是渲染缓存图像的像素维度。
The width and height should be less than GL_MAX_RENDERBUFFER_SIZE, otherwise, it generates GL_INVALID_VALUE error.
width和height必须比GL_MAX_RENDERBUFFER_SIZE_EXT小，否则将会产生GL_UNVALID_VALUE错误。</p>
<h4 id="glGetRenderbufferParameteriv"><a href="#glGetRenderbufferParameteriv" class="headerlink" title="glGetRenderbufferParameteriv()"></a>glGetRenderbufferParameteriv()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">void glGetRenderbufferParameteriv(GLenum target,</div><div class="line">                                  GLenum param,</div><div class="line">                                  GLint* value)</div></pre></td></tr></table></figure>
<p>You also get various parameters of the currently bound renderbuffer object. target should be GL_RENDERBUFFER, and the second parameter is the name of parameter. The last is the pointer to an integer variable to store the returned value. The available names of the renderbuffer parameters are;
我们也可以得到当前绑定的渲染缓存对象的一些参数。Target应该是GL_RENDERBUFFER_EXT，第二个参数是所要得到的参数名字。最后一个是指向存储返回值的整型量的指针。渲染缓存的变量名有如下:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">GL_RENDERBUFFER_WIDTH</div><div class="line">GL_RENDERBUFFER_HEIGHT</div><div class="line">GL_RENDERBUFFER_INTERNAL_FORMAT</div><div class="line">GL_RENDERBUFFER_RED_SIZE</div><div class="line">GL_RENDERBUFFER_GREEN_SIZE</div><div class="line">GL_RENDERBUFFER_BLUE_SIZE</div><div class="line">GL_RENDERBUFFER_ALPHA_SIZE</div><div class="line">GL_RENDERBUFFER_DEPTH_SIZE</div><div class="line">GL_RENDERBUFFER_STENCIL_SIZE</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="Attaching-images-to-FBO"><a href="#Attaching-images-to-FBO" class="headerlink" title="Attaching images to FBO"></a>Attaching images to FBO</h3><p>FBO itself does not have any image storage(buffer) in it. Instead, we must attach framebuffer-attachable images (texture or renderbuffer objects) to the FBO. This mechanism allows that FBO quickly switch (detach and attach) the framebuffer-attachable images in a FBO. It is much faster to switch framebuffer-attachable images than to switch between FBOs. And, it saves unnecessary data copies and memory consumption. For example, a texture can be attached to multiple FBOs, and its image storage can be shared by multiple FBOs.
FBO本身没有图像存储区。我们必须帧缓存关联图像（纹理或渲染对象）关联到FBO。这种机制允许FBO快速地切换（分离和关联）帧缓存关联图像。切换帧缓存关联图像比在FBO之间切换要快得多。而且，它节省了不必要的数据拷贝和内存消耗。比如，一个纹理可以被关联到多个FBO上，图像存储区可以被多个FBO共享。</p>
<h4 id="Attaching-a-2D-texture-image-to-FBO"><a href="#Attaching-a-2D-texture-image-to-FBO" class="headerlink" title="Attaching a 2D texture image to FBO"></a>Attaching a 2D texture image to FBO</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">glFramebufferTexture2D(GLenum target,</div><div class="line">                       GLenum attachmentPoint,</div><div class="line">                       GLenum textureTarget,</div><div class="line">                       GLuint textureId,</div><div class="line">                       GLint  level)</div></pre></td></tr></table></figure>
<p>glFramebufferTexture2D() is to attach a 2D texture image to a FBO. The first parameter must be GL_FRAMEBUFFER, and the second parameter is the attachment point where to connect the texture image. A FBO has multiple color attachment points (GL_COLOR_ATTACHMENT0, …, GL_COLOR_ATTACHMENTn), GL_DEPTH_ATTACHMENT, and GL_STENCIL_ATTACHMENT. The third parameter, “textureTarget” is GL_TEXTURE_2D in most cases. The fourth parameter is the identifier of the texture object. The last parameter is the mipmap level of the texture to be attached.
glFramebufferTexture2DEXT()把一幅纹理图像关联到一个FBO。第一个参数一定是GL_FRAMEBUFFER_EXT，第二个参数是关联纹理图像的关联点。第三个参数textureTarget在多数情况下是GL_TEXTURE_2D。第四个参数是纹理对象的ID号。最后一个参数是要被关联的纹理的mipmap等级
If the textureId parameter is set to 0, then, the texture image will be detached from the FBO. If a texture object is deleted while it is still attached to a FBO, then, the texture image will be automatically detached from the currently bound FBO. However, if it is attached to multiple FBOs and deleted, then it will be detached from only the bound FBO, but will not be detached from any other un-bound FBOs.
如果参数textureId被设置为0，那么纹理图像将会被从FBO分离。如果纹理对象在依然关联在FBO上时被删除，那么纹理对象将会自动从当前帮的FBO上分离。然而，如果它被关联到多个FBO上然后被删除，那么它将只被从绑定的FBO上分离，而不会被从其他非绑定的FBO上分离。</p>
<h4 id="Attaching-a-Renderbuffer-image-to-FBO"><a href="#Attaching-a-Renderbuffer-image-to-FBO" class="headerlink" title="Attaching a Renderbuffer image to FBO"></a>Attaching a Renderbuffer image to FBO</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">void glFramebufferRenderbuffer(GLenum target,</div><div class="line">                               GLenum attachmentPoint,</div><div class="line">                               GLenum renderbufferTarget,</div><div class="line">                               GLuint renderbufferId)</div></pre></td></tr></table></figure>
<p>A renderbuffer image can be attached by calling glFramebufferRenderbuffer(). The first and second parameters are same as glFramebufferTexture2D(). The third parameter must be GL_RENDERBUFFER, and the last parameter is the ID of the renderbuffer object.
通过调用glFramebufferRenderbufferEXT()可以关联渲染缓存图像。前两个参数和glFramebufferTexture2DEXT()一样。第三个参数只能是GL_RENDERBUFFER_EXT，最后一个参数是渲染缓存对象的ID号。
If renderbufferId parameter is set to 0, the renderbuffer image will be detached from the attachment point in the FBO. If a renderbuffer object is deleted while it is still attached in a FBO, then it will be automatically detached from the bound FBO. However, it will not be detached from any other non-bound FBOs.
如果参数renderbufferId被设置为0，渲染缓存图像将会从FBO的关联点分离。如果渲染缓存图像在依然关联在FBO上时被删除，那么纹理对象将会自动从当前绑定的FBO上分离，而不会从其他非绑定的FBO上分离。</p>
<hr>
<h3 id="FBO-with-MSAA-Multi-Sample-Anti-Aliasing"><a href="#FBO-with-MSAA-Multi-Sample-Anti-Aliasing" class="headerlink" title="FBO with MSAA (Multi Sample Anti Aliasing)"></a>FBO with MSAA (Multi Sample Anti Aliasing)</h3><p>When you render to a FBO, anti-aliasing is not automatically enabled even if you properly create a OpenGL rendering context with the multisampling attribute (SAMPLEBUFFERS_ARB) for window-system-provided framebuffer.</p>
<p>In order to activate multisample anti-aliasing mode for rendering to a FBO, you need to prepare and attach multisample images to a FBO’s color and/or depth attachement points.</p>
<p>FBO extension provides glRenderbufferStorageMultisample() to create a renderbuffer image for multisample anti-aliasing rendering mode.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">void glRenderbufferStorageMultisample(GLenum  target,</div><div class="line">                                      GLsizei samples,</div><div class="line">                                      GLenum  internalFormat,</div><div class="line">                                      GLsizei width,</div><div class="line">                                      GLsizei height)</div></pre></td></tr></table></figure>
<p>It adds new parameter, samples on top of glRenderbufferStorage(), which is the number of multisamples for anti-aliased rendering mode. If it is 0, then no MSAA mode is enabled and glRenderbufferStorage() is called instead. You can query the maximum number of samples with GL_MAX_SAMPLES token in glGetIntegerv().</p>
<p>The following code is to create a FBO with multisample colorbuffer and depthbuffer images. Note that if multiple images are attached to a FBO, then all images must have the same number of multisamples. Otherwise, the FBO status is incomplete.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">// create a 4x MSAA renderbuffer object for colorbuffer</div><div class="line">int msaa = 4;</div><div class="line">GLuint rboColorId;</div><div class="line">glGenRenderbuffers(1, &amp;rboColorId);</div><div class="line">glBindRenderbuffer(GL_RENDERBUFFER, rboColorId);</div><div class="line">glRenderbufferStorageMultisample(GL_RENDERBUFFER, msaa, GL_RGB8, width, height);</div><div class="line"></div><div class="line">// create a 4x MSAA renderbuffer object for depthbuffer</div><div class="line">GLuint rboDepthId;</div><div class="line">glGenRenderbuffers(1, &amp;rboDepthId);</div><div class="line">glBindRenderbuffer(GL_RENDERBUFFER, rboDepthId);</div><div class="line">glRenderbufferStorageMultisample(GL_RENDERBUFFER, msaa, GL_DEPTH_COMPONENT, width, height);</div><div class="line"></div><div class="line">// create a 4x MSAA framebuffer object</div><div class="line">GLuint fboId;</div><div class="line">glGenFramebuffers(1, &amp;fboMsaaId);</div><div class="line">glBindFramebuffer(GL_FRAMEBUFFER, fboMsaaId);</div><div class="line"></div><div class="line">// attach colorbuffer image to FBO</div><div class="line">glFramebufferRenderbuffer(GL_FRAMEBUFFER,       // 1. fbo target: GL_FRAMEBUFFER</div><div class="line">                          GL_COLOR_ATTACHMENT0, // 2. color attachment point</div><div class="line">                          GL_RENDERBUFFER,      // 3. rbo target: GL_RENDERBUFFER</div><div class="line">                          rboColorId);          // 4. rbo ID</div><div class="line"></div><div class="line">// attach depthbuffer image to FBO</div><div class="line">glFramebufferRenderbuffer(GL_FRAMEBUFFER,       // 1. fbo target: GL_FRAMEBUFFER</div><div class="line">                          GL_DEPTH_ATTACHMENT,  // 2. depth attachment point</div><div class="line">                          GL_RENDERBUFFER,      // 3. rbo target: GL_RENDERBUFFER</div><div class="line">                          rboDepthId);          // 4. rbo ID</div><div class="line"></div><div class="line">// check FBO status</div><div class="line">GLenum status = glCheckFramebufferStatus(GL_FRAMEBUFFER);</div><div class="line">if(status != GL_FRAMEBUFFER_COMPLETE)</div><div class="line">    fboUsed = false;</div></pre></td></tr></table></figure>
<p>It is important to know that glRenderbufferStorageMultisample() only enables MSAA rendering to FBO. However, you cannot directly use the result from MSAA FBO. If you need to transfer the result to a texture or other non-multisampled framebuffer, you have to convert (downsample) the result to single-sample image using glBlitFramebuffer().</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">void glBlitFramebuffer(GLint srcX0, GLint srcY0, GLint srcX1, GLint srcY1, // source rectangle</div><div class="line">                       GLint dstX0, GLint dstY0, GLint dstX1, GLint dstY1, // destination rect</div><div class="line">                       GLbitfield mask,</div><div class="line">                       GLenum filter)</div></pre></td></tr></table></figure>
<p>glBlitFramebuffer() copies a rectangle of images from the source (GL_READ_BUFFER) to the destination framebuffer (GL_DRAW_BUFFER). The “mask” parameter is to specify which buffers are copied, GL_COLOR_BUFFER_BIT, GL_DEPTH_BUFFER_BIT and/or GL_STENCIL_BUFFER_BIT. The last parameter, “filter” is to specify the interpolation mode if the source and destination rectangles are not same. It is either GL_NEAREST or GL_LINEAR.</p>
<p>The following code is to transfer a multisampled image from a FBO to another non-multisampled FBO. Notice it requires an additional FBO to get the result of MSAA rendering. Please see fboMsaa.zip for details to perform render-to-texture with MSAA.
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">// copy rendered image from MSAA (multi-sample) to normal (single-sample)</div><div class="line">// NOTE: The multi samples at a pixel in read buffer will be converted</div><div class="line">// to a single sample at the target pixel in draw buffer.</div><div class="line">glBindFramebuffer(GL_READ_FRAMEBUFFER, fboMsaaId); // src FBO (multi-sample)</div><div class="line">glBindFramebuffer(GL_DRAW_FRAMEBUFFER, fboId);     // dst FBO (single-sample)</div><div class="line"></div><div class="line">glBlitFramebuffer(0, 0, width, height,             // src rect</div><div class="line">                  0, 0, width, height,             // dst rect</div><div class="line">                  GL_COLOR_BUFFER_BIT,             // buffer mask</div><div class="line">                  GL_LINEAR);                      // scale filter</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="Checking-FBO-Status"><a href="#Checking-FBO-Status" class="headerlink" title="Checking FBO Status"></a>Checking FBO Status</h3><p>Once attachable images (textures and renderbuffers) are attached to a FBO and before performing FBO operation, you must validate if the FBO status is complete or incomplete by using glCheckFramebufferStatus(). If the FBO is not complete, then any drawing and reading command (glBegin(), glCopyTexImage2D(), etc) will be failed.</p>
<p>一旦关联图像（纹理和渲染缓存）被关联到FBO上，在执行FBO的操作之前，你必须检查FBO的状态，这可以通过调用glCheckFramebufferStatusEXT()实现。如果这个FBObuilding完整，那么任何绘制和读取命令（glBegin(),glCopyTexImage2D(), etc）都会失败。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">GLenum glCheckFramebufferStatus(GLenum target)</div></pre></td></tr></table></figure>
<p>glCheckFramebufferStatus() validates all its attached images and framebuffer parameters on the currently bound FBO. And, this function cannot be called within glBegin()/glEnd() pair. The target parameter should be GL_FRAMEBUFFER. It returns non-zero value after checking the FBO. If all requirements and rules are satisfied, then it returns GL_FRAMEBUFFER_COMPLETE. Otherwise, it returns a relevant error value, which tells what rule is violated.
glCheckFramebufferStatusEXT()检查当前帧缓存的关联图像和帧缓存参数。这个函数不能在glBegin()/glEnd()之间调用。Target参数必须为GL_FRAMEBUFFER_EXT。它返回一个非零值。如果所有要求和准则都满足，它返回GL_FRAMEBUFFER_COMPLETE_EXT。否则，返回一个相关错误代码告诉我们哪条准则没有满足。</p>
<p>The rules of FBO completeness are:</p>
<ul>
<li>The width and height of framebuffer-attachable image must be not zero.</li>
<li>If an image is attached to a color attachment point, then the image must have a color-renderable internal format. (GL_RGBA, GL_DEPTH_COMPONENT, GL_LUMINANCE, etc)</li>
<li>If an image is attached to GL_DEPTH_ATTACHMENT, then the image must have a depth-renderable internal format. (GL_DEPTH_COMPONENT, GL_DEPTH_COMPONENT24, etc)</li>
<li>If an image is attached to GL_STENCIL_ATTACHMENT, then the image must have a stencil-renderable internal format. (GL_STENCIL_INDEX, GL_STENCIL_INDEX8, etc)</li>
<li>FBO must have at least one image attached.</li>
<li>All images attached a FBO must have the same width and height.</li>
<li>All images attached the color attachment points must have the same internal format.</li>
</ul>
<p>FBO完整性准则有：</p>
<ol>
<li>帧缓存关联图像的宽度和高度必须非零。</li>
<li>如果一幅图像被关联到一个颜色关联点，那么这幅图像必须有颜色可渲染的内部格式（GL_RGBA, GL_DEPTH_COMPONENT, GL_LUMINANCE, etc)。</li>
<li>如果一幅被图像关联到GL_DEPTH_ATTACHMENT_EXT，那么这幅图像必须有深度可渲染的内部格式(GL_DEPTH_COMPONENT,GL_DEPTH_COMPONENT24_EXT, etc)。</li>
<li>如果一幅被图像关联到GL_STENCIL_ATTACHMENT_EXT，那么这幅图像必须有模板可渲染的内部格式(GL_STENCIL_INDEX,GL_STENCIL_INDEX8_EXT, etc)。</li>
<li>FBO至少有一幅图像关联。</li>
<li>被关联到FBO的缩影图像必须有相同的宽度和高度。</li>
<li>被关联到颜色关联点上的所有图像必须有相同的内部格式。</li>
</ol>
<p>Note that even though all of the above conditions are satisfied, your OpenGL driver may not support some combinations of internal formats and parameters. If a particular implementation is not supported by OpenGL driver, then glCheckFramebufferStatus() returns GL_FRAMEBUFFER_UNSUPPORTED.
注意：即使以上所有条件都满足，你的OpenGL驱动也可能不支持某些格式和参数的组合。如果一种特别的实现不被OpenGL驱动支持，那么glCheckFramebufferStatusEXT()返回GL_FRAMEBUFFER_UNSUPPORTED_EXT。</p>
<p>The sample code provides some utility functions to report the information of the current FBO; printFramebufferInfo() and checkFramebufferStatus().</p>
<hr>
<h3 id="Example-Render-To-Texture"><a href="#Example-Render-To-Texture" class="headerlink" title="Example: Render To Texture"></a>Example: Render To Texture</h3><p>Sometimes, you need to generate dynamic textures on the fly. The most common examples are generating mirroring/reflection effects, dynamic cube/environment maps and shadow maps. Dynamic texturing can be accomplished by rendering the scene to a texture. A traditional way of render-to-texture is to draw a scene to the framebuffer as normal, and then copy the framebuffer image to a texture by using glCopyTexSubImage2D().
有时候，你需要产生动态纹理。比较常见的例子是产生镜面反射效果、动态环境贴图和阴影等效果。动态纹理可以通过把场景渲染到纹理来实现。渲染到纹理的一种传统方式是将场景绘制到普通的帧缓存上，然后调用glCopyTexSubImage2D()拷贝帧缓存图像至纹理。</p>
<p>Using FBO, we can render a scene directly onto a texture, so we don’t have to use the window-system-provided framebuffer at all. Further more, we can eliminate an additional data copy (from framebuffer to texture).
使用FBO，我们能够将场景直接渲染到纹理，所以我们不必使用window系统提供的帧缓存。并且，我们能够去除额外的数据拷贝（从帧缓存到纹理）；。</p>
<p>This demo program performs render to texture operation with/without FBO, and compares the performance difference. Other than performance gain, there is another advantage of using FBO. If the texture resolution is larger than the size of the rendering window in traditional render-to-texture mode (without FBO), then the area out of the window region will be clipped. However, FBO does not suffer from this clipping problem. You can create a framebuffer-renderable image larger than the display window.
这个demo实现了使用FBO和不使用FBO两种情况下渲染到纹理的操作，并且比较了性能差异。除了能够获得性能上的提升，使用FBO的还有另外一个优点。在传统的渲染到纹理的模式中（不使用FBO），如果纹理分辨率比渲染窗口的尺寸大，超出窗口区域的部分将被剪切掉。然后，使用FBO就不会有这个问题。你可以产生比显示窗口大的帧缓存渲染图像。</p>
<p>The following codes is to setup a FBO and framebuffer-attachable images before the rendering loop is started. Note that not only a texture image is attached to the FBO, but also, a renderbuffer image is attached to the depth attachment point of the FBO. We do not actually use this depth buffer, however, the FBO itself needs it for depth test. If we don’t attach this depth renderable image to the FBO, then the rendering output will be corrupted because of missing depth test. If stencil test is also required during FBO rendering, then additional renderbuffer image should be attached to GL_STENCIL_ATTACHMENT.
以下代码在渲染循环开始之前，对FBO和帧缓存关联图像进行了初始化。注意只有一幅纹理图像被关联到FBO，但是，一个深度渲染图像被关联到FBO的深度关联点。实际上我们并没有使用这个深度缓存，但是FBO本身需要它进行深度测试。如果我们不把这个深度可渲染的图像关联到FBO，那么由于缺少深度测试渲染输出结果是不正确的。如果在FBO渲染期间模板测试也是必要的，那么也需要把额外的渲染图像和GL_STENCIL_ATTACHMENT_EXT关联起来。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">// create a texture object</div><div class="line">GLuint textureId;</div><div class="line">glGenTextures(1, &amp;textureId);</div><div class="line">glBindTexture(GL_TEXTURE_2D, textureId);</div><div class="line">glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);</div><div class="line">glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR);</div><div class="line">glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);</div><div class="line">glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);</div><div class="line">glTexParameteri(GL_TEXTURE_2D, GL_GENERATE_MIPMAP, GL_TRUE); // automatic mipmap</div><div class="line">glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA8, TEXTURE_WIDTH, TEXTURE_HEIGHT, 0,</div><div class="line">             GL_RGBA, GL_UNSIGNED_BYTE, 0);</div><div class="line">glBindTexture(GL_TEXTURE_2D, 0);</div><div class="line"></div><div class="line">// create a renderbuffer object to store depth info</div><div class="line">GLuint rboId;</div><div class="line">glGenRenderbuffers(1, &amp;rboId);</div><div class="line">glBindRenderbuffer(GL_RENDERBUFFER, rboId);</div><div class="line">glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT,</div><div class="line">                      TEXTURE_WIDTH, TEXTURE_HEIGHT);</div><div class="line">glBindRenderbuffer(GL_RENDERBUFFER, 0);</div><div class="line"></div><div class="line">// create a framebuffer object</div><div class="line">GLuint fboId;</div><div class="line">glGenFramebuffers(1, &amp;fboId);</div><div class="line">glBindFramebuffer(GL_FRAMEBUFFER, fboId);</div><div class="line"></div><div class="line">// attach the texture to FBO color attachment point</div><div class="line">glFramebufferTexture2D(GL_FRAMEBUFFER,        // 1. fbo target: GL_FRAMEBUFFER </div><div class="line">                       GL_COLOR_ATTACHMENT0,  // 2. attachment point</div><div class="line">                       GL_TEXTURE_2D,         // 3. tex target: GL_TEXTURE_2D</div><div class="line">                       textureId,             // 4. tex ID</div><div class="line">                       0);                    // 5. mipmap level: 0(base)</div><div class="line"></div><div class="line">// attach the renderbuffer to depth attachment point</div><div class="line">glFramebufferRenderbuffer(GL_FRAMEBUFFER,      // 1. fbo target: GL_FRAMEBUFFER</div><div class="line">                          GL_DEPTH_ATTACHMENT, // 2. attachment point</div><div class="line">                          GL_RENDERBUFFER,     // 3. rbo target: GL_RENDERBUFFER</div><div class="line">                          rboId);              // 4. rbo ID</div><div class="line"></div><div class="line">// check FBO status</div><div class="line">GLenum status = glCheckFramebufferStatus(GL_FRAMEBUFFER);</div><div class="line">if(status != GL_FRAMEBUFFER_COMPLETE)</div><div class="line">    fboUsed = false;</div><div class="line"></div><div class="line">// switch back to window-system-provided framebuffer</div><div class="line">glBindFramebuffer(GL_FRAMEBUFFER, 0);</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>The rendering procedure of render-to-texture is almost same as normal drawing. We only need to switch the rendering destination from the window-system-provided to the non-displayable, application-created framebuffer (FBO).
渲染到纹理的过程和普通的绘制过程基本一样。我们只需要把渲染的目的地由window系统提供的帧缓存改成不可显示的应用程序创建的帧缓存（FBO）就可以了。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">// set rendering destination to FBO</div><div class="line">glBindFramebuffer(GL_FRAMEBUFFER, fboId);</div><div class="line"></div><div class="line">// clear buffers</div><div class="line">glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</div><div class="line"></div><div class="line">// draw a scene to a texture directly</div><div class="line">draw();</div><div class="line"></div><div class="line">// unbind FBO</div><div class="line">glBindFramebuffer(GL_FRAMEBUFFER, 0);</div><div class="line"></div><div class="line">// trigger mipmaps generation explicitly</div><div class="line">// NOTE: If GL_GENERATE_MIPMAP is set to GL_TRUE, then glCopyTexSubImage2D()</div><div class="line">// triggers mipmap generation automatically. However, the texture attached</div><div class="line">// onto a FBO should generate mipmaps manually via glGenerateMipmap().</div><div class="line">glBindTexture(GL_TEXTURE_2D, textureId);</div><div class="line">glGenerateMipmap(GL_TEXTURE_2D);</div><div class="line">glBindTexture(GL_TEXTURE_2D, 0);</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>Note that glGenerateMipmap() is also included as part of FBO extension in order to generate mipmaps explicitly after modifying the base level texture image. If GL_GENERATE_MIPMAP is set to GL_TRUE, then glTex{Sub}Image2D() and glCopyTex{Sub}Image2D() trigger automatic mipmap generation (in OpenGL version 1.4 or greater). However, FBO operation does not generate its mipmaps automatically when the base level texture is modified because FBO does not call glCopyTex{Sub}Image2D() to modify the texture. Therefore, glGenerateMipmap() must be explicitly called for mipmap generation.
注意到，glGenerateMipmapEXT()也是作为FBO扩展的一部分，用来在改变了纹理图像的基级之后显式生成mipmap的。如果GL_GENERATE_MIPMAP被设置为GL_TRUE，那么glTex{Sub}Image2D()和 glCopyTex{Sub}Image2D()将会启用自动mipmap生成（在OpenGL版本1.4或者更高版本中）。然后，当纹理基级被改变时，FBO操作不会自动产生mipmaps。因为FBO不会调用glCopyTex{Sub}Image2D()来修改纹理。因此，要产生mipmap，glGenerateMipmapEXT()必须被显示调用。</p>
<p>If you need to a post processing of the texture, it is possible to combine with Pixel Buffer Object (PBO) to modify the texture efficiently.</p>
<hr>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><blockquote>
<p><a href="http://www.songho.ca/opengl/gl_fbo.html" target="_blank" rel="external">原文</a>
<a href="http://blog.csdn.net/xiajun07061225/article/details/7283929" target="_blank" rel="external">译文</a></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Update: Framebuffer object extension is promoted as a core feature of OpenGL version 3.0, and is approved by ARB combining the following 
    
    </summary>
    
      <category term="OpenGL" scheme="http://wodekouwei.com/categories/OpenGL/"/>
    
    
      <category term="OpenGL" scheme="http://wodekouwei.com/tags/OpenGL/"/>
    
      <category term="翻译" scheme="http://wodekouwei.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>android textureview处理预览摄像头变形问题</title>
    <link href="http://wodekouwei.com/2017/05/04/android-view-textureview/"/>
    <id>http://wodekouwei.com/2017/05/04/android-view-textureview/</id>
    <published>2017-05-04T12:03:05.000Z</published>
    <updated>2017-06-03T11:05:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>当TextureView的大小和匹配到的摄像头PreviewSize宽高比例不完全一致时,TextureView可通过setTransform函数对预览画面进行处理后再显示到TextureView,如下对图形居中裁剪:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">public void sizeNotify(Camera.Size size) &#123;</div><div class="line">            float viewWidth = getWidth();</div><div class="line">            float viewHeight = getHeight();</div><div class="line"></div><div class="line">            float scaleX = 1.0f;</div><div class="line">            float scaleY = 1.0f;</div><div class="line">            int mPreviewWidth = size.width;</div><div class="line">            int mPreviewHeight = size.height;</div><div class="line">            if(viewWidth &lt; viewHeight) &#123;</div><div class="line">                mPreviewWidth = size.height;</div><div class="line">                mPreviewHeight = size.width;</div><div class="line">            &#125;</div><div class="line"></div><div class="line"></div><div class="line">            if (mPreviewWidth &gt; viewWidth &amp;&amp; mPreviewHeight &gt; viewHeight) &#123;</div><div class="line">                scaleX = mPreviewWidth / viewWidth;</div><div class="line">                scaleY = mPreviewHeight / viewHeight;</div><div class="line">            &#125; else if (mPreviewWidth &lt; viewWidth &amp;&amp; mPreviewHeight &lt; viewHeight) &#123;</div><div class="line">                scaleY = viewWidth / mPreviewWidth;</div><div class="line">                scaleX = viewHeight / mPreviewHeight;</div><div class="line">            &#125; else if (viewWidth &gt; mPreviewWidth) &#123;</div><div class="line">                scaleY = (viewWidth / mPreviewWidth) / (viewHeight / mPreviewHeight);</div><div class="line">            &#125; else if (viewHeight &gt; mPreviewHeight) &#123;</div><div class="line">                scaleX = (viewHeight / mPreviewHeight) / (viewWidth / mPreviewWidth);</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            // Calculate pivot points, in our case crop from center</div><div class="line">            int pivotPointX = (int) (viewWidth / 2);</div><div class="line">            int pivotPointY = (int) (viewHeight / 2);</div><div class="line"></div><div class="line">            Matrix matrix = new Matrix();</div><div class="line">            matrix.setScale(scaleX, scaleY, pivotPointX, pivotPointY);</div><div class="line">            /*Log.e(TAG, &quot;viewsize:&quot; + viewWidth + &quot; * &quot; + viewHeight +</div><div class="line">                    &quot;;prviewSize:&quot; + mPreviewWidth + &quot; * &quot; + mPreviewHeight +</div><div class="line">                    &quot;;scale:&quot; + scaleX + &quot; * &quot; + scaleY +</div><div class="line">                    &quot;;pivot:&quot; + pivotPointX + &quot; * &quot; + pivotPointY);*/</div><div class="line">            setTransform(matrix);</div><div class="line">        &#125;</div></pre></td></tr></table></figure></p>
<p>TextureView中setTransform函数说明:
Sets the transform to associate with this texture view. The specified transform applies to the underlying surface texture and does not affect the size or position of the view itself, only of its content.</p>
<p>Some transforms might prevent the content from drawing all the pixels contained within this view’s bounds. In such situations, make sure this texture view is not marked opaque.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当TextureView的大小和匹配到的摄像头PreviewSize宽高比例不完全一致时,TextureView可通过setTransform函数对预览画面进行处理后再显示到TextureView,如下对图形居中裁剪:
&lt;figure class=&quot;highlight pl
    
    </summary>
    
      <category term="Android" scheme="http://wodekouwei.com/categories/Android/"/>
    
    
      <category term="android" scheme="http://wodekouwei.com/tags/android/"/>
    
      <category term="view" scheme="http://wodekouwei.com/tags/view/"/>
    
      <category term="TextureView" scheme="http://wodekouwei.com/tags/TextureView/"/>
    
  </entry>
  
  <entry>
    <title>webrtc之Native APIs</title>
    <link href="http://wodekouwei.com/2017/05/03/webrtc-nativeapis/"/>
    <id>http://wodekouwei.com/2017/05/03/webrtc-nativeapis/</id>
    <published>2017-05-03T03:51:59.000Z</published>
    <updated>2017-05-03T03:58:23.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Block-diagram"><a href="#Block-diagram" class="headerlink" title="Block diagram"></a>Block diagram</h3><p><img src="http://images.wodekouwei.com/protocol/WebRTCNativeAPIsDocument1.png" alt="image"></p>
<h3 id="Calling-sequences"><a href="#Calling-sequences" class="headerlink" title="Calling sequences"></a>Calling sequences</h3><h4 id="Set-up-a-call"><a href="#Set-up-a-call" class="headerlink" title="Set up a call"></a>Set up a call</h4><p><img src="http://images.wodekouwei.com/protocol/WebRTCNativeAPIsDocument2.png" alt="image"></p>
<h4 id="Receive-a-call"><a href="#Receive-a-call" class="headerlink" title="Receive a call"></a>Receive a call</h4><p><img src="http://images.wodekouwei.com/protocol/WebRTCNativeAPIsDocument3.png" alt="image"></p>
<h4 id="Close-down-a-call"><a href="#Close-down-a-call" class="headerlink" title="Close down a call"></a>Close down a call</h4><p><img src="http://images.wodekouwei.com/protocol/WebRTCNativeAPIsDocument4.png" alt="image"></p>
<h3 id="Threading-model"><a href="#Threading-model" class="headerlink" title="Threading model"></a>Threading model</h3><p>WebRTC native APIs use two globally available threads: the signaling thread and the worker thread. Depending on how the PeerConnection factory is created, the application can either provide those 2 threads or just let them be created internally.</p>
<p>The calls to the Stream APIs and the PeerConnection APIs will be proxied to the signaling thread which means that the application can call those APIs from whatever thread.</p>
<p>All callbacks will be made on the signaling thread. The application should return the callback as quickly as possible to avoid blocking the signaling thread. Resource intensive processes should be posted to a different thread.</p>
<p>The worker thread is used to handle more resource intensive processes such as data streaming.</p>
<blockquote>
<p><a href="https://sites.google.com/site/webrtc/native-code/native-apis" target="_blank" rel="external">https://sites.google.com/site/webrtc/native-code/native-apis</a></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Block-diagram&quot;&gt;&lt;a href=&quot;#Block-diagram&quot; class=&quot;headerlink&quot; title=&quot;Block diagram&quot;&gt;&lt;/a&gt;Block diagram&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://images.wo
    
    </summary>
    
      <category term="webrtc" scheme="http://wodekouwei.com/categories/webrtc/"/>
    
    
      <category term="webrtc" scheme="http://wodekouwei.com/tags/webrtc/"/>
    
  </entry>
  
  <entry>
    <title>webrtc源码走读之api</title>
    <link href="http://wodekouwei.com/2017/05/02/webrtc-source-api/"/>
    <id>http://wodekouwei.com/2017/05/02/webrtc-source-api/</id>
    <published>2017-05-02T08:23:59.000Z</published>
    <updated>2017-05-05T01:43:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>api目录下封装了webrtc相关的供外部调用接口.</p>
<div id="sequence-0"></div>

<h3 id="peerconnection接口"><a href="#peerconnection接口" class="headerlink" title="peerconnection接口"></a>peerconnection接口</h3><p>关于<code>peerconnectioninterface.h</code>说明:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">// This file contains the PeerConnection interface as defined in</div><div class="line">// http://dev.w3.org/2011/webrtc/editor/webrtc.html#peer-to-peer-connections.</div><div class="line">//</div><div class="line">// The PeerConnectionFactory class provides factory methods to create</div><div class="line">// PeerConnection, MediaStream and MediaStreamTrack objects.</div><div class="line">//</div><div class="line">// The following steps are needed to setup a typical call using WebRTC:</div><div class="line">//</div><div class="line">// 1. Create a PeerConnectionFactoryInterface. Check constructors for more</div><div class="line">// information about input parameters.</div><div class="line">//</div><div class="line">// 2. Create a PeerConnection object. Provide a configuration struct which</div><div class="line">// points to STUN and/or TURN servers used to generate ICE candidates, and</div><div class="line">// provide an object that implements the PeerConnectionObserver interface,</div><div class="line">// which is used to receive callbacks from the PeerConnection.</div><div class="line">//</div><div class="line">// 3. Create local MediaStreamTracks using the PeerConnectionFactory and add</div><div class="line">// them to PeerConnection by calling AddTrack (or legacy method, AddStream).</div><div class="line">//</div><div class="line">// 4. Create an offer, call SetLocalDescription with it, serialize it, and send</div><div class="line">// it to the remote peer</div><div class="line">//</div><div class="line">// 5. Once an ICE candidate has been gathered, the PeerConnection will call the</div><div class="line">// observer function OnIceCandidate. The candidates must also be serialized and</div><div class="line">// sent to the remote peer.</div><div class="line">//</div><div class="line">// 6. Once an answer is received from the remote peer, call</div><div class="line">// SetRemoteDescription with the remote answer.</div><div class="line">//</div><div class="line">// 7. Once a remote candidate is received from the remote peer, provide it to</div><div class="line">// the PeerConnection by calling AddIceCandidate.</div><div class="line">//</div><div class="line">// The receiver of a call (assuming the application is &quot;call&quot;-based) can decide</div><div class="line">// to accept or reject the call; this decision will be taken by the application,</div><div class="line">// not the PeerConnection.</div><div class="line">//</div><div class="line">// If the application decides to accept the call, it should:</div><div class="line">//</div><div class="line">// 1. Create PeerConnectionFactoryInterface if it doesn&apos;t exist.</div><div class="line">//</div><div class="line">// 2. Create a new PeerConnection.</div><div class="line">//</div><div class="line">// 3. Provide the remote offer to the new PeerConnection object by calling</div><div class="line">// SetRemoteDescription.</div><div class="line">//</div><div class="line">// 4. Generate an answer to the remote offer by calling CreateAnswer and send it</div><div class="line">// back to the remote peer.</div><div class="line">//</div><div class="line">// 5. Provide the local answer to the new PeerConnection by calling</div><div class="line">// SetLocalDescription with the answer.</div><div class="line">//</div><div class="line">// 6. Provide the remote ICE candidates by calling AddIceCandidate.</div><div class="line">//</div><div class="line">// 7. Once a candidate has been gathered, the PeerConnection will call the</div><div class="line">// observer function OnIceCandidate. Send these candidates to the remote peer.</div></pre></td></tr></table></figure></p>
<p><script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.27/webfontloader.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/snap.svg/0.4.1/snap.svg-min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">Alice->Bob: Hello Bob, how are you?
Note right of Bob: Bob thinks
Bob-->Alice: I am good thanks!</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;api目录下封装了webrtc相关的供外部调用接口.&lt;/p&gt;
&lt;div id=&quot;sequence-0&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;peerconnection接口&quot;&gt;&lt;a href=&quot;#peerconnection接口&quot; class=&quot;headerlink&quot; titl
    
    </summary>
    
      <category term="webrtc" scheme="http://wodekouwei.com/categories/webrtc/"/>
    
    
      <category term="webrtc" scheme="http://wodekouwei.com/tags/webrtc/"/>
    
      <category term="源码" scheme="http://wodekouwei.com/tags/%E6%BA%90%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>webrtc源码走读之base</title>
    <link href="http://wodekouwei.com/2017/05/02/webrtc-source-base/"/>
    <id>http://wodekouwei.com/2017/05/02/webrtc-source-base/</id>
    <published>2017-05-02T08:12:58.000Z</published>
    <updated>2017-05-02T08:23:27.000Z</updated>
    
    <content type="html"><![CDATA[<p><code>src/webrtc/base</code>是webrtc基础平台库，包括线程、锁、socket,智能指针等.</p>
<h3 id="智能指针"><a href="#智能指针" class="headerlink" title="智能指针"></a>智能指针</h3><p><code>refcount.h</code>定义了rtc::RefCountInterface:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">#include &quot;webrtc/base/refcountedobject.h&quot;</div><div class="line"></div><div class="line">namespace rtc &#123;</div><div class="line"></div><div class="line">// Reference count interface.</div><div class="line">class RefCountInterface &#123;</div><div class="line"> public:</div><div class="line">  virtual int AddRef() const = 0;</div><div class="line">  virtual int Release() const = 0;</div><div class="line"></div><div class="line"> protected:</div><div class="line">  virtual ~RefCountInterface() &#123;&#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line">&#125;  // namespace rtc</div></pre></td></tr></table></figure></p>
<p><code>refcountedobject.h</code>下定义了RefCountedObject:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">#include &lt;utility&gt;</div><div class="line"></div><div class="line">#include &quot;webrtc/base/atomicops.h&quot;</div><div class="line"></div><div class="line">namespace rtc &#123;</div><div class="line"></div><div class="line">template &lt;class T&gt;</div><div class="line">class RefCountedObject : public T &#123;</div><div class="line"> public:</div><div class="line">  RefCountedObject() &#123;&#125;</div><div class="line"></div><div class="line">  template &lt;class P0&gt;</div><div class="line">  explicit RefCountedObject(P0&amp;&amp; p0) : T(std::forward&lt;P0&gt;(p0)) &#123;&#125;</div><div class="line"></div><div class="line">  template &lt;class P0, class P1, class... Args&gt;</div><div class="line">  RefCountedObject(P0&amp;&amp; p0, P1&amp;&amp; p1, Args&amp;&amp;... args)</div><div class="line">      : T(std::forward&lt;P0&gt;(p0),</div><div class="line">          std::forward&lt;P1&gt;(p1),</div><div class="line">          std::forward&lt;Args&gt;(args)...) &#123;&#125;</div><div class="line"></div><div class="line">  virtual int AddRef() const &#123; return AtomicOps::Increment(&amp;ref_count_); &#125;</div><div class="line"></div><div class="line">  virtual int Release() const &#123;</div><div class="line">    int count = AtomicOps::Decrement(&amp;ref_count_);</div><div class="line">    if (!count) &#123;</div><div class="line">      delete this;</div><div class="line">    &#125;</div><div class="line">    return count;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // Return whether the reference count is one. If the reference count is used</div><div class="line">  // in the conventional way, a reference count of 1 implies that the current</div><div class="line">  // thread owns the reference and no other thread shares it. This call</div><div class="line">  // performs the test for a reference count of one, and performs the memory</div><div class="line">  // barrier needed for the owning thread to act on the object, knowing that it</div><div class="line">  // has exclusive access to the object.</div><div class="line">  virtual bool HasOneRef() const &#123;</div><div class="line">    return AtomicOps::AcquireLoad(&amp;ref_count_) == 1;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"> protected:</div><div class="line">  virtual ~RefCountedObject() &#123;&#125;</div><div class="line"></div><div class="line">  mutable volatile int ref_count_ = 0;</div><div class="line">&#125;;</div><div class="line"></div><div class="line">&#125;  // namespace rtc</div></pre></td></tr></table></figure></p>
<h3 id="线程Thread"><a href="#线程Thread" class="headerlink" title="线程Thread"></a>线程Thread</h3><h3 id="网络Socket"><a href="#网络Socket" class="headerlink" title="网络Socket"></a>网络Socket</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;src/webrtc/base&lt;/code&gt;是webrtc基础平台库，包括线程、锁、socket,智能指针等.&lt;/p&gt;
&lt;h3 id=&quot;智能指针&quot;&gt;&lt;a href=&quot;#智能指针&quot; class=&quot;headerlink&quot; title=&quot;智能指针&quot;&gt;&lt;/a&gt;智能指针&lt;
    
    </summary>
    
      <category term="webrtc" scheme="http://wodekouwei.com/categories/webrtc/"/>
    
    
      <category term="webrtc" scheme="http://wodekouwei.com/tags/webrtc/"/>
    
      <category term="源码" scheme="http://wodekouwei.com/tags/%E6%BA%90%E7%A0%81/"/>
    
  </entry>
  
</feed>
