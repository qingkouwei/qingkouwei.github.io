<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="老司机种菜" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="老司机种菜">
<meta property="og:url" content="http://wodekouwei.com/page/10/index.html">
<meta property="og:site_name" content="老司机种菜">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="老司机种菜">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://wodekouwei.com/page/10/"/>





  <title> 老司机种菜 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?2021aa5f03a4203621d42ef374e0d5f7";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">老司机种菜</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/07/06/webrtc-modules/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/06/webrtc-modules/" itemprop="url">
                  WebRTC的模块处理机制
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-06T17:09:30+08:00">
                2017-07-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/webrtc/" itemprop="url" rel="index">
                    <span itemprop="name">webrtc</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对于实时音视频应用来讲，媒体数据从采集到渲染，在数据流水线上依次完成一系列处理。流水线由不同的功能模块组成，彼此分工协作：数据采集模块负责从摄像头/麦克风采集音视频数据，编解码模块负责对数据进行编解码，RTP模块负责数据打包和解包。数据流水线上的数据处理速度是影响应用实时性的最重要因素。与此同时，从服务质量保证角度讲，应用需要知道数据流水线的运行状态，如视频采集模块的实时帧率、当前网络的实时速率、接收端的数据丢包率，等等。各个功能模块可以基于这些运行状态信息作相应调整，从而在质量、速度等方面优化数据流水线的运行，实现更快、更好的用户体验。</p>
<p>WebRTC采用模块机制，把数据流水线上功能相对独立的处理点定义为模块，每个模块专注于自己的任务，模块之间基于数据流进行通信。与此同时，专有线程收集和处理模块内部的运行状态信息，并把这些信息反馈到目标模块，实现模块运行状态监控和服务质量保证。本文在深入分析WebRTC源代码基础上，学习研究其模块处理机制的实现细节，从另一个角度理解WebRTC的技术原理。</p>
<h3 id="1-WebRTC数据流水线"><a href="#1-WebRTC数据流水线" class="headerlink" title="1 WebRTC数据流水线"></a>1 WebRTC数据流水线</h3><p>我们可以把WebRTC看作是一个专注于实时音视频通信的SDK。其对外的API主要负责PeerConnection建立、MediaStream创建、NAT穿透、SDP协商等工作，对内则主要集中于音视频数据的处理，从数据采集到渲染的整个处理过程可以用一个数据流水线来描述，如图1所示。
<img src="http://images.wodekouwei.com/media/webrtc_module_pipeline.png" alt="图1 WebRTC音视频数据流水线"></p>
<p>音视频数据首先从采集端进行采集，一般来说音频数据来自麦克风，视频数据来自摄像头。在某些应用场景下，音频数据来自扬声器，视频数据来自桌面共享。采集端的输出是音视频Raw Data。然后Raw Data到达编码模块，数据被编码器编码成符合语法规则的NAL单元，到达发送端缓冲区PacedSender处。接下来PacedSender把NAL单元发送到RTP模块打包为RTP数据包，最后经过网络模块发送到网络。</p>
<p>在接收端，RTP数据包经过网络模块接收后进行解包得到NAL单元，接下来NAL单元到达接收端缓冲区(JitterBuffer或者NetEQ)进行乱序重排和组帧。一帧完整的数据接收并组帧之后，调用解码模块进行解码，得到该帧数据的Raw Data。最后Raw Data交给渲染模块进行播放/显示。</p>
<p>在数据流水线上，还有一系列模块负责服务质量监控，如丢帧策略，丢包策略，编码器过度使用保护，码率估计，前向纠错，丢包重传，等等。</p>
<p>WebRTC数据流水线上的功能单元被定义为模块，每个模块从上游模块获取输入数据，在本模块进行加工后得到输出数据，交给下游模块进行下一步处理。WebRTC的模块处理机制包括模块和模块处理线程，前者把WebRTC数据流水线上的功能部件封装为模块，后者驱动模块内部状态更新和模块之间状态传递。模块一般挂载到模块处理线程上，由处理线程驱动模块的处理函数。下面分别描述之。</p>
<h3 id="WebRTC模块"><a href="#WebRTC模块" class="headerlink" title="WebRTC模块"></a>WebRTC模块</h3><p>WebRTC模块虚基类Module定义在<code>webrtc/modules/include/modue.h</code>中，如图2所示。
<img src="http://images.wodekouwei.com/media/webrtc_module_module.png" alt="图2 模块虚基类Module定义"></p>
<p>Module虚基类对外提供三个函数作为API：TimeUntilNextProcess()用来计算距下次调用处理函数Process()的时间间隔；Process()是模块的处理函数，负责模块内部运行监控、状态更新和模块间通信；ProcessThreadAttached()用来把模块挂载到模块处理线程，或者从模块处理线程分离出来，实际实现中这个函数暂时没有被用到。</p>
<p>Module的派生类分布在WebRTC数据流水线上的不同部分，各自承担自己的数据处理和服务质量保证任务。</p>
<h3 id="3-WebRTC模块处理线程"><a href="#3-WebRTC模块处理线程" class="headerlink" title="3 WebRTC模块处理线程"></a>3 WebRTC模块处理线程</h3><p>WebRTC模块处理线程是模块处理机制的驱动器，它的核心作用是对所有挂载在本线程下的模块，周期性调用其Process()处理函数处理模块内部事务，并处理异步任务。其虚基类ProcessThread和派生类ProcessThreadImpl如图3所示。</p>
<p><img src="http://images.wodekouwei.com/media/webrtc_module_thread.png" alt="图3 模块处理线程虚基类ProcessThread及派生类ProcessThreadImpl"></p>
<p>ProcessThread基类提供一系列API完成线程功能：Start()/Stop()函数用来启动和结束线程；WakeUp()函数用来唤醒挂载在本线程下的某个模块，使得该模块有机会马上执行其Process()处理函数；PostTask()函数用来邮递一个任务给本线程；RegisterModule()和DeRegisterModule()用来向线程注册/注销模块。</p>
<p>WebRTC基于ProcessThread线程实现派生类ProcessThreadImpl，如图3所示。在成员变量方面，wake<em>up</em>用来唤醒处于等待状态的线程；thread<em>是平台相关的线程实现如POSIX线程；modules</em>是注册在本线程下的模块集合；queue_是邮递给本线程的任务集合；thread<em>name</em>是线程名字。在成员函数方面，Process()完成ProcessThread的核心任务，其伪代码如下所示。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">bool ProcessThreadImpl::Process() &#123;</div><div class="line">    for (ModuleCallback&amp; m : modules_) &#123;</div><div class="line">      if (m.next_callback == 0)</div><div class="line">        m.next_callback = GetNextCallbackTime(m.module, now);</div><div class="line">      if (m.next_callback &lt;= now || m.next_callback == kCallProcessImmediately) &#123;</div><div class="line">        m.module-&gt;Process();</div><div class="line">        m.next_callback = GetNextCallbackTime(m.module, rtc::TimeMillis(););</div><div class="line">      &#125;</div><div class="line">      if (m.next_callback &lt; next_checkpoint)</div><div class="line">        next_checkpoint = m.next_callback;</div><div class="line">    &#125;</div><div class="line">    while (!queue_.empty()) &#123;</div><div class="line">      ProcessTask* task = queue_.front();</div><div class="line">      queue_.pop();</div><div class="line">      task-&gt;Run();</div><div class="line">      delete task;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  int64_t time_to_wait = next_checkpoint - rtc::TimeMillis();</div><div class="line">  if (time_to_wait &gt; 0)</div><div class="line">    wake_up_-&gt;Wait(static_cast&lt;unsigned long&gt;(time_to_wait));</div><div class="line">  return true;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Process()函数首先处理挂载在本线程下的模块，这也是模块处理线程的核心任务：针对每个模块，计算其下次调用模块Process()处理函数的时刻(调用该模块的TimeUntilNextProcess()函数)。如果时刻是当前时刻，则调用模块的Process()处理函数，并更新下次调用时刻。需要注意，不同模块的执行频率不一样，线程在本轮调用末尾的等待时间和本线程下所有模块的最近下次调用时刻相关。</p>
<p>接下来线程Process()函数处理ProcessTask队列中的异步任务，针对每个任务调用Run()函数，然后任务出队列并销毁。等模块调用和任务都处理完后，则把本次时间片的剩余时间等待完毕，然后返回。如果在等待期间其他线程向本线程Wakeup模块或者邮递一个任务，则线程被立即唤醒并返回，进行下一轮时间片的执行。</p>
<p>至此，关于WebRTC的模块和模块处理线程的基本实现分析完毕，下一节将对WebRTC SDK内模块实例和模块处理线程实例进行详细分析。</p>
<h3 id="4-WebRTC模块处理线程实例"><a href="#4-WebRTC模块处理线程实例" class="headerlink" title="4 WebRTC模块处理线程实例"></a>4 WebRTC模块处理线程实例</h3><p>WebRTC关于模块和处理线程的实现在webrtc/modules目录下，该目录汇集了所有派生类模块和模块处理线程的实现及实例分布。本节对这些内容进行总结。</p>
<p>WebRTC目前创建三个ProcessThreadImpl线程实例，分别是负责处理音频的VoiceProcessTread，负责处理视频和音视频同步的ModuleProcessThread，以及负责数据平滑发送的PacerThread。这三个线程和挂载在线程下的模块如图4所示。
<img src="http://images.wodekouwei.com/media/webrtc_module_thread_instance.png" alt="图4 模块处理线程实例"></p>
<p>VoiceProcessThread线程由Worker线程在创建VoiceEngine时创建，负责音频端模块的处理。挂载在该线程下的模块如图4所示，其中MonitorModule负责对音频数据混音处理过程中产生的警告和错误进行处理，AudioDeviceModuleImpl负责对音频设备采集和播放音频数据时产生的警告和错误进行处理，ModuleRtpRtcpImpl负责音频RTP数据包发送过程中的码率计算、RTT更新、RTCP报文发送等内容。</p>
<p>ModuleProcessThread线程由Worker线程在创建VideoChannel时创建，负责视频端模块的处理。挂载在该线程下的模块如图4所示，其中CallStats负责Call对象统计数据(如RTT)的更新，CongestionController负责拥塞控制[1][2]，VideoSender负责视频发送端统计数据的更新，VideoReceiver负责视频接收端统计数据更新和处理状态反馈(如请求关键帧)，ModuleRtpRtcpImpl负责视频RTP数据包发送过程中的码率计算、RTT更新、RTCP报文发送等内容，OveruseFrameDetector负责视频帧采集端过载监控，ReceiveStatisticsImpl负责由接收端统计数据触发的码率更新过程，ViESyncModule负责音视频同步。</p>
<p>PacerThread线程由Worker线程在创建VideoChannel时创建，负责数据平滑发送。挂载在该线程下的PacedSender负责发送端数据平滑发送；RemoteEstimatorProxy派生自RemoteBitrateEstimator，负责在启用发送端码率估计的情况下把接收端收集到的反馈信息发送回发送端。</p>
<p>由以上分析可知，WebRTC创建的模块处理线程实例基本上涵盖了音视频数据从采集到渲染过程中的大部分数据操作。但还有一些模块不依赖于模块线程工作，这部分模块是少数，本文不展开具体的描述。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1] <a href="http://www.jianshu.com/p/0f7ee0e0b3be" target="_blank" rel="external">WebRTC基于GCC的拥塞控制(上) – 算法分析</a>
[2] <a href="http://www.jianshu.com/p/5259a8659112" target="_blank" rel="external">WebRTC基于GCC的拥塞控制(下) - 实现分析</a></p>
<h3 id="转至"><a href="#转至" class="headerlink" title="转至"></a><a href="http://www.th7.cn/Program/Android/201701/1093218.shtml" target="_blank" rel="external">转至</a></h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/07/03/tips-android-performance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/03/tips-android-performance/" itemprop="url">
                  Android应用性能优化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-03T22:53:59+08:00">
                2017-07-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Android/" itemprop="url" rel="index">
                    <span itemprop="name">Android</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Android手机由于其本身的后台机制和硬件特点，性能上一直被诟病，所以软件开发者对软件本身的性能优化就显得尤为重要；本文将对Android开发过程中性能优化的各个方面做一个回顾与总结。</p>
<h3 id="Cache优化"><a href="#Cache优化" class="headerlink" title="Cache优化"></a>Cache优化</h3><ul>
<li><p>ListView缓存：</p>
<ul>
<li>ListView中有一个回收器，Item滑出界面的时候View会回收到这里，需要显示新的Item的时候，就尽量重用回收器里面的View；每次在getView函数中inflate新的item之前会先判断回收器中是否有缓存的view，即判断convertView是否为null，是则inflate一个新的item View，否则重用回收器中的item。</li>
<li>此外，ListView还使用静态的ViewHolder减少findViewById的次数</li>
<li>ListView中有getViewTypeCount()函数用于获取列表有几种布局类型，getItemViewType(int position)用于获取在position位置上的布局类型; 我们可以利用ViewType来给不同类型的item创建不同的View，这样可以利于ListView的回收</li>
<li>对Item中图片进行适当压缩, 并进行异步加载；如果快速滑动，不加载图片；实现数据的分页加载</li>
</ul>
</li>
<li><p>IO缓存：在文件和网络IO中使用具有缓存策略的输入流，BufferedInputStream替代InputStream，BufferedReader替代Reader，BufferedReader替代BufferedInputStream</p>
</li>
<li><p>data缓存(空间换时间)：①缓存数据库的查询结果，比如缓存数据库表中的数据条数，这样就可以避免多次的select count查询 ②缓存磁盘文件中需要频繁访问的数据到内存中 ③缓存耗时计算的结果</p>
</li>
</ul>
<h3 id="Battery优化"><a href="#Battery优化" class="headerlink" title="Battery优化"></a>Battery优化</h3><ul>
<li>cpu的使用率和使用频率将直接或间接的影响电量的分配和使用，cpu降频可以节约电量</li>
<li><p>service优化</p>
<ul>
<li><p>service作为一个运行在主线程中的后台服务，应该尽量避免耗时动作，而应该尽量新开线程去处理耗时动作
监听系统广播看service是否存活，否则kill掉；降低service优先级使得系统内存吃紧时会被自动kill掉</p>
</li>
<li><p>使用Android提供的IntentService代替service，因为IntentService会在运行完成之后自动停止，而service需要手动调用stopService()才能停止运行</p>
</li>
<li><p>定时执行任务的Alarm机制：Android的定时任务有两种实现方式，Timer类和Alarm机制；Timer不适合长期后台运行的定时任务。因为每种手机都会有自己的休眠策略，Android手机就会在长时间不操作的情况下自动让CPU进入到睡眠状态，这就有可能导致Timer中的定时任务无法正常运行。而Alarm机制则不存在这种情况，它具有唤醒CPU的功能，即可以保证每次需要执行定时任务的时候CPU能正常工作。然而从Android4.4之后，Alarm任务的触发时间将会变得不准确，有可能会延迟一段时间后任务才能得到执行。这不是bug，而是系统在耗电性方面进行的优化。系统会自动检测目前有多少Alarm任务存在，然后将触发时间将近的几个任务放在一起执行，这就可以大幅度的减少CPU被唤醒的次数，从而有效延长电池的使用时间</p>
</li>
</ul>
</li>
</ul>
<h3 id="渲染层优化"><a href="#渲染层优化" class="headerlink" title="渲染层优化"></a>渲染层优化</h3><ul>
<li><p>Android 界面卡顿的原因？</p>
<ul>
<li>UI线程中做耗时操作，比如进行网络请求,磁盘读取，位图修改，更新UI等耗时操作，从而导致UI线程卡顿</li>
<li>布局Layout过于复杂，无法在16ms内完成渲染，或者嵌套层次过深</li>
<li>View过度绘制或者频繁的触发measure、layout，同一时间动画执行的次数过多，导致CPU或GPU负载过重</li>
<li>冗余资源及逻辑等导致加载和执行缓慢</li>
</ul>
</li>
<li>Android 界面卡顿怎么处理？<ul>
<li>xml布局优化：尽量使用include、merge、ViewStub标签，尽量不存在冗余嵌套及过于复杂布局（譬如10层就会直接异常），例如使用RelativeLayout代替LinearLayout可以减少布局层次和复杂性，View的嵌套层次不能过深，尽量使用GONE替换INVISIBLE，使用weight后尽量将width和heigh设置为0dp，减少运算，Item存在非常复杂的嵌套时考虑使用自定义Item View来取代，减少measure与layout次数等。</li>
<li>ListView及Adapter优化；尽量复用getView方法中的相关View，不重复获取实例导致卡顿，列表尽量在滑动过程中不进行UI元素刷新等。</li>
<li>背景和图片等内存分配优化；尽量减少不必要的背景设置，图片尽量压缩处理显示，尽量避免频繁内存抖动等问题出现；尽可能为不同分辨率创建资源，以减少不必要的硬件缩放</li>
<li>自定义View等绘图与布局优化；尽量避免在draw、measure、layout中做过于耗时及耗内存操作，尤其是draw方法中，尽量减少draw、measure、layout等执行次数，避免过度渲染和绘制；减少不必要的inflate，尽量使用全局变量缓存View</li>
<li>避免ANR，不要在UI线程中做耗时操作，譬如多次数据库操作等</li>
</ul>
</li>
<li><p>Layout常用的标签</p>
<ul>
<li><p>include标签：该标签可以用于将布局文件中的公共部分提取出来给其它布局文件复用，从而使得布局模块化，代码轻量化; 注意点: ①如果标签已经定义了id，而嵌入布局文件的root布局文件也定义了id，标签的id会覆盖掉嵌入布局文件root的id，如果include标签没有定义id则会使用嵌入文件root的id ②如果想使用标签覆盖嵌入布局root布局属性，必须同时覆盖layout_height和layout_width属性，否则会直接报编译时语法错误</p>
</li>
<li><p>viewstub标签：该标签与include一样用于引入布局模块，只是引入的布局默认不会扩张，既不会占用显示也不会占用位置，从而在解析layout时节省cpu和内存，只有通过调用setVisibility函数或者Inflate函数才会将其要装载的目标布局给加载出来，从而达到延迟加载的效果；例如条目详情、进度条标识或者未读消息等，这些情况如果在一开始初始化，虽然设置可见性View.GONE,但是在Inflate的时候View仍然会被Inflate，仍然会创建对象。</p>
</li>
<li><p>merge标签：该标签在layout中会被自动忽略，从而减少一层布局嵌套，其主要用处是当一个布局作为子布局被其他布局include时，使用merge当作该布局的顶节点来代替layout顶节点就可以减少一层嵌套</p>
</li>
<li><p>hierarchy viewer：该工具可以方便的查看Activity的布局，各个View的属性、measure、layout、draw的时间，如果耗时较多会用红色标记，否则显示绿色</p>
</li>
</ul>
</li>
</ul>
<h3 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h3><ul>
<li>异步请求网络数据，避免频繁请求数据（例如如果某个页面内请求过多，可以考虑做一定的请求合并），尽可能的减少网络请求次数和减小网络请求时间间隔</li>
<li>网络应用传输中使用高效率的数据格式，譬如使用JSON代替XML，使用WebP代替其他图片格式,并对数据进行Gzip压缩数据，比如post请求的body和header内容</li>
<li>及时缓存数据到内存/文件/数据库</li>
<li><p>执行某些操作前尽量先进行网络状态判断，比如wifi传输数据比蜂窝数据更省电，所以尽量在wifi下进行数据的预加载</p>
</li>
<li><p>httpClient和httpUrlConnection对比：</p>
<ul>
<li>httpClient是apache的开源实现，API数量多，非常稳定</li>
<li>httpUrlConnection是java自带的模块: ①可以直接支持GZIP压缩,而HttpClient虽然也支持GZIP，但要自己写代码处理 ②httpUrlConnection直接在系统层面做了缓存策略处理，加快重复请求的速度 ③API简单，体积较小,而且直接支持系统级连接池，即打开的连接不会直接关闭，在一段时间内所有程序可共用</li>
<li>HttpURLConnection在Android2.2之前有个重大Bug，调用close()函数会影响连接池，导致连接复用失效，需要关闭keepAlive;因此在2.2之前http请求都是用httpClient，2.2之后则是使用HttpURLConnection</li>
<li>但是!!!现在!!!Android不再推荐这两种方式！二是直接使用OKHttp这种成熟方案！支持Android 2.3及其以上版本</li>
</ul>
</li>
</ul>
<h3 id="数据结构优化"><a href="#数据结构优化" class="headerlink" title="数据结构优化"></a>数据结构优化</h3><ul>
<li><p>ArrayList和LinkedList的选择：ArrayList根据index取值更快，LinkedList更占内存、随机插入删除更快速、扩容效率更高</p>
</li>
<li><p>ArrayList、HashMap、LinkedHashMap、HashSet的选择：hash系列数据结构查询速度更优，ArrayList存储有序元素，HashMap为键值对数据结构，LinkedHashMap可以记住加入次序的hashMap，HashSet不允许重复元素</p>
</li>
<li><p>HashMap、WeakHashMap选择：WeakHashMap中元素可在适当时候被系统垃圾回收器自动回收，所以适合在内存紧张时使用</p>
</li>
<li><p>Collections.synchronizedMap和ConcurrentHashMap的选择：ConcurrentHashMap为细分锁，锁粒度更小，并发性能更优；Collections.synchronizedMap为对象锁，自己添加函数进行锁控制更方便</p>
</li>
<li><p>Android中性能更优的数据类型：如SparseArray、SparseBooleanArray、SparseIntArray、Pair；Sparse系列的数据结构是为key为int情况的特殊处理，采用二分查找及简单的数组存储，加上不需要泛型转换的开销，相对Map来说性能更优</p>
</li>
</ul>
<h3 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h3><ul>
<li>Android应用内存溢出OOM<ul>
<li>内存溢出的主要导致原因有如下几类：①应用代码存在内存泄露，长时间积累无法释放导致OOM；②应用的某些逻辑操作疯狂的消耗掉大量内存（譬如加载一张不经过处理的超大超高清图片等）导致超过阈值OOM</li>
<li>解决思路①在内存引用上做些处理，常用的有软引用、弱引用 ②在内存中加载图片时直接在内存中做处理，如：边界压缩 ③动态回收内存，手动recyle bitmap，回收对象 ④优化Dalvik虚拟机的堆内存分配 ⑤自定义堆内存大小</li>
</ul>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/06/27/media-terminology/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/27/media-terminology/" itemprop="url">
                  多媒体之术语
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-27T22:29:35+08:00">
                2017-06-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/media/" itemprop="url" rel="index">
                    <span itemprop="name">media</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一-编解码术语"><a href="#一-编解码术语" class="headerlink" title="一.编解码术语"></a>一.编解码术语</h2><h3 id="1-GOP-码流-比特率-帧速率-分辨率"><a href="#1-GOP-码流-比特率-帧速率-分辨率" class="headerlink" title="1.GOP/码流/比特率/帧速率/分辨率"></a>1.GOP/码流/比特率/帧速率/分辨率</h3><h4 id="1-1GOP（Group-of-picture）"><a href="#1-1GOP（Group-of-picture）" class="headerlink" title="1.1GOP（Group of picture）"></a>1.1GOP（Group of picture）</h4><p>关键帧的周期，也就是两个IDR帧之间的距离，一个帧组的最大帧数，一般而言，每一秒视频至少需要使用 1 个关键帧。增加关键帧个数可改善质量，但是同时增加带宽和网络负载。</p>
<p>需要说明的是，通过提高GOP值来提高图像质量是有限度的，在遇到场景切换的情况时，H.264编码器会自动强制插入一个I帧，此时实际的GOP值被缩短了。另一方面，在一个GOP中，P、B帧是由I帧预测得到的，当I帧的图像质量比较差时，会影响到一个GOP中后续P、B帧的图像质量，直到下一个GOP开始才有可能得以恢复，所以GOP值也不宜设置过大。</p>
<p>同时，由于P、B帧的复杂度大于I帧，所以过多的P、B帧会影响编码效率，使编码效率降低。另外，过长的GOP还会影响Seek操作的响应速度，由于P、B帧是由前面的I或P帧预测得到的，所以Seek操作需要直接定位，解码某一个P或B帧时，需要先解码得到本GOP内的I帧及之前的N个预测帧才可以，GOP值越长，需要解码的预测帧就越多，seek响应的时间也越长。</p>
<h5 id="1-12CABAC-CAVLC"><a href="#1-12CABAC-CAVLC" class="headerlink" title="1.12CABAC/CAVLC"></a>1.12CABAC/CAVLC</h5><p>H.264/AVC标准中两种熵编码方法，CABAC叫自适应二进制算数编码，CAVLC叫前后自适应可变长度编码，</p>
<ol>
<li>CABAC：是一种无损编码方式，画质好，X264就会舍弃一些较小的DCT系数，码率降低，可以将码率再降低10-15%（特别是在高码率情况下），会降低编码和解码的速速。</li>
<li>CAVLC将占用更少的CPU资源，但会影响压缩性能。</li>
</ol>
<p>其他:</p>
<ul>
<li>帧：当采样视频信号时，如果是通过逐行扫描，那么得到的信号就是一帧图像，通常帧频为25帧每秒（PAL制）、30帧每秒（NTSC制）；</li>
<li>场：当采样视频信号时，如果是通过隔行扫描（奇、偶数行），那么一帧图像就被分成了两场，通常场频为50Hz（PAL制）、60Hz（NTSC制）；</li>
<li>帧频、场频的由来：最早由于抗干扰和滤波技术的限制，电视图像的场频通常与电网频率（交流电）相一致，于是根据各地交流电频率不同就有了欧洲和中国等PAL制的50Hz和北美等NTSC制的60Hz，但是现在并没有这样的限制了，帧频可以和场频一样，或者场频可以更高。</li>
<li>帧编码、场编码方式：逐行视频帧内邻近行空间相关性较强，因此当活动量非常小或者静止的图像比较适宜采用帧编码方式；而场内相邻行之间的时间相关性较强，对运动量较大的运动图像则适宜采用场编码方式。</li>
</ul>
<h5 id="1-1-3Deblocking"><a href="#1-1-3Deblocking" class="headerlink" title="1.1.3Deblocking"></a>1.1.3Deblocking</h5><p>开启会减少块效应.</p>
<h5 id="1-1-4FORCE-IDR"><a href="#1-1-4FORCE-IDR" class="headerlink" title="1.1.4FORCE_IDR"></a>1.1.4FORCE_IDR</h5><p>是否让每个I帧变成IDR帧，如果是IDR帧，支持随机访问。</p>
<h5 id="1-1-5frame-tff-bff"><a href="#1-1-5frame-tff-bff" class="headerlink" title="1.1.5frame,tff,bff"></a>1.1.5frame,tff,bff</h5><p>–frame 将两场合并作为一帧进行编码,–tff Enable interlaced mode (开启隔行编码并设置上半场在前),–bff Enable interlaced mode。</p>
<p>PAFF 和MBAFF：当对隔行扫描图像进行编码时，每帧包括两个场，由于两个场之间存在较大的扫描间隔，这样，对运动图像来说，帧中相邻两行之间的空间相关性相对于逐行扫描时就会减小，因此这时对两个场分别进行编码会更节省码流。</p>
<p>对帧来说，存在三种可选的编码方式：将两场合并作为一帧进行编码(frame 方式)或将两场分别编码(field 方式)或将两场合并起来作为一帧，但不同的是将帧中垂直相邻的两个宏块合并为宏块对进行编码；前两种称为PAFF 编码，对运动区域进行编码时field 方式有效，对非运区域编码时，由于相邻两行有较大的相关性，因而frame 方式会更有效。当图像同时存在运动区域和非运动区域时，在MB 层次上，对运动区域采取field 方式，对非运动区域采取frame 方式会更加有效，这种方式就称为MBAFF，预测的单位是宏块对。</p>
<h4 id="1-2码流-码率"><a href="#1-2码流-码率" class="headerlink" title="1.2码流/码率"></a>1.2码流/码率</h4><p>码流(Data Rate)是指视频文件在单位时间内使用的数据流量，也叫码率或码流率，通俗一点的理解就是取样率,是视频编码中画面质量控制中最重要的部分，一般我们用的单位是kb/s或者Mb/s。一般来说同样分辨率下，视频文件的码流越大，压缩比就越小，画面质量就越高。码流越大，说明单位时间内取样率越大，数据流，精度就越高，处理出来的文件就越接近原始文件，图像质量越好，画质越清晰，要求播放设备的解码能力也越高。</p>
<p>当然，码流越大，文件体积也越大，其计算公式是文件体积=时间X码率/8。例如，网络上常见的一部90分钟1Mbps码流的720P RMVB文件，其体积就=5400秒×1Mb/8=675MB。</p>
<p>通常来说，一个视频文件包括了画面及声音，例如一个RMVB的视频文件，里面包含了视频信息和音频信息，音频及视频都有各自不同的采样方式和比特率，也就是说，同一个视频文件音频和视频的比特率并不是一样的。而我们所说的一个视频文件码流率大小，一般是指视频文件中音频及视频信息码流率的总和。</p>
<p>以以国内最流行，大家最熟悉的RMVB视频文件为例，RMVB中的VB，指的是VBR，即Variable Bit Rate的缩写，中文含义是可变比特率，它表示RMVB采用的是动态编码的方式，把较高的采样率用于复杂的动态画面(歌舞、飞车、战争、动作等)，而把较低的采样率用于静态画面，合理利用资源，达到画质与体积可兼得的效果。</p>
<p>码率和取样率最根本的差别就是码率是针对源文件来讲的。</p>
<h4 id="1-3采样率"><a href="#1-3采样率" class="headerlink" title="1.3采样率"></a>1.3采样率</h4><p>采样率（也称为采样速度或者采样频率）定义了每秒从连续信号中提取并组成离散信号的采样个数，它用赫兹（Hz）来表示。采样率是指将模拟信号转换成数字信号时的采样频率，也就是单位时间内采样多少点。一个采样点数据有多少个比特。比特率是指每秒传送的比特(bit)数。单位为 bps(Bit Per Second)，比特率越高，传送的数据越大，音质越好.比特率 =采样率 x 采用位数 x声道数.</p>
<p>采样率类似于动态影像的帧数，比如电影的采样率是24赫兹，PAL制式的采样率是25赫兹，NTSC制式的采样率是30赫兹。当我们把采样到的一个个静止画面再以采样率同样的速度回放时，看到的就是连续的画面。同样的道理，把以44.1kHZ采样率记录的CD以同样的速率播放时，就能听到连续的声音。显然，这个采样率越高，听到的声音和看到的图像就越连贯。当然，人的听觉和视觉器官能分辨的采样率是有限的，基本上高于44.1kHZ采样的声音，绝大部分人已经觉察不到其中的分别了。</p>
<p>而声音的位数就相当于画面的颜色数，表示每个取样的数据量，当然数据量越大，回放的声音越准确，不至于把开水壶的叫声和火车的鸣笛混淆。同样的道理，对于画面来说就是更清晰和准确，不至于把血和西红柿酱混淆。不过受人的器官的机能限制，16位的声音和24位的画面基本已经是普通人类的极限了，更高位数就只能靠仪器才能分辨出来了。比如电话就是3kHZ取样的7位声音，而CD是44.1kHZ取样的16位声音，所以CD就比电话更清楚。</p>
<p>当你理解了以上这两个概念，比特率就很容易理解了。以电话为例，每秒3000次取样，每个取样是7比特，那么电话的比特率是21000。 而CD是每秒 44100次取样，两个声道，每个取样是13位PCM编码，所以CD的比特率是44100<em>2</em>13=1146600，也就是说CD每秒的数据量大约是 144KB，而一张CD的容量是74分等于4440秒，就是639360KB＝640MB。</p>
<p>码率和取样率最根本的差别就是码率是针对源文件来讲的</p>
<h4 id="1-4比特率"><a href="#1-4比特率" class="headerlink" title="1.4比特率"></a>1.4比特率</h4><p>比特率是指每秒传送的比特(bit)数。单位为bps(Bit Per Second)，比特率越高，传送的数据越大。在视频领域,比特率常翻译为码率 !!!</p>
<p>比特率表示经过编码（压缩）后的音、视频数据每秒钟需要用多少个比特来表示，而比特就是二进制里面最小的单位，要么是0，要么是1。比特率与音、视频压缩的关系，简单的说就是比特率越高，音、视频的质量就越好，但编码后的文件就越大；如果比特率越少则情况刚好相反。</p>
<p>比特率是指将数字声音、视频由模拟格式转化成数字格式的采样率，采样率越高，还原后的音质、画质就越好。</p>
<p>常见编码模式：</p>
<ul>
<li>VBR（Variable Bitrate）动态比特率 也就是没有固定的比特率，压缩软件在压缩时根据音频数据即时确定使用什么比特率，这是以质量为前提兼顾文件大小的方式，推荐编码模式；</li>
<li>ABR（Average Bitrate）平均比特率 是VBR的一种插值参数。LAME针对CBR不佳的文件体积比和VBR生成文件大小不定的特点独创了这种编码模式。ABR在指定的文件大小内，以每50帧（30帧约1秒）为一段，低频和不敏感频率使用相对低的流量，高频和大动态表现时使用高流量，可以做为VBR和CBR的一种折衷选择。</li>
<li>CBR（Constant Bitrate），常数比特率 指文件从头到尾都是一种位速率。相对于VBR和ABR来讲，它压缩出来的文件体积很大，而且音质相对于VBR和ABR不会有明显的提高。</li>
</ul>
<h4 id="1-5帧速率"><a href="#1-5帧速率" class="headerlink" title="1.5帧速率"></a>1.5帧速率</h4><p>帧速率也称为FPS(Frames PerSecond)的缩写——帧/秒。是指每秒钟刷新的图片的帧数，也可以理解为图形处理器每秒钟能够刷新几次。越高的帧速率可以得到更流畅、更逼真的动画。每秒钟帧数(FPS)越多，所显示的动作就会越流畅。</p>
<h4 id="1-6分辨率"><a href="#1-6分辨率" class="headerlink" title="1.6分辨率"></a>1.6分辨率</h4><p>就是帧大小每一帧就是一副图像。</p>
<p>640*480分辨率的视频，建议视频的码速率设置在700以上，音频采样率44100就行了</p>
<p>一个音频编码率为128Kbps，视频编码率为800Kbps的文件，其总编码率为928Kbps，意思是经过编码后的数据每秒钟需要用928K比特来表示。</p>
<p>视频分辨率是指视频成像产品所成图像的大小或尺寸。常见的视像分辨率有352×288，176×144，640×480，1024×768。在成像的两组数字中，前者为图片长度，后者为图片的宽度，两者相乘得出的是图片的像素，长宽比一般为4：3. 　目前监控行业中主要使用Qcif(176×144）、CIF(352×288）、HALF D1(704×288）、D1(704×576）等几种分辨率。
D1是数字电视系统显示格式的标准，共分为以下5种规格：</p>
<ol>
<li>D1：480i格式（525i）：720×480（水平480线，隔行扫描），和NTSC模拟电视清晰度相同，行频为15.25kHz，相当于我们所说的4CIF(720×576)</li>
<li>D2：480P格式（525p）：720×480（水平480线，逐行扫描），较D1隔行扫描要清晰不少，和逐行扫描DVD规格相同，行频为31.5kHz</li>
<li>D3：1080i格式（1125i）：1920×1080（水平1080线，隔行扫描），高清方式采用最多的一种分辨率，分辨率为1920×1080i/60Hz，行频为33.75kHz</li>
<li>D4：720p格式（750p）：1280×720（水平720线，逐行扫描），虽然分辨率较D3要低，但是因为逐行扫描，市面上更多人感觉相对于1080I（实际逐次540线）视觉效果更加清晰。不过个人感觉来说，在最大分辨率达到1920×1080的情况下，D3要比D4感觉更加清晰，尤其是文字表现力上，分辨率为1280×720p/60Hz，行频为45kHz</li>
<li>D5：1080p格式（1125p）：1920×1080（水平1080线，逐行扫描），目前民用高清视频的最高标准，分辨率为1920×1080P/60Hz，行频为67.5KHZ。</li>
</ol>
<p>其中D1 和D2标准是我们一般模拟电视的最高标准，并不能称的上高清晰，D3的1080i标准是高清晰电视的基本标准，它可以兼容720p格式，而D5的1080P只是专业上的标准。</p>
<p>计算输出文件大小公式：
（音频编码率（KBit为单位）/8 +视频编码率（KBit为单位）/8）×影片总长度（秒为单位）=文件大小（MB为单位）</p>
<h3 id="2-高清视频"><a href="#2-高清视频" class="headerlink" title="2.高清视频"></a>2.高清视频</h3><p>目前的720P以及1080P采用了很多种编码，例如主流的MPEG2，VC-1以及H.264，还有Divx以及Xvid，至于封装格式更多到令人发指，ts、mkv、wmv以及蓝光专用等等。</p>
<p>720和1080代表视频流的分辨率，前者1280<em>720，后者1920</em>1080，不同的编码需要不同的系统资源，大概可以认为是H.264&gt;VC-1&gt;MPEG2。 　</p>
<p>VC-1是最后被认可的高清编码格式，不过因为有微软的后台，所以这种编码格式不能小窥。相对于MPEG2，VC-1的压缩比更高，但相对于H.264而言，编码解码的计算则要稍小一些，目前来看，VC-1可能是一个比较好的平衡，辅以微软的支持，应该是一只不可忽视的力量。一般来说，VC-1多为 “.wmv”后缀，但这都不是绝对的，具体的编码格式还是要通过软件来查询。</p>
<p>总的来说，从压缩比上来看，H.264的压缩比率更高一些，也就是同样的视频，通过H.264编码算法压出来的视频容量要比VC-1的更小，但是VC-1 格式的视频在解码计算方面则更小一些，一般通过高性能的CPU就可以很流畅的观看高清视频。相信这也是目前NVIDIA Geforce 8系列显卡不能完全解码VC-1视频的主要原因。</p>
<p>PS&amp;TS是两种视频或影片封装格式，常用于高清片。扩展名分别为VOB/EVO和TS等；其文件编码一般用MPEG2/VC-1/H.264</p>
<p> 高清，英文为“High Definition”，即指“高分辨率”。 高清电视(HDTV)，是由美国电影电视工程师协会确定的高清晰度电视标准格式。现在的大屏幕液晶电视机，一般都支持1080i和720P，而一些俗称的“全高清”(Full HD)，则是指支持1080P输出的电视机。</p>
<p>目前的高清视频编码格式主要有H.264、VC-1、MPEG-2、MPEG-4、DivX、XviD、WMA-HD以及X264。事实上，现在网络上流传的高清视频主要以两类文件的方式存在：一类是经过MPEG-2标准压缩，以tp和ts为后缀的视频流文件;一类是经过WMV-HD(Windows Media Video HighDefinition)标准压缩过的wmv文件，还有少数文件后缀为avi或mpg，其性质与wmv是一样的。真正效果好的高清视频更多地以H.264与VC-1这两种主流的编码格式流传。</p>
<p>一般来说，H.264格式以“.avi”、“.mkv”以及“.ts”封装比较常见。</p>
<h3 id="3-位率（定码率，变码率）"><a href="#3-位率（定码率，变码率）" class="headerlink" title="3.位率（定码率，变码率）"></a>3.位率（定码率，变码率）</h3><p>位率又称为“码率”。指单位时间内，单个录像通道所产生的数据量，其单位通常是bps、Kbps或Mbps。可以根据录像的时间与位率估算出一定时间内的录像文件大小。 　位率是一个可调参数，不同的分辨率模式下和监控场景下，合适的位率大小是不同的。在设置时，要综合考虑三个因素： 　　</p>
<ol>
<li>分辨率:分辨率是决定位率（码率）的主要因素，不同的分辨率要采用不同的位率。总体而言，录像的分辨率越高，所要求的位率（码率）也越大，但并不总是如此，图1说明了不同分辨率的合理的码率选择范围。所谓“合理的范围”指的是，如果低于这个范围，图像质量看起来会变得不可接受；如果高于这个范围，则显得没有必要，对于网络资源以及存储资源来说是一种浪费。 　　</li>
<li>场景:监控的场景是设置码率时要考虑的第二个因素。在视频监控中，图像的运动剧烈程度还与位率有一定的关系，运动越剧烈，编码所要求的码率就越高。反之则越低。因此在同样的图像分辨率条件下，监控人多的场景和人少的场景，所要求的位率也是不同的。 　　</li>
<li>存储空间:最后需要考量的因素是存储空间，这个因素主要是决定了录像系统的成本。位率设置得越高，画质相对会越好，但所要求的存储空间就越大。所以在工程实施中，设置合适的位率即可以保证良好的回放图像质量，又可以避免不必要的资源浪费。
　　
QP(quantizer parameter)
介于0~31之间，值越小，量化越精细，图像质量就越高，而产生的码流也越长。</li>
</ol>
<p>PSNR
允许计算峰值信噪比(PSNR,Peak signal-to-noise ratio),编码结束后在屏幕上显示PSNR计算结果。开启与否与输出的视频质量无关，关闭后会带来微小的速度提升。</p>
<p>profile level
分别是BP、EP、MP、HP：</p>
<ol>
<li>BP-Baseline Profile：基本画质。支持I/P 帧，只支持无交错（Progressive）和CAVLC；</li>
<li>EP-Extended profile：进阶画质。支持I/P/B/SP/SI 帧，只支持无交错（Progressive）和CAVLC；</li>
<li>MP-Main profile：主流画质。提供I/P/B 帧，支持无交错（Progressive）和交错（Interlaced），也支持CAVLC 和CABAC 的支持；</li>
<li>HP-High profile：高级画质。在main Profile 的基础上增加了8x8内部预测、自定义量化、无损视频编码和更多的YUV 格式；</li>
</ol>
<p>H.264规定了三种档次，每个档次支持一组特定的编码功能，并支持一类特定的应用。</p>
<ol>
<li>基本档次：利用I片和P片支持帧内和帧间编码，支持利用基于上下文的自适应的变长编码进行的熵编码（CAVLC）。主要用于可视电话、会议电视、无线通信等实时视频通信；</li>
<li>主要档次：支持隔行视频，采用B片的帧间编码和采用加权预测的帧内编码；支持利用基于上下文的自适应的算术编码（CABAC）。主要用于数字广播电视与数字视频存储；</li>
<li>扩展档次：支持码流之间有效的切换（SP和SI片）、改进误码性能（数据分割），但不支持隔行视频和CABAC。主要用于网络的视频流，如视频点播。</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/06/27/media-graphic-image-stride/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/27/media-graphic-image-stride/" itemprop="url">
                  Image Stride(内存图像行跨度)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-27T15:00:14+08:00">
                2017-06-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/media/" itemprop="url" rel="index">
                    <span itemprop="name">media</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>When a video image is stored in memory, the memory buffer might contain extra padding bytes after each row of pixels. The padding bytes affect how the image is store in memory, but do not affect how the image is displayed.</p>
<p>当视频图像存储在内存时，图像的每一行末尾也许包含一些扩展的内容，这些扩展的内容只影响图像如何存储在内存中，但是不影响图像如何显示出来；</p>
<p>The stride is the number of bytes from one row of pixels in memory to the next row of pixels in memory. Stride is also called pitch. If padding bytes are present, the stride is wider than the width of the image, as shown in the following illustration.</p>
<p>Stride 就是这些扩展内容的名称，Stride 也被称作 Pitch，如果图像的每一行像素末尾拥有扩展内容，Stride 的值一定大于图像的宽度值，就像下图所示：
<img src="http://images.wodekouwei.com/media/imagestride1.png" alt="image"></p>
<p>Two buffers that contain video frames with equal dimensions can have two different strides. If you process a video image, you must take into the stride into account.</p>
<p>两个缓冲区包含同样大小（宽度和高度）的视频帧，却不一定拥有同样的 Stride 值，如果你处理一个视频帧，你必须在计算的时候把 Stride 考虑进去；</p>
<p>In addition, there are two ways that an image can be arranged in memory. In a top-down image, the top row of pixels in the image appears first in memory. In a bottom-up image, the last row of pixels appears first in memory. The following illustration shows the difference between a top-down image and a bottom-up image.</p>
<p>另外，一张图像在内存中有两种不同的存储序列（arranged），对于一个从上而下存储（Top-Down） 的图像，最顶行的像素保存在内存中最开头的部分，对于一张从下而上存储（Bottom-Up）的图像，最后一行的像素保存在内存中最开头的部分，下面图示展示了这两种情况：
<img src="http://images.wodekouwei.com/media/imagestride2.png" alt="image"></p>
<p>A bottom-up image has a negative stride, because stride is defined as the number of bytes need to move down a row of pixels, relative to the displayed image. YUV images should always be top-down, and any image that is contained in a Direct3D surface must be top-down. RGB images in system memory are usually bottom-up.</p>
<p>一张从下而上的图像拥有一个负的 Stride 值，因为 Stride 被定义为[从一行像素移动到下一行像素时需要跨过多少个像素]，仅相对于被显示出来的图像而言；而 YUV 图像永远都是从上而下表示的，以及任何包含在 Direct3D Surface 中的图像必须是从上而下，RGB 图像保存在系统内存时通常是从下而上；</p>
<p>Video transforms in particular need to handle buffers with mismatched strides, because the input buffer might not match the output buffer. For example, suppose that you want to convert a source image and write the result to a destination image. Assume that both images have the same width and height, but might not have the same pixel format or the same image stride.</p>
<p>尤其是视频变换，特别需要处理不同 Stride 值的图像，因为输入缓冲也许与输出缓冲不匹配，举个例子，假设你想要将源图像转换并且将结果写入到目标图像，假设两个图像拥有相同的宽度和高度，但是其像素格式与 Stride 值也许不同；</p>
<p>The following example code shows a generalized approach for writing this kind of function. This is not a complete working example, because it abstracts many of the specific details.</p>
<p>下面代码演示了一种通用方法来编写这种功能，这段代码并不完整，因为这只是一个抽象的算法，没有完全考虑到真实需求中的所有细节；
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">void ProcessVideoImage(</div><div class="line">    BYTE*       pDestScanLine0,    </div><div class="line">    LONG        lDestStride,       </div><div class="line">    const BYTE* pSrcScanLine0,     </div><div class="line">    LONG        lSrcStride,        </div><div class="line">    DWORD       dwWidthInPixels,    </div><div class="line">    DWORD       dwHeightInPixels</div><div class="line">    )</div><div class="line">&#123;</div><div class="line">    for (DWORD y = 0; y &lt; dwHeightInPixels; y++)</div><div class="line">    &#123;</div><div class="line">        SOURCE_PIXEL_TYPE *pSrcPixel = (SOURCE_PIXEL_TYPE*)pDestScanLine0;</div><div class="line">        DEST_PIXEL_TYPE *pDestPixel = (DEST_PIXEL_TYPE*)pSrcScanLine0;</div><div class="line"></div><div class="line">        for (DWORD x = 0; x &lt; dwWidthInPixels; x +=2)</div><div class="line">        &#123;</div><div class="line">            pDestPixel[x] = TransformPixelValue(pSrcPixel[x]);</div><div class="line">        &#125;</div><div class="line">        pDestScanLine0 += lDestStride;</div><div class="line">        pSrcScanLine0 += lSrcStride;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>This function takes six parameters:</p>
<ul>
<li>A pointer to the start of scan line 0 in the destination image.</li>
<li>The stride of the destination image.</li>
<li>A pointer to the start of scan line 0 in the source image.</li>
<li>The stride of the source image.</li>
<li>The width of the image in pixels.</li>
<li>The height of the image in pixels.</li>
</ul>
<p>这个函数需要六个参数：</p>
<ul>
<li>目标图像的起始扫描行的内存指针</li>
<li>目标图像的 Stride 值</li>
<li>源图像的起始扫描行的内存指针</li>
<li>源图像的 Stride 值</li>
<li>图像的宽度值（以像素为单位）</li>
<li>图像的高度值（以像素为单位）
The general idea is to process one row at a time, iterating over each pixel in the row. Assume that SOURCE_PIXEL_TYPE and DEST_PIXEL_TYPE are structures representing the pixel layout for the source and destination images, respectively. (For example, 32-bit RGB uses the RGBQUAD structure. Not every pixel format has a pre-defined structure.) Casting the array pointer to the structure type enables you to access the RGB or YUV components of each pixel. At the start of each row, the function stores a pointer to the row. At the end of the row, it increments the pointer by the width of the image stride, which advances the pointer to the next row.</li>
</ul>
<p>这里的要点是如何一次处理一行像素，遍历一行里面的每一个像素，假设源像素类型与目标像素类型各自在像素的层面上已经结构化来表示一个源图像与目标图像的像素，（举个例子，32 位 RGB 像素使用 RGBQUAD 结构体，并不是每一种像素类型都有预定义结构体的）强制转换数组指针到这样的结构体指针，可以方便你直接读写每一个像素的 RGB 或者 YUV 值，在每一行的开头，这个函数保存了一个指向这行像素的指针，函数的最后一行，通过图像的 Stride 值直接将指针跳转到图像的下一行像素的起始点；</p>
<p>This example calls a hypothetical function named TransformPixelValue for each pixel. This could be any function that calculates a target pixel from a source pixel. Of course, the exact details will depend on the particular task. For example, if you have a planar YUV format, you must access the chroma planes independently from the luma plane; with interlaced video, you might need to process the fields separately; and so forth.</p>
<p>To give a more concrete example, the following code converts a 32-bit RGB image into an AYUV image. The RGB pixels are accessed using an RGBQUAD structure, and the AYUV pixels are accessed using aDXVA2_AYUVSample8 Structure structure.</p>
<blockquote>
<p>引用:
如果你用的是 MSDN Library For Visual Studio 2008 SP1，那么你应该能够在下面地址中找到这篇文章的原文：
ms-help://MS.MSDNQTR.v90.chs/medfound/html/13cd1106-48b3-4522-ac09-8efbaab5c31d.htm</p>
<p><a href="http://blog.csdn.net/g0ose/article/details/52116453" target="_blank" rel="external">http://blog.csdn.net/g0ose/article/details/52116453</a></p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/06/27/gl-glBlendFunc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/27/gl-glBlendFunc/" itemprop="url">
                  OpenGL混色
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-27T10:44:39+08:00">
                2017-06-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/OpenGL/" itemprop="url" rel="index">
                    <span itemprop="name">OpenGL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>混合就是把两种颜色混在一起。具体一点，就是把某一像素位置原来的颜色和将要画上去的颜色，通过某种方式混在一起，从而实现特殊的效果。
假设我们需要绘制这样一个场景：透过红色的玻璃去看绿色的物体，那么可以先绘制绿色的物体，再绘制红色玻璃。在绘制红色玻璃的时候，利用“混合”功能，把将要绘制上去的红色和原来的绿色进行混合，于是得到一种新的颜色，看上去就好像玻璃是半透明的。
要使用OpenGL的混合功能，只需要调用：<code>glEnable(GL_BLEND);</code>即可。要关闭OpenGL的混合功能，只需要调用：<code>glDisable(GL_BLEND);</code>即可。
<strong>注意：</strong> 只有在RGBA模式下，才可以使用混合功能，颜色索引模式下是无法使用混合功能的。</p>
<h3 id="1-源因子和目标因子"><a href="#1-源因子和目标因子" class="headerlink" title="1.源因子和目标因子"></a>1.源因子和目标因子</h3><p>混合需要把原来的颜色和将要画上去的颜色找出来，经过某种方式处理后得到一种新的颜色。这里把将要画上去的颜色称为“源颜色”，把原来的颜色称为“目标颜色”。
OpenGL 会把源颜色和目标颜色各自取出，并乘以一个系数（源颜色乘以的系数称为“源因子”，目标颜色乘以的系数称为“目标因子”），然后相加，这样就得到了新的颜 色。（也可以不是相加，新版本的OpenGL可以设置运算方式，包括加、减、取两者中较大的、取两者中较小的、逻辑运算等）
下面用数学公式来表达一下这个运算方式。假设源颜色的四个分量（指红色，绿色，蓝色，alpha值）是(Rs, Gs, Bs,  As)，目标颜色的四个分量是(Rd, Gd, Bd, Ad)，又设源因子为(Sr, Sg, Sb, Sa)，目标因子为(Dr, Dg, Db,  Da)。则混合产生的新颜色可以表示为：
<code>(Rs*Sr+Rd*Dr, Gs*Sg+Gd*Dg, Bs*Sb+Bd*Db, As*Sa+Ad*Da)</code>
如果颜色的某一分量超过了1.0，则它会被自动截取为1.0，不需要考虑越界的问题。</p>
<p>源因子和目标因子是可以通过<code>glBlendFunc</code>函数来进行设置的。glBlendFunc有两个参数，前者表示源因子，后者表示目标因子。这两个参数可以是多种值，下面介绍比较常用的几种。</p>
<ul>
<li>GL_ZERO：表示使用0.0作为因子，实际上相当于不使用这种颜色参与混合运算。</li>
<li>GL_ONE：表示使用1.0作为因子，实际上相当于完全的使用了这种颜色参与混合运算。</li>
<li>GL_SRC_ALPHA：表示使用源颜色的alpha值来作为因子。</li>
<li>GL_DST_ALPHA：表示使用目标颜色的alpha值来作为因子。</li>
<li>GL_ONE_MINUS_SRC_ALPHA：表示用1.0减去源颜色的alpha值来作为因子。</li>
<li>GL_ONE_MINUS_DST_ALPHA：表示用1.0减去目标颜色的alpha值来作为因子。</li>
<li>GL_SRC_COLOR: 把源颜色的四个分量分别作为因子的四个分量</li>
<li>GL_ONE_MINUS_SRC_COLOR</li>
<li>GL_DST_COLOR</li>
<li>GL_ONE_MINUS_DST_COLOR
GL_SRC_COLOR与GL_ONE_MINUS_SRC_COLOR在OpenGL旧版本中只能用于设置目标因子，GL_DST_COLOR与GL_ONE_MINUS_DST_COLOR在OpenGL 旧版本中只能用于设置源因子。新版本的OpenGL则没有这个限制，并且支持新的GL_CONST_COLOR（设定一种常数颜色，将其四个分量分别作为 因子的四个分量）、GL_ONE_MINUS_CONST_COLOR、GL_CONST_ALPHA、 GL_ONE_MINUS_CONST_ALPHA。另外还有GL_SRC_ALPHA_SATURATE。新版本的OpenGL还允许颜色的alpha 值和RGB值采用不同的混合因子。</li>
</ul>
<h3 id="2-模式示例"><a href="#2-模式示例" class="headerlink" title="2.模式示例"></a>2.模式示例</h3><ul>
<li>如果设置了glBlendFunc(GL_ONE, GL_ZERO);，则表示完全使用源颜色，完全不使用目标颜色，因此画面效果和不使用混合的时候一致（当然效率可能会低一点点）。如果没有设置源因子和目标因子，则默认情况就是这样的设置。</li>
<li>如果设置了glBlendFunc(GL_ZERO, GL_ONE);，则表示完全不使用源颜色，因此无论你想画什么，最后都不会被画上去了。（但这并不是说这样设置就没有用，有些时候可能有特殊用途）</li>
<li>如果设置了glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);，则表示源颜色乘以自身的alpha 值，目标颜色乘以1.0减去源颜色的alpha值，这样一来，源颜色的alpha值越大，则产生的新颜色中源颜色所占比例就越大，而目标颜色所占比例则减 小。这种情况下，我们可以简单的将源颜色的alpha值理解为“不透明度”。这也是混合时最常用的方式。</li>
<li>如果设置了glBlendFunc(GL_ONE, GL_ONE);，则表示完全使用源颜色和目标颜色，最终的颜色实际上就是两种颜色的简单相加。例如红色(1, 0, 0)和绿色(0, 1, 0)相加得到(1, 1, 0)，结果为黄色。
注意：
所谓源颜色和目标颜色，是跟绘制的顺序有关的。假如先绘制了一个红色的物体，再在其上绘制绿色的物体。则绿色是源颜色，红色是目标颜色。如果顺序反过来，则 红色就是源颜色，绿色才是目标颜色。在绘制时，应该注意顺序，使得绘制的源颜色与设置的源因子对应，目标颜色与设置的目标因子对应。</li>
</ul>
<h3 id="3-对两种示例模式的具体解释"><a href="#3-对两种示例模式的具体解释" class="headerlink" title="3.对两种示例模式的具体解释:"></a>3.对两种示例模式的具体解释:</h3><p>模式一:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">GLES20.glEnable(GLES20.GL_BLEND);  </div><div class="line">GLES20.glBlendFunc(GLES20.GL_SRC_ALPHA, GLES20.GL_ONE_MINUS_SRC_ALPHA);</div></pre></td></tr></table></figure></p>
<p>模式二:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">GLES20.glEnable(GLES20.GL_BLEND);  </div><div class="line">GLES20.glBlendFunc(GLES20.GL_ONE, GLES20.GL_ONE_MINUS_SRC_ALPHA);</div></pre></td></tr></table></figure></p>
<p>模式一是传统的alpha通道混合，这种模式下颜色和alpha值是分立的，rgb决定颜色，alpha决定..（英文是决定how solid it is 水平有限找不到准确的中文来表达）
在数学上表达式是：<code>blend(source, dest) = (source.rgb * source.a) + (dest.rgb * (1 – source.a)).</code>要注意的是这种模式下，透明只跟alpha有关，跟rgb值无关，一个透明的颜色，不透明的颜色有相同的rgb值，只要alpha=0即可。</p>
<p>模式二是alpha预乘的混合（Premultiplied Alpha Blending），这种模式下rgb与alpha是联系在一起的，数学上的表达式是
<code>blend(source, dest) = source.rgb + (dest.rgb * (1 – source.a))</code>,在这种模式下，透明的表示是rgb值都为0.</p>
<h3 id="4-EGLSurface背景透明设置"><a href="#4-EGLSurface背景透明设置" class="headerlink" title="4.EGLSurface背景透明设置"></a>4.EGLSurface背景透明设置</h3><p>在OpenGL绘制中,除了设置混色外,还要设置EGLSurface配置支持Alpha,如果不设置EGL相关的EGLSurface支持透明,就算OpenGL函数中开启混色,绘制完成后仍是有黑色背景.</p>
<h4 id="4-1GLSurfaceView"><a href="#4-1GLSurfaceView" class="headerlink" title="4.1GLSurfaceView"></a>4.1GLSurfaceView</h4><p>在onSurfaceCreated里，调用GLES20.glClearColor(0f, 0f, 0f, 0f);alpha为0，即透明。</p>
<p>然后，对surfaceview要作一定处理：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">mGLSurfaceView.setEGLConfigChooser(8, 8, 8, 8, 16, 0);</div><div class="line">TestRenderer renderer = new TestRenderer();</div><div class="line">mGLSurfaceView.setRender(renderer);</div><div class="line">mGLSurfaceView.getHolder().setFormat(PixelFormat.TRANSLUCENT);</div><div class="line">mGLSurfaceView.setZOrderOnTop(true);</div></pre></td></tr></table></figure></p>
<h4 id="4-2SurfaceTexture或Surface构造的Surface"><a href="#4-2SurfaceTexture或Surface构造的Surface" class="headerlink" title="4.2SurfaceTexture或Surface构造的Surface"></a>4.2SurfaceTexture或Surface构造的Surface</h4><p>EGL14.eglChooseConfig中config数组增加EGL10.EGL_ALPHA_SIZE配置:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">int[] CONFIG_RGBA = &#123;</div><div class="line">          EGL10.EGL_RED_SIZE, 8,</div><div class="line">          EGL10.EGL_GREEN_SIZE, 8,</div><div class="line">          EGL10.EGL_BLUE_SIZE, 8,</div><div class="line">          EGL10.EGL_ALPHA_SIZE, 8,</div><div class="line">          EGL10.EGL_RENDERABLE_TYPE, EGL_OPENGL_ES2_BIT,</div><div class="line">          EGL10.EGL_NONE</div><div class="line">  &#125;;</div></pre></td></tr></table></figure></p>
<h3 id="5-实现三维混合"><a href="#5-实现三维混合" class="headerlink" title="5.实现三维混合"></a>5.实现三维混合</h3><p>在进行三维场景的混合时必须注意的是深度缓冲。</p>
<p>深度缓冲是这样一段数据，它记录了每一个像素距离观察者有多近。在启用深度缓冲测试的情况下，如果将要绘制的像素比原来的像素更近，则像素将被绘制。否则,像素就会被忽略掉，不进行绘制。这在绘制不透明的物体时非常有用——不管是先绘制近的物体再绘制远的物体，还是先绘制远的物体再绘制近的物体，或者干脆以 混乱的顺序进行绘制，最后的显示结果总是近的物体遮住远的物体。
然而在你需要实现半透明效果时，发现一切都不是那么美好了。如果你绘制了一个近距离的半透明物体，则它在深度缓冲区内保留了一些信息，使得远处的物体将无法再被绘制出来。虽然半透明的物体仍然半透明，但透过它看到的却不是正确的内容了。
要 解决以上问题，需要在绘制半透明物体时将深度缓冲区设置为只读，这样一来，虽然半透明物体被绘制上去了，深度缓冲区还保持在原来的状态。如果再有一个物体 出现在半透明物体之后，在不透明物体之前，则它也可以被绘制（因为此时深度缓冲区中记录的是那个不透明物体的深度）。以后再要绘制不透明物体时，只需要再 将深度缓冲区设置为可读可写的形式即可。怎么绘制一个一部分半透明一部分不透明的物体？这个好办，只需要把物体分为两个部分，一部分全是半透明 的，一部分全是不透明的，分别绘制就可以了。
即使使用了以上技巧，我们仍然不能随心所欲的按照混乱顺序来进行绘制。必须是先绘制不透明的物体，然 后绘制透明的物体。否则，假设背景为蓝色，近处一块红色玻璃，中间一个绿色物体。如果先绘制红色半透明玻璃的话，它先和蓝色背景进行混合，则以后绘制中间 的绿色物体时，想单独与红色玻璃混合已经不能实现了。
总结起来，绘制顺序就是：首先绘制所有不透明的物体。如果两个物体都是不透明的，则谁先谁后 都没有关系。然后，将深度缓冲区设置为只读。接下来，绘制所有半透明的物体。如果两个物体都是半透明的，则谁先谁后只需要根据自己的意愿（注意了，先绘制 的将成为“目标颜色”，后绘制的将成为“源颜色”，所以绘制的顺序将会对结果造成一些影响）。最后，将深度缓冲区设置为可读可写形式。
调用glDepthMask(GL_FALSE);可将深度缓冲区设置为只读形式。调用glDepthMask(GL_TRUE);可将深度缓冲区设置为可读可写形式。</p>
<blockquote>
<p><a href="https://www.khronos.org/registry/OpenGL-Refpages/es2.0/xhtml/glBlendFunc.xml" target="_blank" rel="external">glBlendFunc函数官网文档</a></p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/06/25/tips-android-mediacodec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/25/tips-android-mediacodec/" itemprop="url">
                  android系统编码MediaCodec
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-25T16:03:24+08:00">
                2017-06-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Android/" itemprop="url" rel="index">
                    <span itemprop="name">Android</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="android编码器局限性"><a href="#android编码器局限性" class="headerlink" title="android编码器局限性"></a>android编码器局限性</h3><h4 id="1-颜色格式问题"><a href="#1-颜色格式问题" class="headerlink" title="1. 颜色格式问题"></a>1. 颜色格式问题</h4><p>MediaCodec在初始化的时候，在configure的时候，需要传入一个MediaFormat对象，当作为编码器使用的时候，我们一般需要在MediaFormat中指定视频的宽高，帧率，码率，I帧间隔等基本信息，除此之外，还有一个重要的信息就是，指定编码器接受的YUV帧的颜色格式。这个是因为由于YUV根据其采样比例，UV分量的排列顺序有很多种不同的颜色格式，而对于Android的摄像头在onPreviewFrame输出的YUV帧格式，如果没有配置任何参数的情况下，基本上都是NV21格式，但Google对MediaCodec的API在设计和规范的时候，显得很不厚道，过于贴近Android的HAL层了，导致了NV21格式并不是所有机器的MediaCodec都支持这种格式作为编码器的输入格式！</p>
<p>因此，在初始化MediaCodec的时候，我们需要通过codecInfo.getCapabilitiesForType来查询机器上的MediaCodec实现具体支持哪些YUV格式作为输入格式，一般来说，起码在4.4+的系统上，这两种格式在大部分机器都有支持：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">MediaCodecInfo.CodecCapabilities.COLOR_FormatYUV420Plannder</div><div class="line">MediaCodecInfo.CodecCapabilities.COLOR_FormatYUV420SemiPlannder</div></pre></td></tr></table></figure></p>
<p>两种格式分别是YUV420P和NV21，如果机器上只支持YUV420P格式的情况下，则需要先将摄像头输出的NV21格式先转换成YUV420P，才能送入编码器进行编码，否则最终出来的视频就会花屏，或者颜色出现错乱</p>
<p>这个算是一个不大不小的坑，基本上用上了MediaCodec进行视频编码都会遇上这个问题</p>
<h4 id="2-编码器支持特性相当有限"><a href="#2-编码器支持特性相当有限" class="headerlink" title="2. 编码器支持特性相当有限"></a>2. 编码器支持特性相当有限</h4><p>如果使用MediaCodec来编码H264视频流，对于H264格式来说，会有一些针对压缩率以及码率相关的视频质量设置，典型的诸如Profile(baseline, main, high)，Profile Level, Bitrate mode(CBR, CQ, VBR)，合理配置这些参数可以让我们在同等的码率下，获得更高的压缩率，从而提升视频的质量，Android也提供了对应的API进行设置，可以设置到MediaFormat中这些设置项:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">MediaFormat.KEY_BITRATE_MODE</div><div class="line">MediaFormat.KEY_PROFILE</div><div class="line">MediaFormat.KEY_LEVEL</div></pre></td></tr></table></figure></p>
<p>但问题是，对于Profile，Level, Bitrate mode这些设置，在大部分手机上都是不支持的，即使是设置了最终也不会生效，例如设置了Profile为high，最后出来的视频依然还会是Baseline….</p>
<p>这个问题，在7.0以下的机器几乎是必现的，其中一个可能的原因是，Android在源码层级hardcode了profile的的设置：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">//XXXX</div><div class="line">if(h264type.eProfile != OMX_VIDEO_AVCProfileBaseline)&#123;</div><div class="line">  ALOGW(&quot;Use baseline profile instead of %d for AVC recording&quot;, h264type.eProfile);</div><div class="line">  h264type.eProfile = OMX_VIDEO_AVCProfileBaseline;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Android直到7.0之后才取消了这段地方的Hardcode:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">if(h264type.eProfile == OMX_VIDEO_AVCProfileBaseline)&#123;</div><div class="line">  ....</div><div class="line">&#125;else if(h264type.eProfile == OMX_VIDEO_AVCProfileMain</div><div class="line">  || h264type.eProfile == OMX_VIDEO_AVCProfileBaseHigh)</div><div class="line">  .....</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这个问题可以说间接导致了MediaCodec编码出来的视频质量偏低，同等码率下，难以获得跟软编码甚至iOS那样的视频质量。</p>
<h4 id="3-16位对齐要求"><a href="#3-16位对齐要求" class="headerlink" title="3. 16位对齐要求"></a>3. 16位对齐要求</h4><p>前面说到，MediaCodec这个API在设计的时候，过于贴近HAL层，这在很多Soc的实现上，是直接把传入MediaCodec的buffer，在不经过任何前置处理的情况下就直接送入了Soc中。而在编码h264视频流的时候，由于h264的编码块大小一般是16x16，于是乎在一开始设置视频的宽高的时候，如果设置了一个没有对齐16的大小，例如960x540，在某些cpu上，最终编码出来的视频就会直接花屏！</p>
<p>很明显这还是因为厂商在实现这个API的时候，对传入的数据缺少校验以及前置处理导致的，目前来看，华为，三星的Soc出现这个问题会比较频繁，其他厂商的一些早期Soc也有这种问题，一般来说解决方法还是在设置视频宽高的时候，统一设置成对齐16位之后的大小就好了。</p>
<h4 id="4-软编解码介绍"><a href="#4-软编解码介绍" class="headerlink" title="4.软编解码介绍"></a>4.软编解码介绍</h4><p>除了使用MediaCodec进行编码之外，另外一种比较流行的方案就是使用ffmpeg+x264/openh264进行软编码，ffmpeg是用于一些视频帧的预处理。这里主要是使用x264/openh264作为视频的编码器。</p>
<ol>
<li>x264基本上被认为是当今市面上最快的商用视频编码器，而且基本上所有h264的特性都支持，通过合理配置各种参数还是能够得到较好的压缩率和编码速度的，限于篇幅，这里不再阐述h264的参数配置，有兴趣可以看下这两篇文章对x264编码参数的调优：</li>
</ol>
<ul>
<li><a href="https://www.nmm-hd.org/d/index.php?title=X264%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D&amp;variant=zh-cn" target="_blank" rel="external">https://www.nmm-hd.org/d/index.php?title=X264%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D&amp;variant=zh-cn</a></li>
<li><a href="http://www.cnblogs.com/wainiwann/p/5647521.html" target="_blank" rel="external">http://www.cnblogs.com/wainiwann/p/5647521.html</a></li>
</ul>
<ol>
<li>openh264(<a href="https://github.com/cisco/openh264)则是由思科开源的另外一个h264编码器，项目在2013年开源，对比起x264来说略显年轻，不过由于思科支付满了h264的年度专利费，所以对于外部用户来说，相当于可以直接免费使用了，另外，firefox直接内置了openh264，作为其在webRTC中的视频的编解码器使用。" target="_blank" rel="external">https://github.com/cisco/openh264)则是由思科开源的另外一个h264编码器，项目在2013年开源，对比起x264来说略显年轻，不过由于思科支付满了h264的年度专利费，所以对于外部用户来说，相当于可以直接免费使用了，另外，firefox直接内置了openh264，作为其在webRTC中的视频的编解码器使用。</a></li>
</ol>
<p>但对比起x264，openh264在h264高级特性的支持比较差：</p>
<ul>
<li>Profile只支持到baseline, level 5.2</li>
<li>多线程编码只支持slice based，不支持frame based的多线程编码</li>
</ul>
<p>从编码效率上来看，openh264的速度也并不会比x264快，不过其最大的好处，还是能够直接免费使用吧。</p>
<blockquote>
<p>参考<a href="https://mp.weixin.qq.com/s/gwjL2OuoLVExbhYtbdpF2g" target="_blank" rel="external">微信Android视频编码爬过的那些坑</a></p>
</blockquote>
<h3 id="android编码器支持参数"><a href="#android编码器支持参数" class="headerlink" title="android编码器支持参数"></a>android编码器支持参数</h3><h4 id="Supported-Media-Formats"><a href="#Supported-Media-Formats" class="headerlink" title="Supported Media Formats"></a><a href="https://developer.android.com/guide/topics/media/media-formats.html" target="_blank" rel="external">Supported Media Formats</a></h4><p><strong>Video encoding recommendations</strong>
The table below lists the Android media framework video encoding profiles and parameters recommended for playback using the H.264 Baseline Profile codec. The same recommendations apply to the Main Profile codec, which is only available in Android 6.0 and later.</p>
<table>
<thead>
<tr>
<th></th>
<th>SD (Low quality)</th>
<th>SD (High quality)</th>
<th>HD 720p (N/A on all devices)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Video resolution</td>
<td>176 x 144 px</td>
<td>480 x 360 px</td>
<td>1280 x 720 px</td>
</tr>
<tr>
<td>Video frame rate</td>
<td>12 fps</td>
<td>30 fps</td>
<td>30 fps</td>
</tr>
<tr>
<td>Video bitrate</td>
<td>56 Kbps</td>
<td>500 Kbps</td>
<td>2 Mbps</td>
</tr>
<tr>
<td>Audio codec</td>
<td>AAC-LC</td>
<td>AAC-LC</td>
<td>AAC-LC</td>
</tr>
<tr>
<td>Audio channels</td>
<td>1 (mono)</td>
<td>2 (stereo)</td>
<td>2 (stereo)</td>
</tr>
<tr>
<td>Audio bitrate</td>
<td>24 Kbps</td>
<td>128 Kbps</td>
<td>192 Kbps</td>
</tr>
</tbody>
</table>
<p>The table below lists the Android media framework video encoding profiles and parameters recommended for playback using the VP8 media codec.</p>
<table>
<thead>
<tr>
<th></th>
<th>SD (Low quality)</th>
<th>SD (High quality)</th>
<th>HD 720p (N/A on all devices)</th>
<th>HD 1080p (N/A on all devices)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Video resolution</td>
<td>320 x 180 px</td>
<td>640 x 360 px</td>
<td>1280 x 720 px</td>
<td>1920 x 1080 px</td>
</tr>
<tr>
<td>Video frame rate</td>
<td>30 fps</td>
<td>30 fps</td>
<td>30 fps</td>
<td>30 fps</td>
</tr>
<tr>
<td>Video bitrate</td>
<td>800 Kbps</td>
<td>2 Mbps</td>
<td>4 Mbps</td>
<td>10 Mbps</td>
</tr>
</tbody>
</table>
<h4 id="CamcorderProfile"><a href="#CamcorderProfile" class="headerlink" title="CamcorderProfile"></a><a href="https://developer.android.com/reference/android/media/CamcorderProfile.html" target="_blank" rel="external">CamcorderProfile</a></h4><p>Retrieves the predefined camcorder profile settings for camcorder applications. These settings are read-only.</p>
<p>The compressed output from a recording session with a given CamcorderProfile contains two tracks: one for audio and one for video.</p>
<p>Each profile specifies the following set of parameters:</p>
<ul>
<li>The file output format</li>
<li>Video codec format</li>
<li>Video bit rate in bits per second</li>
<li>Video frame rate in frames per second</li>
<li>Video frame width and height,</li>
<li>Audio codec format</li>
<li>Audio bit rate in bits per second,</li>
<li>Audio sample rate</li>
<li>Number of audio channels for recording.</li>
</ul>
<h3 id="Android编码器常见问题"><a href="#Android编码器常见问题" class="headerlink" title="Android编码器常见问题"></a>Android编码器常见问题</h3><h4 id="MediaCodec-KEY-FRAME-RATE-seems-to-be-ignored"><a href="#MediaCodec-KEY-FRAME-RATE-seems-to-be-ignored" class="headerlink" title="MediaCodec KEY_FRAME_RATE seems to be ignored"></a><a href="https://stackoverflow.com/questions/22336604/mediacodec-key-frame-rate-seems-to-be-ignored" target="_blank" rel="external">MediaCodec KEY_FRAME_RATE seems to be ignored</a></h4><p><strong>总结起来就是和输入编码器的帧率有关系</strong></p>
<p>I am trying to modify the source for screenrecord in android 4.4 and lower the captured frame rate, but no matter what value I put in:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">format-&gt;setFloat(&quot;frame-rate&quot;, 5);</div></pre></td></tr></table></figure></p>
<p>the result is always the same ( a very high frame rate )
Is the encoder ignoring this property ? how can I control the frame rate ?</p>
<p>The frame-rate value is not ignored, but it doesn’t do what you want.</p>
<p>The combination of frame-rate and i-frame-interval determines how often I-frames (also called “sync frames”) appear in the encoded output. The frame rate value might also play a role in meeting the bitrate target on some devices, but I’m not sure about that (see e.g. this post).</p>
<p>The MediaCodec encoder does not drop frames. If you want to reduce the frame rate, you have to do so by sending fewer frames to it.</p>
<p>The screenrecord command doesn’t “sample” the screen at a fixed frame rate. Instead, every frame it receives from the surface compositor (SurfaceFlinger) is sent to the encoder, with an appropriate time stamp. If screenrecord receives 60 frames per seconds, you’ll have 60fps output. If it receives 10 frames in quick succession, followed by nothing for 5 seconds, followed by a couple more, you’ll have exactly that in the output file.</p>
<p>You can modify screenrecord to drop frames, but you have to be a bit careful. If you try to reduce the maximum frame rate from 60fps to 30fps by dropping every-other frame, you run the risk that in a “frame0 - frame1 - long_pause - frame2” sequence you’ll drop frame1, and the video will hold on frame0 instead, showing a not-quite-complete animation. So you need to buffer up a frame, and then encode or drop frame N-1 if the difference in presentation times between that and frame N is ~17ms.</p>
<p>The tricky part is that screenrecord, in its default operating mode, directs the frames to the encoder without touching them, so all you see is the encoded output. You can’t arbitrarily drop individual frames of encoded data, so you really want to prevent the encoder from seeing them in the first place. If you use the screenrecord v1.1 sources you can tap into “overlay” mode, used for –bugreport, to have the frames pass through screenrecord on their way to the encoder.</p>
<p>In some respects it might be simpler to write a post-processor that reduces the frame rate. I don’t know how much quality would be lost by decoding and re-encoding the video.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/06/20/media-storage-and-transfer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/20/media-storage-and-transfer/" itemprop="url">
                  多媒体技术(三)之多媒体信息的存储与传输编码
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-20T23:10:09+08:00">
                2017-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/media/" itemprop="url" rel="index">
                    <span itemprop="name">media</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/06/20/media-compress/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/20/media-compress/" itemprop="url">
                  多媒体技术(五)之数字音频视频信号的压缩
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-20T23:09:04+08:00">
                2017-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/media/" itemprop="url" rel="index">
                    <span itemprop="name">media</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><blockquote>
<p><a href="http://blog.csdn.net/u013354805/article/details/50112353" target="_blank" rel="external">数字视频编码概述(熵编码/Huffman编码)</a></p>
<h4 id="1-数字视频压缩的必要性和可能性"><a href="#1-数字视频压缩的必要性和可能性" class="headerlink" title="1. 数字视频压缩的必要性和可能性"></a>1. 数字视频压缩的必要性和可能性</h4><p>按ITU-R BT. 601建议，数字化后的输入图像格式为720<em>576像素，帧频为25帧/s，采样格式为4:2:2，量化精度为8bit，
则数码率：(720 </em> 576 + 360 <em> 576 + 360 </em> 576) <em> 25帧/s </em> 8bit = 165.888Mbit/s。
如果视频信号数字化后直接存放在650MB的光盘中，在不考虑音频信号的情况下，每张光盘只能存储31s的视频信号。
数据压缩的理论基础是信息论。从信息论的角度来看，压缩就是去掉数据中的冗余，即保留不确定的信息，去掉确定的信息（可推知的），也就是用一种更接近信息本质的描述来代替原有冗余的描述。数据图像和视频数据中存在着大量的数据冗余和主观视觉冗余，因此图像和视频数据压缩不仅是必要的，而且也是可能的。</p>
</blockquote>
<h5 id="1-1-空间冗余"><a href="#1-1-空间冗余" class="headerlink" title="1.1 空间冗余"></a>1.1 空间冗余</h5><p>空间冗余也称空域冗余，是一种与像素间相关性直接联系的数据冗余。例如：数字图像的亮度信号和色度信号在空间域（X，Y坐标系）虽然属于一个随机场分布，但是他们可以看成为一个平稳的马尔可夫场。通俗讲，图像像素点在空间域中的亮度值和色度信号值，除了边界轮毂外，都是缓慢变化。因此，图像的帧内编码，即以减少空间冗余进行数据压缩。</p>
<h5 id="1-2-时间冗余"><a href="#1-2-时间冗余" class="headerlink" title="1.2 时间冗余"></a>1.2 时间冗余</h5><p>时间冗余也称为时域冗余，它是针对视频序列图像而言的。视频序列每秒有25 ~ 30帧图像，相邻帧之间的时间间隔很小（例如，帧频为25Hz的电视信号，期帧间间隔只有 0.04s）,使得图像有很强的相关性。因此，对于视频压缩而言，通常采用运动估值和运动补偿预测技术来消除时间冗余。</p>
<h5 id="1-3-统计冗余"><a href="#1-3-统计冗余" class="headerlink" title="1.3 统计冗余"></a>1.3 统计冗余</h5><p>统计冗余也称编码表示冗余或符合冗余。由信息论的有关原来可知，为了表示图像数据的一个像素点，只要按其信息熵的大小分配相应的比特数即可。若用相同码长表示不同出现概率的符号，则会造成比特数的浪费。如果采用可变长编码技术，对出现概率大的符号用短码字表示，对出现概率小的符号用长码字表示，则可去除符号冗余，从而节约码字，这就是熵编码的思想。</p>
<h5 id="1-4-结构冗余"><a href="#1-4-结构冗余" class="headerlink" title="1.4 结构冗余"></a>1.4 结构冗余</h5><p>在有些图像的部分区域内有着很相似的纹理结构，或是图像的各个部分之间存在着某种关系，例如自相似性等，这些都是结构冗余的表现。分形图像编码的基本思想就是利用了结构冗余。</p>
<h5 id="1-5-知识冗余"><a href="#1-5-知识冗余" class="headerlink" title="1.5 知识冗余"></a>1.5 知识冗余</h5><p>在某些特定的应用场合，编码对象中包含的信息与某些先验的基本知识有关。这时，可以利用这些先验知识为编码对象建立模型。通过提取模型参数，对参数进行编码，而不是对图像像素值直接进行编码，可以达到非常高的压缩比。这是模型基编码（或称知识基编码、语义基编码）的基本思想。</p>
<h5 id="1-6-人眼的视觉冗余"><a href="#1-6-人眼的视觉冗余" class="headerlink" title="1.6 人眼的视觉冗余"></a>1.6 人眼的视觉冗余</h5><p>视觉冗余度是相对于人眼的视觉特性而言。压缩视觉冗余的核心思想是去掉那些相对人眼而言是看不到的或可有可无的图像数据。对视觉冗余的压缩通常反映在各种具体的压缩编码中。如对于离散余弦变换（DCT）系数的直流与低频部分采用细量化，而对高频部分采用粗量化。在帧间预测编码中，高压缩比的预测帧及双向预测帧的采用，也是利用了人眼对运动图像细节不敏感的特性。</p>
<h4 id="2-无失真编码和限失真编码"><a href="#2-无失真编码和限失真编码" class="headerlink" title="2. 无失真编码和限失真编码"></a>2. 无失真编码和限失真编码</h4><p>从信息论的角度出发，根据解码后还原的数据是否与原始数据完全相同，可将数据压缩方法分为两大类：无失真编码和限失真编码。</p>
<h5 id="无失真编码："><a href="#无失真编码：" class="headerlink" title="无失真编码："></a>无失真编码：</h5><p>无失真编码又称无损编码、信息保持编码、熵编码。</p>
<p>熵指的是具体数据所含的平均信息量，定义为在不丢失信息的前提下描述该信息内容所需的最小比特数。</p>
<p>熵编码事纯粹基于信号统计特性的一种编码方法，他利用信源概率分布的不均匀性，通过变长编码来减少信源数据冗余，解码后还原的数据与压缩编码前的原始数据完全相同而不引入任何失真。</p>
<p>但无失真编码的压缩比较低，可达到的最高压缩比受到信源熵的理论限制：一般是2:1到5:1。最常用的无失真编码方法有哈夫曼（Huffman）编码、算术编码和游程编码（Run-Length Encoding，RLE）等。</p>
<h5 id="限失真编码："><a href="#限失真编码：" class="headerlink" title="限失真编码："></a>限失真编码：</h5><p>限失真编码也称有损编码、非信息保持编码、熵压缩编码。也就是说，解码后还原的数据与压缩编码前的原始数据有差别的，编码会一定程度的失真。</p>
<p>限失真编码方法利用了人类视觉的感知特性，允许压缩过程中损失一部分信息，虽然在解码时不能完全恢复原始数据，但是如果把失真控制在视觉阈值一下或者控制在可容忍的限度你，则不影响人们对图像的理解，却换来了高压缩比。在限失真编码中，允许的是真愈大，则可达到的压缩比愈高。</p>
<p>常见的限失真编码方法：预测编码、变化编码、矢量量化、基于模型编码等。</p>
<h4 id="3-熵编码"><a href="#3-熵编码" class="headerlink" title="3. 熵编码"></a>3. 熵编码</h4><h5 id="3-1-熵-Entropy-：信源的平均信息量，更精确的描述为表示信源所有符号包含信息的平均比特数。"><a href="#3-1-熵-Entropy-：信源的平均信息量，更精确的描述为表示信源所有符号包含信息的平均比特数。" class="headerlink" title="3.1 熵(Entropy)：信源的平均信息量，更精确的描述为表示信源所有符号包含信息的平均比特数。"></a>3.1 熵(Entropy)：信源的平均信息量，更精确的描述为表示信源所有符号包含信息的平均比特数。</h5><ul>
<li>信源编码要尽可能的减少信源的冗余，使之接近熵</li>
<li>用更少的比特传递更多的信源信息</li>
</ul>
<h4 id="3-2-熵编码：数据压缩中根据信源消息的概率模型使消息的熵最小化"><a href="#3-2-熵编码：数据压缩中根据信源消息的概率模型使消息的熵最小化" class="headerlink" title="3.2 熵编码：数据压缩中根据信源消息的概率模型使消息的熵最小化"></a>3.2 熵编码：数据压缩中根据信源消息的概率模型使消息的熵最小化</h4><ul>
<li>无损压缩</li>
<li>变长压缩</li>
</ul>
<h3 id="压缩编码基础"><a href="#压缩编码基础" class="headerlink" title="压缩编码基础"></a>压缩编码基础</h3><h4 id="H-264中普通I帧和IDR帧究竟有什么区别"><a href="#H-264中普通I帧和IDR帧究竟有什么区别" class="headerlink" title="H.264中普通I帧和IDR帧究竟有什么区别"></a>H.264中普通I帧和IDR帧究竟有什么区别</h4><p>在MPEG2中，有个重要的概念叫GOP(group of pictures)，假设编码的帧类型为：IBBPBBPBBPBBIBBPBBPBBPBBI…, 那么这个IBBPBBPBBPBB就叫一个GOP. 由于误差会积累，但MPEG2中的I帧可以阻断误差的积累，也就是说，在MPEG中I帧后面的帧永远不会参考I帧前面的帧，也就是说，一个GOP中的帧永远不会参考前一个GOP中的帧.(另外说句题外话：B帧可以参考下一个GOP的I帧，但在MPEG2中，B帧不会作为参考帧，所以B帧不会导致误差积累)</p>
<p>在H.264中就不同了.很多人说，在H.264中没有I帧这个概念了，当然这也是有道理的，标准中的确没有这么叫，但是，为了方便，也可以延续I帧这个概念，那么H.264中什么叫I帧呢？
定义：H.264中的I帧是指帧中的宏块都是采用帧内预测方式，在H.264中有两种I帧: 普通I帧和IDR帧（特殊I帧）.</p>
<p>在H.264中，是IDR帧阻断了误差的积累, IDR帧后面的帧都不能参考该IDR帧前面的帧. 在H.264中，普通的I帧并没有阻断误差的积累，那就是说普通I帧后面的帧就可以参考该I帧之前的帧么？</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/06/20/media-video/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/20/media-video/" itemprop="url">
                  多媒体技术(三)之视频
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-20T23:08:35+08:00">
                2017-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/media/" itemprop="url" rel="index">
                    <span itemprop="name">media</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://wodekouwei.com/2017/06/20/media-audio/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="轻口味">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="老司机种菜">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/06/20/media-audio/" itemprop="url">
                  多媒体技术(二)之声音
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-20T23:08:29+08:00">
                2017-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/media/" itemprop="url" rel="index">
                    <span itemprop="name">media</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>信息论的观点来看，描述信源的数据是信息和数据冗余之和，即：数据=信息+数据冗余。音频信号在时域和频域上具有相关性，也即存在数据冗余。将音频作为一个信源，音频编码的实质是减少音频中的冗余。
自然界中的声音非常复杂，波形极其复杂，通常我们采用的是脉冲代码调制编码，即PCM编码。PCM通过抽样、量化、编码三个步骤将连续变化的模拟信号转换为数字编码。</p>
<h3 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a>内容介绍</h3><h4 id="声音三要素"><a href="#声音三要素" class="headerlink" title="声音三要素"></a>声音三要素</h4><p>声音的特性可由三个要素来描述，即响度、音调和音色。</p>
<ul>
<li>响度：人耳对声音强弱的主观感觉称为响度。响度和声波振动的幅度有关。一般说来，声波振动幅度越大则响度也越大。当我们用较大的力量敲鼓时，鼓膜振动的幅度大，发出的声音响；轻轻敲鼓时，鼓膜振动的幅度小，发出的声音弱。
音叉振动时发出的声波为单音，即只有一个频率成分。若设法将音叉的振动规律记录下来，可发现其振动波形为一正弦波。当用不同力量敲击某个音叉时，音叉发出的声波幅度不同，这意味着声音的响度不同。给出了两个声音波形，其幅度一大一小，幅度大的波形其声音响度大，幅度小的波形其声音响度小。另外，人们对响度的感觉还和声波的频率有关，同样强度的声波，如果其频率不同，人耳感觉到的响度也不同。</li>
<li>音调：人耳对声音高低的感觉称为音调。音调主要与声波的频率有关。声波的频率高，则音调也高。当我们分别敲击一个小鼓和一个大鼓时，会感觉它们所发出的声音不同。小鼓被敲击后振动频率快，发出的声音比较清脆，即音调较高；而大鼓被敲击后振动频率较慢，发出的声音比较低沉，即音调较低。如果分别敲击一个小音叉和一个大音叉时，同样会感觉到小音叉所发声音的音调较高，大音叉所发声音音调较低。如果设法把大、小音叉所发出的声波记录下来，可发现小音叉在单位时间内振动的次数多，即频率高，大音叉在单位时间内振动的次数少，即频率低。给出了两个频率不同的声音波形，从声音可听出，频率高的声音波形听起来音调较高，而频率低的声音波形听起来则音调较低。</li>
<li>音色：音色是人们区别具有同样响度、同样音调的两个声音之所以不同的特性，或者说是人耳对各种频率、各种强度的声波的综合反应。音色与声波的振动波形有关，或者说与声音的频谱结构有关。前面说过，音叉可产生一个单一频率的声波，其波形为正弦波。但实际上人们在自然界中听到的绝大部分声音都具有非常复杂的波形，这些波形由基波和多种谐波构成。谐波的多少和强弱构成了不同的音色。各种发声物体在发出同一音调声音时，其基波成分相同。但由于谐波的多少不同，并且各次谐波的幅度各异，因而产生了不同的音色。
例如当我们听胡琴和扬琴等乐器同奏一个曲子时，虽然它们的音调相同，但我们却能把不同乐器的声音区别开来。这是因为，各种乐器的发音材料和结构不同，它们发出同一个音调的声音时，虽然基波相同，但谐波构成不同，因此产生的波形不同，从而造成音色不同。给出了小提琴和钢琴的波形和声音，这两个声音的响度和音调都是相同的，但听起来却不一样，这就是因为这两个声音的音色不同（波形不同）。</li>
</ul>
<p>音调是指声音的高低，它是由声源振动的频率决定的．响度是指声音的大小，它是由声源振动的振幅决定的且与距离有关。比如我们看电视时，调节音量开关，可以控制声音的大小，而不能改变音调的高低．在日常生活用的语言中，往往用“高低”来表示声音的大小，比如“高声喧哗”中的“高”，不是指音调高低，而是指声音“大小”．在学习中不注意声音大小和高低的区别，会造成音调和响度两个不同的概念模糊不清，这在学习中应该引起特别注意。</p>
<h4 id="采样率和采样大小"><a href="#采样率和采样大小" class="headerlink" title="采样率和采样大小"></a>采样率和采样大小</h4><p>声音其实是一种能量波，因此也有频率和振幅的特征，频率对应于时间轴线，振幅对应于电平轴线。波是无限光滑的，弦线可以看成由无数点组成，由于存储空间是相对有限的，数字编码过程中，必须对弦线的点进行采样。采样的过程就是抽取某点的频率值，很显然，在一秒中内抽取的点越多，获取得频率信息更丰富，<strong>为了复原波形，一次振动中，必须有2个点的采样</strong>，人耳能够感觉到的最高频率为20kHz，因此要满足人耳的听觉要求，则需要至少每秒进行40k次采样，用40kHz表达，这个40kHz就是采样率。我们常见的CD，采样率为44.1kHz。光有频率信息是不够的，我们还必须获得该频率的能量值并量化，用于表示信号强度。量化电平数为2的整数次幂，我们常见的CD位16bit的采样大小，即2的16次方。采样大小相对采样率更难理解，因为要显得抽象点，举个简单例子：假设对一个波进行8次采样，采样点分别对应的能量值分别为A1-A8，但我们只使用2bit的采样大小，结果我们只能保留A1-A8中4个点的值而舍弃另外4个。如果我们进行3bit的采样大小，则刚好记录下8个点的所有信息。采样率和采样大小的值越大，记录的波形更接近原始信号。</p>
<h4 id="有损和无损"><a href="#有损和无损" class="headerlink" title="有损和无损"></a>有损和无损</h4><p>根据采样率和采样大小可以得知，相对自然界的信号，音频编码最多只能做到无限接近，至少目前的技术只能这样了，相对自然界的信号，任何数字音频编码方案都是有损的，因为无法完全还原。在计算机应用中，能够达到最高保真水平的就是PCM编码，被广泛用于素材保存及音乐欣赏，CD、DVD以及我们常见的WAV文件中均有应用。因此，PCM约定俗成了无损编码，因为PCM代表了数字音频中最佳的保真水准，并不意味着PCM就能够确保信号绝对保真，PCM也只能做到最大程度的无限接近。我们而习惯性的把MP3列入有损音频编码范畴，是相对PCM编码的。强调编码的相对性的有损和无损，是为了告诉大家，要做到真正的无损是困难的，就像用数字去表达圆周率，不管精度多高，也只是无限接近，而不是真正等于圆周率的值。</p>
<h4 id="使用音频压缩技术的原因"><a href="#使用音频压缩技术的原因" class="headerlink" title="使用音频压缩技术的原因"></a>使用音频压缩技术的原因</h4><p>要算一个PCM音频流的码率是一件很轻松的事情，采样率值×采样大小值×声道数 bps。一个采样率为44.1KHz，采样大小为16bit，双声道的PCM编码的WAV文件，它的数据速率则为 44.1K×16×2 =1411.2 Kbps。我们常说128K的MP3，对应的WAV的参数，就是这个1411.2 Kbps，这个参数也被称为数据带宽，它和ADSL中的带宽是一个概念。将码率除以8,就可以得到这个WAV的数据速率，即176.4KB/s。这表示存储一秒钟采样率为44.1KHz，采样大小为16bit，双声道的PCM编码的音频信号，需要176.4KB的空间，1分钟则约为10.34M，这对大部分用户是不可接受的，尤其是喜欢在电脑上听音乐的朋友，要降低磁盘占用，只有2种方法，降低采样指标或者压缩。降低指标是不可取的，因此专家们研发了各种压缩方案。由于用途和针对的目标市场不一样，各种音频压缩编码所达到的音质和压缩比都不一样，在后面的文章中我们都会一一提到。有一点是可以肯定的，他们都压缩过。</p>
<h4 id="频率与采样率的关系"><a href="#频率与采样率的关系" class="headerlink" title="频率与采样率的关系"></a>频率与采样率的关系</h4><p>采样率表示了每秒对原始信号采样的次数，我们常见到的音频文件采样率多为44.1KHz，这意味着什么呢？假设我们有2段正弦波信号，分别为20Hz和20KHz，长度均为一秒钟，以对应我们能听到的最低频和最高频，分别对这两段信号进行40KHz的采样，我们可以得到一个什么样的结果呢？结果是：20Hz的信号每次振动被采样了40K/20=2000次，而20K的信号每次振动只有2次采样。显然，在相同的采样率下，记录低频的信息远比高频的详细。这也是为什么有些音响发烧友指责CD有数码声不够真实的原因，CD的44.1KHz采样也无法保证高频信号被较好记录。要较好的记录高频信号，看来需要更高的采样率，于是有些朋友在捕捉CD音轨的时候使用48KHz的采样率，这是不可取的！这其实对音质没有任何好处，对抓轨软件来说，保持和CD提供的44.1KHz一样的采样率才是最佳音质的保证之一，而不是去提高它。较高的采样率只有相对模拟信号的时候才有用，如果被采样的信号是数字的，请不要去尝试提高采样率。</p>
<p>亨利·奈奎斯特(Harry Nyquist)采样定理:当对连续变化的信号波形进行采样时,若采样率fs高于该信号所含最高频率的两倍,那么可以由采样值通过插补技术正确的回复原信号中的波形,否则将会引起频谱混叠(Aliasing),产生混叠噪音(Aliasing Noise),而重叠的部分是不能恢复的.(同样适用于模拟视频信号的采样)
人声语音的特点。人类的听力感知范围是从20Hz到20kHz。这个频宽范围被划分成四个频宽类别：窄带、宽带、超宽带和全带。</p>
<h4 id="音轨和声道的区别？"><a href="#音轨和声道的区别？" class="headerlink" title="音轨和声道的区别？"></a>音轨和声道的区别？</h4><ul>
<li>音轨:过去，当歌手在录音棚里录音的情况跟现在大不一样 就是让歌手和乐队一起录音 歌手一边唱，乐队一边伴奏 然后录音机把这些声音一起录下来。 大多数读者都认为录音就是这样录 而且过去也的确这样录的 不过这样录音有一个很不方便的地方 那就是，如果歌手唱错了，录音师要歌手从新开始唱 那么乐队也要重新开始伴奏 如果歌手唱错一两次还没大关系 如果歌手唱错三、四次甚至是更多次数 那么乐队就要跟着重新伴奏四、五次或更多 而且这样的情况并不少见 这样乐队就会产生不满的情绪 还会产生其他一┎环奖? 后来就采用了一项新技术，可以避免这样的不便 那就是让乐队单独演奏，并用录音设备录下来，只是录在单独一条音轨上
然后让歌手带上耳机，听着乐队的伴奏录音演唱， 并录下来，录在另一条和伴奏音轨平行的音轨上 如果歌手唱错了，需要重新唱，只要把伴奏录音重新放就可以了 这种技术就是双音轨技术，这种技术能够把多种声音混合的录在一起，实现各种美妙的音效 其实我们唱的卡拉OK就是采用了音轨技术
其实以上说的是双音轨技术，还有多音轨技术，可以把三、四种声音或者更多的声音分别录制成单独的音轨然后一起播放出来</li>
<li>声道数:声卡所支持的声道数是衡量声卡档次的重要指标之一，从单声道到最新的环绕立体声，下面一一详细介绍：</li>
</ul>
<ol>
<li>单声道
单声道是比较原始的声音复制形式，早期的声卡采用的比较普遍。当通过两个扬声器回放单声道信息的时候，我们可以明显感觉到声音是从两个音箱中间传递到我们耳朵里的。这种缺乏位置感的录制方式用现在的眼光看自然是很落后的，但在声卡刚刚起步时，已经是非常先进的技术了。</li>
<li>立体声
单声道缺乏对声音的位置定位，而立体声技术则彻底改变了这一状况。声音在录制过程中被分配到两个独立的声道，从而达到了很好的声音定位效果。这种技术在音乐欣赏中显得尤为有用，听众可以清晰地分辨出各种乐器来自的方向，从而使音乐更富想象力，更加接近于临场感受。立体声技术广泛运用于自Sound Blaster Pro以后的大量声卡，成为了影响深远的一个音频标准。时至今日，立体声依然是许多产品遵循的技术标准。</li>
<li>准立体声
准立体声声卡的基本概念就是：在录制声音的时候采用单声道，而放音有时是立体声，有时是单声道。采用这种技术的声卡也曾在市面上流行过一段时间，但现在已经销声匿迹了。</li>
<li>四声道环绕
人们的欲望是无止境的，立体声虽然满足了人们对左右声道位置感体验的要求，但是随着技术的进一步发展，大家逐渐发现双声道已经越来越不能满足我们的需求。由于PCI声卡的出现带来了许多新的技术，其中发展最为神速的当数三维音效。三维音效的主旨是为人们带来一个虚拟的声音环境，通过特殊的HRTF技术营造一个趋于真实的声场，从而获得更好的游戏听觉效果和声场定位。而要达到好的效果，仅仅依靠两个音箱是远远不够的，所以立体声技术在三维音效面前就显得捉襟见肘了，但四声道环绕音频技术则很好的解决了这一问题。
四声道环绕规定了4个发音点：前左、前右，后左、后右，听众则被包围在这中间。同时还建议增加一个低音音箱，以加强对低频信号的回放处理(这也就是如今4.1声道音箱系统广泛流行的原因)。就整体效果而言，四声道系统可以为听众带来来自多个不同方向的声音环绕，可以获得身临各种不同环境的听觉感受，给用户以全新的体验。如今四声道技术已经广泛融入于各类中高档声卡的设计中，成为未来发展的主流趋势。</li>
<li>5.1声道
5.1声道已广泛运用于各类传统影院和家庭影院中，一些比较知名的声音录制压缩格式，譬如杜比AC-3（Dolby Digital）、DTS等都是以5.1声音系统为技术蓝本的，其中“.1”声道，则是一个专门设计的超低音声道，这一声道可以产生频响范围20～120Hz的超低音。其实5.1声音系统来源于4.1环绕，不同之处在于它增加了一个中置单元。这个中置单元负责传送低于80Hz的声音信号，在欣赏影片时有利于加强人声，把对话集中在整个声场的中部，以增加整体效果。相信每一个真正体验过Dolby AC-3音效的朋友都会为5.1声道所折服。</li>
</ol>
<h4 id="流特征"><a href="#流特征" class="headerlink" title="流特征"></a>流特征</h4><p>随着网络的发展，人们对在线收听音乐提出了要求，因此也要求音频文件能够一边读一边播放，而不需要把这个文件全部读出后然后回放，这样就可以做到不用下载就可以实现收听了；也可以做到一边编码一边播放，正是这种特征，可以实现在线的直播，架设自己的数字广播电台成为了现实。</p>
<h3 id="编码分类"><a href="#编码分类" class="headerlink" title="编码分类"></a>编码分类</h3><p>根据编码方式的不同，音频编码技术分为三种：波形编码、参数编码和混合编码。一般来说，波形编码的话音质量高，但编码速率也很高；参数编码的编码速率很低，产生的合成语音的音质不高；混合编码使用参数编码技术和波形编码技术，编码速率和音质介于它们之间。</p>
<h4 id="1、波形编码"><a href="#1、波形编码" class="headerlink" title="1、波形编码"></a>1、波形编码</h4><p>波形编码是指不利用生成音频信号的任何参数，直接将时间域信号变换为数字代码，使重构的语音波形尽可能地与原始语音信号的波形形状保持一致。波形编码的基本原理是在时间轴上对模拟语音信号按一定的速率抽样，然后将幅度样本分层量化，并用代码表示。
波形编码方法简单、易于实现、适应能力强并且语音质量好。不过因为压缩方法简单也带来了一些问题：压缩比相对较低，需要较高的编码速率。一般来说，波形编码的复杂程度比较低，编码速率较高、通常在16 kbit/s以上，质量相当高。但编码速率低于16 kbit/s时，音质会急剧下降。
最简单的波形编码方法是PCM（Pulse Code Modulation，脉冲编码调制），它只对语音信号进行采样和量化处理。优点是编码方法简单，延迟时间短，音质高，重构的语音信号与原始语音信号几乎没有差别。不足之处是编码速率比较高（64 kbit/s），对传输通道的错误比较敏感。</p>
<h4 id="2、参数编码"><a href="#2、参数编码" class="headerlink" title="2、参数编码"></a>2、参数编码</h4><p>参数编码是从语音波形信号中提取生成语音的参数，使用这些参数通过语音生成模型重构出语音，使重构的语音信号尽可能地保持原始语音信号的语意。也就是说，参数编码是把语音信号产生的数字模型作为基础，然后求出数字模型的模型参数，再按照这些参数还原数字模型，进而合成语音。
参数编码的编码速率较低，可以达到2.4 kbit/s，产生的语音信号是通过建立的数字模型还原出来的，因此重构的语音信号波形与原始语音信号的波形可能会存在较大的区别、失真会比较大。而且因为受到语音生成模型的限制，增加数据速率也无法提高合成语音的质量。不过，虽然参数编码的音质比较低，但是保密性很好，一直被应用在军事上。典型的参数编码方法为LPC（Linear Predictive Coding，线性预测编码）。</p>
<h4 id="3、混合编码"><a href="#3、混合编码" class="headerlink" title="3、混合编码"></a>3、混合编码</h4><p>混合编码是指同时使用两种或两种以上的编码方法进行编码。这种编码方法克服了波形编码和参数编码的弱点，并结合了波形编码高质量和参数编码的低编码速率，能够取得比较好的效果。</p>
<h3 id="编码格式"><a href="#编码格式" class="headerlink" title="编码格式"></a>编码格式</h3><h4 id="PCM编码"><a href="#PCM编码" class="headerlink" title="PCM编码"></a>PCM编码</h4><p>PCM 脉冲编码调制是Pulse Code Modulation的缩写。前面的文字我们提到了PCM大致的工作流程，我们不需要关心PCM最终编码采用的是什么计算方式，我们只需要知道PCM编码的音频流的优点和缺点就可以了。PCM编码的最大的优点就是音质好，最大的缺点就是体积大。我们常见的Audio CD就采用了PCM编码，一张光盘的容量只能容纳72分钟的音乐信息。
WAV格式
这是一种古老的音频文件格式，由微软开发。WAV是一种文件格式，符合RIFF (Resource Interchange File Format) 规范。所有的WAV都有一个文件头，这个文件头包含了音频流的编码参数。WAV对音频流的编码没有硬性规定，除了PCM之外，还有几乎所有支持ACM规范的编码都可以为WAV的音频流进行编码。很多朋友没有这个概念，我们拿AVI做个示范，因为AVI和WAV在文件结构上是非常相似的，不过AVI多了一个视频流而已。我们接触到的AVI有很多种，因此我们经常需要安装一些Decode才能观看一些AVI，我们接触到比较多的DivX就是一种视频编码，AVI可以采用DivX编码来压缩视频流，当然也可以使用其他的编码压缩。同样，WAV也可以使用多种音频编码来压缩其音频流，不过我们常见的都是音频流被PCM编码处理的WAV，但这不表示WAV只能使用PCM编码，MP3编码同样也可以运用在WAV中，和AVI一样，只要安装好了相应的Decode，就可以欣赏这些WAV了。
在Windows平台下，基于PCM编码的WAV是被支持得最好的音频格式，所有音频软件都能完美支持，由于本身可以达到较高的音质的要求，因此，WAV也是音乐编辑创作的首选格式，适合保存音乐素材。因此，基于PCM编码的WAV被作为了一种中介的格式，常常使用在其他编码的相互转换之中，例如MP3转换成WMA。</p>
<h4 id="MP3编码"><a href="#MP3编码" class="headerlink" title="MP3编码"></a>MP3编码</h4><p>动态图像专家组-1或动态图像专家组-2 音频层III（MPEG-1 or MPEG-2 Audio Layer III），经常称为MP3，是当今相当流行的一种数字音频编码和有损压缩格式，它被设计来大幅降低音频数据量，它舍弃PCM音讯资料中，对人类听觉不重要的资料，从而达到了压缩成较小的档案。而对于大多数用户的听觉感受来说，MP3的音质与最初的不压缩音频相比没有明显的下降。它是在1991年，由位于德国埃尔朗根的研究组织Fraunhofer-Gesellschaft的一组工程师发明和标准化的。MP3的普及，曾对音乐产业造成冲击与影响。
MP3是一个数据压缩格式。它舍弃脉冲编码调制（PCM）音频数据中，对人类听觉不重要的数据（类似于JPEG是一个有损图像压缩），从而达到了压缩成小得多的文件大小。</p>
<p>在MP3中使用了许多技术，其中包括心理声学，以确定音频的哪一部分可以丢弃。MP3音频可以按照不同的比特率进行压缩，提供了权衡数据大小和音质之间的依据。</p>
<p>MP3格式使用了混合的转换机制将时域信号转换成频域信号：</p>
<ul>
<li>32波段多相积分滤波器（PQF）</li>
<li>36或者12 tap 改良离散余弦滤波器（MDCT）；每个子波段大小可以在0…1和2…31之间独立选择</li>
<li>混叠衰减后处理</li>
</ul>
<p>尽管有许多创造和推广其他格式的重要努力，如 MPEG 标准中的 AAC（Advanced Audio Coding）和 Xiph.Org 开源无专利的 Ogg Vorbis。然而，由于MP3的空前的流通，在目前来说，其他格式不可能威胁其地位。MP3不仅有广泛的用户端软体支持，也有很多的硬件支持，比如便携式数位音频播放器（泛指MP3播放器）、移动电话、DVD和CD播放器。</p>
<p>MP3作为目前最为普及的音频压缩格式，为大家所大量接受，各种与MP3相关的软件产品层出不穷，而且更多的硬件产品也开始支持MP3，我们能够买到的VCD/DVD播放机都很多都能够支持MP3，还有更多的便携的MP3播放器等等，虽然几大音乐商极其反感这种开放的格式，但也无法阻止这种音频压缩的格式的生存与流传。MP3发展已经有10个年头了，他是MPEG(MPEG：Moving Picture Experts Group) Audio Layer-3的简称，是MPEG1的衍生编码方案，1993年由德国Fraunhofer IIS研究院和汤姆生公司合作发展成功。MP3可以做到12:1的惊人压缩比并保持基本可听的音质，在当年硬盘天价的日子里，MP3迅速被用户接受，随着网络的普及，MP3被数以亿计的用户接受。MP3编码技术的发布之初其实是非常不完善的，由于缺乏对声音和人耳听觉的研究，早期的mp3编码器几乎全是以粗暴方式来编码，音质破坏严重。随着新技术的不断导入，mp3编码技术一次一次的被改良，其中有2次重大技术上的改进。</p>
<p><strong>发展</strong>
MPEG-1 Audio Layer II编码开始时是德国Deutsche Forschungs- und Versuchsanstalt für Luft- und Raumfahrt（后来称为Deutsches Zentrum für Luft- und Raumfahrt, 德国太空中心）Egon Meier-Engelen管理的数字音频广播（DAB）项目。这个项目是欧盟作为EUREKA研究项目资助的，它的名字通常称为EU-147。EU-147的研究期间是1987年到1994年。</p>
<p>到了1991年，就已经出现了两个提案：Musicam（称为Layer 2）和ASPEC（自适应频谱感知熵编码）。荷兰飞利浦公司、法国CCETT和德国Institut für Rundfunktechnik提出的Musicam方法由于它的简单、出错时的稳定性以及在高质量压缩时较少的计算量而被选中。基于子带编码的Musicam格式是确定MPEG音频压缩格式（采样率、帧结构、数据头、每帧采样点）的一个关键因素。这项技术和它的设计思路完全融合到了ISO MPEG Audio Layer I、II以及后来的Layer III（MP3）格式的定义中。在Mussmann教授（汉诺威大学）的主持下，标准的制定由Leon van de Kerkhof（Layer I）和Gerhard Stoll（Layer II）完成。</p>
<p>一个由荷兰Leon Van de Kerkhof、德国Gerhard Stoll、法国Yves-François Dehery和德国Karlheinz Brandenburg组成的工作小组吸收了Musicam和ASPEC的设计思想，并添加了他们自己的设计思想从而开发出了MP3，MP3能够在128kbit/s达到MP2 192kbit/s音质。</p>
<p>所有这些算法最终都在1992年成为了MPEG的第一个标准组MPEG-1的一部分，并且生成了1993年公布的国际标准ISO／IEC 11172-3。MPEG音频上的更进一步的工作最终成为了1994年制定的第二个MPEG标准组MPEG-2标准的一部分，这个标准正式的称呼是1995年首次公布的ISO／IEC 13818-3。</p>
<p>编码器的压缩效率通常由比特率定义，因为压缩率依赖于位数（bit depth）和输入信号的采样率。然而，经常有产品使用CD参数（44.1kHz、两个通道、每通道16位或者称为2x16位）作为压缩率参考，使用这个参考的压缩率通常较高，这也说明了压缩率对于有损压缩存在的问题。</p>
<p>Karlheinz Brandenburg使用CD介质的Suzanne Vega的歌曲Tom’s Diner来评价MP3压缩算法。使用这首歌是因为这首歌的柔和、简单旋律使得在回放时更容易听到压缩格式中的缺陷。一些人开玩笑地将Suzanne Vega称为“MP3之母”。来自于EBU V3/SQAM参考CD的更多一些严肃和critical音频选段（钟琴，三角铁，手风琴，…）被专业音频工程师用来评价MPEG音频格式的主观感受质量。</p>
<p><strong>MP3走向大众</strong>
为了生成位兼容的MPEG Audio文件（Layer 1、Layer 2、Layer 3），ISO MPEG Audio委员会成员用C语言开发的一个称为ISO 11172-5的参考模拟软件。在一些非实时操作系统上它能够演示第一款压缩音频基于DSP的实时硬件解码。一些其他的MPEG Audio实时开发出来用于面向消费接收机和机顶盒的数字广播（无线电DAB和电视DVB）。</p>
<p>后来，1994年7月7日Fraunhofer-Gesellschaft发布了第一个称为l3enc的MP3编码器。</p>
<p>Fraunhofer开发组在1995年7月14日选定扩展名：”.mp3”（以前扩展名是”.bit”）。使用第一款实时软件MP3播放器Winplay3（1995年9月9日发布）许多人能够在自己的个人电脑上编码和回放MP3文件。由于当时的硬盘相对较小（如500MB），这项技术对于在计算机上存储娱乐音乐来说是至关重要的。</p>
<p><strong>MP2、MP3与因特网</strong>
1993年10月，MP2（MPEG-1 Audio Layer 2）文件在因特网上出现，它们经常使用Xing MPEG Audio Player播放，后来又出现了Tobias Bading为Unix开发的MAPlay。MAPlay于1994年2月22日首次发布，现在已经移植到微软视窗平台上。</p>
<p>刚开始仅有的MP2编码器产品是Xing Encoder和CDDA2WAV，CDDA2WAV是一个将CD音轨转换成WAV格式的CD抓取器。</p>
<p>Internet Underground Music Archive（IUMA）通常被认为是在线音乐革命的鼻祖，IUMA是因特网上第一个高保真音乐网站，在MP3和网络流行之前它有数千首授权的MP2录音。</p>
<p>从1995年上半年开始直到整个九十年代后期，MP3开始在因特网上蓬勃发展。MP3的流行主要得益于如Nullsoft于1997年发布的Winamp和于1999年发布的Napster，这样的公司和软件包的成功，并且它们相互促进发展。这些程序使得普通用户很容易地播放、制作、共享和收集MP3文件。</p>
<p>关于MP3文件的点对点技术文件共享的争论在最近几年迅速蔓延—这主要是由于压缩使得文件共享成为可能，未经压缩的文件过于庞大难于共享。由于MP3文件通过因特网大量传播，一些主要唱片厂商通过法律起诉Napster来保护它们的版权（参见知识产权）。</p>
<p>如iTunes Store这样的商业在线音乐发行服务通常选择其他或者专有的支持数字版权管理（DRM）的音乐文件格式以控制和限制数字音乐的使用。支持DRM的格式的使用是为了防止受版权保护的素材免被侵犯版权，但是大多数的保护机制都能被一些方法破解。这些方法能够被计算机高手用来生成能够自由复制的解锁文件。如果希望得到一个压缩的音频文件，这个录制的音频流必须进行压缩且代价是音质的降低。
<strong>比特率</strong>
比特率对于MP3文件来说是可变的。总的原则是比特率越高则声音文件中包含的原始声音信息越多，这样回放时声音质量也越高。在MP3编码的早期，整个文件使用一个固定的比特率，称为固定码率（CBR）。</p>
<p>MPEG-1 Layer 3允许使用的比特率是32、40、48、56、64、80、96、112、128、160、192、224、256和320 kbit/s，允许的采样频率是32、44.1和48kHz。44.1kHz是最为经常使用的速度（与CD的采样速率相同），128kbit/s是事实上“好品质”的标准，尽管320kbit/s在P2P文件共享网络上越来越受到欢迎。MPEG-2和[非正式的]MPEG-2.5包括其他一些比特率：6、12、24、32、40、48、56、64、80、96、112、128、144、160kbit/s。</p>
<p>可变码率（VBR）也是可能的。MP3文件的中的音频切分成有自己不同比特率的帧，这样在文件编码的时候就可以动态地改变比特率。尽管在最初的实现中并没有这项功能。VBR技术现在音频/视频编码领域已经得到了广泛的应用，这项技术使得在声音变化大的部分使用较大的比特率而在声音变化小的部分使用较小的比特率成为可能。这个方法类似于声音控制的磁带录音机不记录静止部分节省磁带消耗。一些编码器在很大程度上依赖于这项技术。</p>
<p>高达640kbit/s的比特率可以使用LAME编码器和自由格式来实现，但是由于它并非标准比特率之一，有些低端或早期的MP3播放器不能够播放这些文件。</p>
<p><strong>MP3的音频质量</strong>
因为MP3是一种有损压缩格式，它提供了多种不同“比特率”（bit rate）的选项—也就是用来表示每秒音频所需的编码数据位数。典型的速度介于128kbps和320kbps（kbit/s）之间。与此对照的是，CD上未经压缩的音频比特率是1411.2 kbps（16位／采样点× 44100采样点／秒× 2通道）。</p>
<p>使用较低比特率编码的MP3文件通常回放质量较低。使用过低的比特率，“压缩噪声（compression artifact）”（原始录音中没有的声音）将会在回放时出现。说明压缩噪声的一个好例子是：压缩欢呼的声音；由于它的随机性和急剧变化，所以编码器的错误就会更明显，并且听起来就象回声。</p>
<p>除了编码文件的比特率之外；MP3文件的质量，也与编码器的质量以及编码信号的难度有关。使用优质编码器编码的普通信号，一些人认为128kbit/s的MP3以及44.1kHz的CD采样的音质近似于CD音质，同时得到了大约11:1的压缩率。在这个比率下正确编码的MP3只能够获得比调频广播更好的音质，这主要是那些模拟介质的带宽限制、信噪比和其他一些限制。然而，听力测试显示经过简单的练习测试听众能够可靠地区分出128kbit/s MP3与原始CD的区别[来源请求]。在许多情况下他们认为MP3音质不佳是不可接受的，然而其他一些听众或者换个环境（如在嘈杂的车中或者聚会上）他们又认为音质是可接受的。很显然，MP3编码的瑕疵在低端声卡或者扬声器上比较不明显，而在连接到计算机的高质量立体声系统，尤其是使用高保真音响设备或者高质量的耳机时则比较明显。</p>
<p>Fraunhofer Gesellschaft（FhG）在他们的官方网站上，公布了下面的MPEG-1 Layer 1/2/3的压缩率和数据速率用于比较：</p>
<ul>
<li>Layer 1: 384 kbit/s，压缩率4:1</li>
<li>Layer 2: 192 - 256 kbit/s，压缩率8:1-6:1</li>
<li>Layer 3: 112 - 128 kbit/s，压缩率12:1-10:1
不同层面之间的差别是因为它们使用了不同的心理声学模型导致的；Layer 1的算法相当简单，所以透明编码就需要更高的比特率。然而，由于不同的编码器使用不同的模型，很难进行这样的完全比较。</li>
</ul>
<p>许多人认为所引用的速率，出于对Layer 2和Layer 3记录的偏爱，而出现了严重扭曲。他们争辩说实际的速率如下所列：</p>
<ul>
<li>Layer 1: 384 kbit/s优秀</li>
<li>Layer 2: 256 - 384 kbit/s优秀，224 - 256 kbit/s很好，192 - 224 kbit/s好</li>
<li>Layer 3: 224 - 320 kbit/s优秀，192 - 224 kbit/s很好，128 - 192 kbit/s好</li>
</ul>
<p>当比较压缩机制时，很重要的是要使用同等音质的编码器。将新编码器与基于过时技术甚至是带有缺陷的旧编码器比较可能会产生对于旧格式不利的结果。由于有损编码会丢失信息这样一个现实，MP3算法通过创建人类听觉总体特征的模型尽量保证丢弃的部分不被人耳识别出来（例如，由于noise masking），不同的编码器能够在不同程度上实现这一点。</p>
<p>一些可能的编码器：</p>
<ul>
<li>Mike Cheng在1998年早些时候首次开发的LAME。与其他相比，它是一个完全遵循LGPL的MP3编码器，它有良好的速度和音质，甚至对MP3技术的后继版本形成了挑战[来源请求]。</li>
<li>Fraunhofer Gesellschaft：有些编码器不错，有些有缺陷。
有许多的早期编码器现在已经不再广泛使用：</li>
<li>ISO dist10</li>
<li>Xing</li>
<li>BladeEnc</li>
<li>ACM Producer Pro.</li>
</ul>
<p>好的编码器能够在128到160kbit/s下达到可接受的音质，在160到192kbit/s下达到接近透明的音质。所以不在特定编码器或者最好的编码器话题内说128kbit/s或者192kbit/s下的音质是容易引起误解的。一个好的编码器在128kbit/s下生成的MP3有可能比一个不好的编码器在192kbit/s下生成的MP3音质更好。另外，即使是同样的编码器同样的文件大小，一个不变比特率的MP3可能比一个变比特率的MP3音质要差很多。</p>
<p>需要注意的一个重要问题是音频信号的质量是一个主观判断。安慰效果（Placebo effect）是很严重的，许多用户声明要有一定水准的透明度。许多用户在A/B测试中都没有通过，他们无法在更低的比特率下区分文件。一个特定的比特率对于有些用户来说是足够的，对于另外一些用户来说是不够的。每个人的声音感知可能有所不同，所以一个能够满足所有人的特定心理声学模型并不明显存在。仅仅改变试听环境，如音频播放系统或者环境可能就会显现出有损压缩所产生的音质降低。上面给出的数字只是大多数人的一个大致有效参考，但是在有损压缩领域真正有效的压缩过程质量测试手段就是试听音频结果。</p>
<p>如果你的目标是实现没有质量损失的音频文件或者用在演播室中的音频文件，就应该使用无损压缩（Lossless）算法，目前能够将16位PCM音频数据压缩到38%并且声音没有任何损失，这样的无损压缩编码有LA、Sony ATRAC Advanced Lossless、Dolby TrueHD、DTS Master Lossless Audio、MLP、Sony Reality Audio、WavPack、Apple Lossless、TTA、FLAC、Windows Media Audio 9 Lossless（WMA）和APE（Monkey’s Audio）等等。</p>
<p>对于需要进行编辑、混合处理的音频文件要尽量使用无损格式，否则有损压缩产生的误差可能在处理后无法预测，多次编码产生的损失将会混杂在一起，在处理之后进行编码这些损失将会变得更加明显。无损压缩在降低压缩率的代价下能够达到最好的结果。</p>
<p>一些简单的编辑操作，如切掉音频的部分片段，可以直接在MP3数据上操作而不需要重新编码。对于这些操作来说，只要使用合适的软件（”mp3DirectCut”和”MP3Gain”），上面提到的问题可以不必考虑。</p>
<p><strong>MP3的设计限制</strong>
MP3格式存有设计限制，即使使用更好的编码器仍旧不能克服这些限制。一些新的压缩格式如AAC、Ogg Vorbis等不再有这些限制。</p>
<p>按照技术术语，MP3有如下一些限制：</p>
<ul>
<li>编码的比特率位速最高质量可达320Kbps，基本不损失原本音效质量[4]。</li>
<li>时间分辨率相对于变化迅速的信号来说太低。</li>
<li>采样频率最高为48kHz，对于超过48kHz采样频率的音频无法编码在MP3内，而CD经常使用的采样速率为44.1kHz。</li>
<li>联合立体声（Joint stereo）是基于帧与帧完成的。</li>
<li>没有定义编码器／解码器的整体时延，这就意味着gapless playback缺少一个正式的规定。
然而，即使有这些限制，一个经良好的调整MP3编码器仍能够提供与其他格式相提并论或更高的编码质量。</li>
</ul>
<p>MPEG-1标准中没有MP3编码器的一个精确规范，然而与此相反，解码算法和文件格式却进行了细致的定义。人们设想编码的实现是设计自己的适合去除原始音频中部分信息的算法（或者是它在频域中的修正离散余弦（MDCT）表示）。在编码过程中，576个时域样本被转换成576个频域样本，如果是瞬变信号就使用192而不是576个采样点，这是限制量化噪声随着随瞬变信号短暂扩散。</p>
<p>这是听觉心理学的研究领域：人类主观声音感知。</p>
<p>这样带来的结果就是出现了许多不同的MP3编码器，每种生成的声音质量都不相同。有许多它们的比较结果，这样一个潜在用户很容易选择合适的编码器。需要记住的是高比特率编码表现优秀的编码器（如LAME这个在高比特率广泛使用的编码器）未必在低比特率的表现也同样好。</p>
<p><strong>MP3音频编码</strong>
MPEG-1标准中没有MP3编码器的一个精确规范，然而与此相反，解码算法和文件格式却进行了细致的定义。人们设想编码的实现是设计自己的适合去除原始音频中部分信息的算法（或者是它在频域中的修正离散余弦（MDCT）表示）。在编码过程中，576个时域样本被转换成576个频域样本，如果是瞬变信号就使用192而不是576个采样点，这是限制量化噪声随着随瞬变信号短暂扩散。</p>
<p>这是听觉心理学的研究领域：人类主观声音感知。</p>
<p>这样带来的结果就是出现了许多不同的MP3编码器，每种生成的声音质量都不相同。有许多它们的比较结果，这样一个潜在用户很容易选择合适的编码器。需要记住的是高比特率编码表现优秀的编码器（如LAME这个在高比特率广泛使用的编码器）未必在低比特率的表现也同样好。</p>
<p><strong>MP3音频解码</strong>
另一方面，解码在标准中进行了细致的定义。</p>
<p>多数解码器是bitstream compliant，也就是说MP3文件解码出来的非压缩输出信号将与标准文档中数学定义的输出信号一模一样（在规定的近似误差范围内）。</p>
<p>MP3文件有一个标准的格式，这个格式就是包括384、576、或者1152个采样点（随MPEG的版本和层不同而不同）的帧，并且所有的帧都有关联的头信息（32位）和辅助信息（9、17或者32字节，随着MPEG版本和立体声或者单通道的不同而不同）。头和辅助信息能够帮助解码器正确地解码相关的霍夫曼编码数据。</p>
<p>所以，大多数的解码器比较几乎都是完全基于它们的计算效率（例如，它们在解码过程中所需要的内存或者CPU时间）。</p>
<p><strong>关于VBR</strong>
VBR：MP3格式的文件有一个有意思的特征，就是可以边读边放，这也符合流媒体的最基本特征。也就是说播放器可以不用预读文件的全部内容就可以播放，读到哪里播放到哪里，即使是文件有部分损坏。虽然mp3可以有文件头，但对于mp3格式的文件却不是很重要，正因为这种特性，决定了MP3文件的每一段每一帧都可以单独的平均数据速率，而无需特别的解码方案。于是出现了一种叫VBR（Variable bitrate，动态数据速率）的技术，可以让MP3文件的每一段甚至每一帧都可以有单独的bitrate，这样做的好处就是在保证音质的前提下最大程度的限制了文件的大小。这种技术的优越性是显而易见的，但要运用确实是一件难事，因为这要求编码器知道如何为每一段分配bitrate，这对没有波形分析的编码器而言，这种技术如同虚设。正是如此，VBR技术并没有一出现就显得光彩夺目。
专家们通过长期的声学研究，发现人耳存在 <strong>遮蔽效应</strong>。声音信号实际是一种能量波，在空气或其他媒介中传播，人耳对声音能量的多少即响度或声压最直接的反应就是听到这个声音的大小，我们称它为响度，表示响度这种能量的单位为分贝（dB）。即使是同样响度的声音，人们也会因为它们频率不同而感觉到声音大小不同。人耳最容易听到的就是4000Hz的频率，不管频率是否增高或降低，即使是响度在相同的情况下，大家都会觉得声音在变小。但响度降到一定程度时，人耳就听不到了，每一个频率都有着不同的值。
可以看到这条曲线基本成一个V字型，当频率超过15000Hz时，人耳的会感觉到声音很小，很多听觉不是很好的人，根本就听不到20000Hz的频率，不管响度有多大。当人耳同时听到两个不同频率、不同响度的声音时，响度较小的那个也会被忽略，例如：在白天我们很难听到电脑中散热风扇的声音，晚上却成了噪声源，根据这种原理，编码器可以过滤掉很多听不到的声音，以简化信息复杂度，增加压缩比，而不明显的降低音质。这种遮蔽被称为同时遮蔽效应。但声音A被声音B遮蔽，如果A处于B为中心的遮蔽范围内，遮蔽会更明显,这个范围叫临界带宽。每一种频率的临界带宽都不一样，频率越高的临界带宽越宽。</p>
<p><strong>频率(Hz) 临界带宽(Hz) 频率(Hz) 临界带宽(Hz)</strong>
根据这种效应，专家们设计出人耳听觉心理模型，这个模型被导入到mp3编码中后，导致了一场翻天覆地的音质革命，mp3编码技术一直背负着音质差的恶名，但这个恶名现在已经逐渐被洗脱。到了此时，一直被埋没的VBR技术光彩四射，配合心理模型的运用便现实出强大的诱惑力与杀伤力。
长期来，很多人对MP3印象不好，更多人认为WMA的最佳音质要好过MP3，这种说法是不正确的，在中高码率下，编码得当的MP3要比WMA优秀很多，可以非常接近CD音质，在不太好的硬件设备支持下，没有多少人可以区分两者的差异，这不是神话故事，尽管你以前盲听就可以很轻松区分MP3和CD，但现在你难保证你可以分辨正确。因为MP3是优秀的编码，以前被埋没了。</p>
<h4 id="OGG编码"><a href="#OGG编码" class="headerlink" title="OGG编码"></a>OGG编码</h4><p>网络上出现了一种叫Ogg Vorbis的音频编码，号称MP3杀手！Ogg Vorbis究竟什么来头呢？OGG是一个庞大的多媒体开发计划的项目名称，将涉及视频音频等方面的编码开发。整个OGG项目计划的目的就是向任何人提供完全免费多媒体编码方案！OGG的信念就是：OPEN！FREE！Vorbis这个词汇是特里·普拉特柴特的幻想小说《Small Gods》中的一个”花花公子”人物名。这个词汇成为了OGG项目中音频编码的正式命名。目前Vorbis已经开发成功，并且开发出了编码器。
Ogg Vorbis是高质量的音频编码方案，官方数据显示：Ogg Vorbis可以在相对较低的数据速率下实现比MP3更好的音质！Ogg Vorbis这种编码也远比90年代开发成功的MP3先进，它可以支持多声道，这意味着什么？这意味着Ogg Vorbis在SACD、DTSCD、DVD AUDIO抓轨软件（目前这种软件还没有）的支持下，可以对所有的声道进行编码，而不是MP3只能编码2个声道。多声道音乐的兴起，给音乐欣赏带来了革命性的变化，尤其在欣赏交响时，会带来更多临场感。这场革命性的变化是MP3无法适应的。
和MP3一样，Ogg Vorbis是一种灵活开放的音频编码，能够在编码方案已经固定下来后还能对音质进行明显的调节和新算法的改良。因此，它的声音质量将会越来越好，和MP3相似，Ogg Vorbis更像一个音频编码框架，可以不断导入新技术逐步完善。和MP3一样，OGG也支持VBR。</p>
<h4 id="MPC编码"><a href="#MPC编码" class="headerlink" title="MPC编码"></a>MPC编码</h4><p>MPC是又是另外一个令人刮目相看的实力派选手，它的普及过程非常低调，也没有什么复杂的背景故事，她的出现目的就只有一个，更小的体积更好的音质！MPC以前被称作MP+，很显然，可以看出她针对的竞争对手是谁。但是，只要用过这种编码的人都会有个深刻的印象，就是她出众的音质。</p>
<h4 id="mp3PRO编码"><a href="#mp3PRO编码" class="headerlink" title="mp3PRO编码"></a>mp3PRO编码</h4><p>2001年6月14日，美国汤姆森多媒体公司(Thomson Multimedia SA)与佛朗赫弗协会(Fraunhofer Institute)于6月14日发布了一种新的音乐格式版本，名称为mp3PRO，这是一种基于mp3编码技术的改良方案，从官方公布的特征看来确实相当吸引人。从各方面的资料显示，mp3PRO并不是一种全新的格式，完全是基于传统mp3编码技术的一种改良，本身最大的技术亮点就在于SBR（Spectral Band Replication 频段复制），这是一种新的音频编码增强算法。它提供了改善低位率情况下音频和语音编码的性能的可能。这种方法可在指定的位率下增加音频的带宽或改善编码效率。SBR最大的优势就是在低数据速率下实现非常高效的编码，与传统的编码技术不同的是，SBR更像是一种后处理技术，因此解码器的算法的优劣直接影响到音质的好坏。高频实际上是由解码器（播放器）产生的，SBR编码的数据更像是一种产生高频的命令集，或者称为指导性的信号源，这有点駇idi的工作方式。我们可以看到，mp3PRO其实是一种mp3信号流和SBR信号流的混合数据流编码。有关资料显示，SBR技术可以改善低数据流量下的高频音质，改善程度约为30%，我们不管这个30%是如何得来的，但可以事先预知这种改善可以让64kbps的mp3达到128kbps的mp3的音质水平（注：在相同的编码条件下，数据速率的提升和音质的提升不是成正比的，至少人耳听觉上是这样的），这和官方声称的64kbps的mp3PRO可以媲美128kbps的mp3的宣传基本是吻合的。</p>
<h4 id="WMA格式"><a href="#WMA格式" class="headerlink" title="WMA格式"></a>WMA格式</h4><p>WMA就是Windows Media Audio编码后的文件格式，由微软开发，WMA针对的不是单机市场，是网络！竞争对手就是网络媒体市场中著名的Real Networks。微软声称，在只有64kbps的码率情况下，WMA可以达到接近CD的音质。和以往的编码不同，WMA支持防复制功能，她支持通过Windows Media Rights Manager 加入保护，可以限制播放时间和播放次数甚至于播放的机器等等。WMA支持流技术，即一边读一边播放，因此WMA可以很轻松的实现在线广播，由于是微软的杰作，因此，微软在Windows中加入了对WMA的支持，WMA有着优秀的技术特征，在微软的大力推广下，这种格式被越来越多的人所接受。</p>
<h4 id="RA格式"><a href="#RA格式" class="headerlink" title="RA格式"></a>RA格式</h4><p>RA就是RealAudio格式，这是各位网虫接触得非常多的一种格式，大部分音乐网站的在线试听都是采用了RealAudio，这种格式完全针对的就是网络上的媒体市场，支持非常丰富的功能。最大的闪烁点就是这种格式可以根据听众的带宽来控制自己的码率，在保证流畅的前提下尽可能提高音质。RA可以支持多种音频编码，包括ATRAC3。和WMA一样，RA不但都支持边读边放，也同样支持使用特殊协议来隐匿文件的真实网络地址，从而实现只在线播放而不提供下载的欣赏方式。这对唱片公司和唱片销售公司很重要，在各方的大力推广下，RA和WMA是目前互联网上，用于在线试听最多的音频媒体格式。</p>
<h4 id="APE格式"><a href="#APE格式" class="headerlink" title="APE格式"></a>APE格式</h4><p>APE是Monkey’s Audio提供的一种无损压缩格式。Monkey’s Audio提供了Winamp的插件支持，因此这就意味着压缩后的文件不再是单纯的压缩格式，而是和MP3一样可以播放的音频文件格式。这种格式的压缩比远低于其他格式，但能够做到真正无损，因此获得了不少发烧用户的青睐。在现有不少无损压缩方案种，APE是一种有着突出性能的格式，令人满意的压缩比以及飞快的压缩速度，成为了不少朋友私下交流发烧音乐的唯一选择。</p>
<h3 id="格式特点"><a href="#格式特点" class="headerlink" title="格式特点"></a>格式特点</h3><p>各种各样的音频编码都有其技术特征及不同场合的适用性，我们大致讲解一下如何去灵活应用这些音频编码。</p>
<h4 id="PCM编码的WAV"><a href="#PCM编码的WAV" class="headerlink" title="PCM编码的WAV"></a>PCM编码的WAV</h4><p>前面就提到过，PCM编码的WAV文件是音质最好的格式，Windows平台下，所有音频软件都能够提供对她的支持。Windows提供的WinAPI中有不少函数可以直接播放wav，因此，在开发多媒体软件时，往往大量采用wav，用作事件声效和背景音乐。PCM编码的wav可以达到相同采样率和采样大小条件下的最好音质，因此，也被大量用于音频编辑、非线性编辑等领域。</p>
<ul>
<li>特点：音质非常好，被大量软件所支持。</li>
<li>适用于：多媒体开发、保存音乐和音效素材。</li>
</ul>
<h4 id="MP3"><a href="#MP3" class="headerlink" title="MP3"></a>MP3</h4><p>MP3具有不错的压缩比，使用LAME编码的中高码率的mp3，听感上已经非常接近源WAV文件。使用合适的参数，LAME编码的MP3很适合于音乐欣赏。由于MP3推出年代已久，加之还算不错的音质及压缩比，不少游戏也使用mp3做事件音效和背景音乐。几乎所有著名的音频编辑软件也提供了对MP3的支持，可以将mp3象wav一样使用，但由于mp3编码是有损的，因此多次编辑后，音质会急剧下降，mp3并不适合保存素材，但作为作品的demo确实相当优秀的。mp3长远的历史和不错的音质，使之成为应用最广的有损编码之一，网络上可以找到大量的mp3资源，mp3player日渐成为一种时尚。不少VCDPlayer、DVDPlayer甚至手机都可以播放mp3，mp3是被支持的最好的编码之一。MP3也并非完美，在较低码率下表现不好。MP3也具有流媒体的基本特征，可以做到在线播放。</p>
<ul>
<li>特点：音质好，压缩比比较高，被大量软件和硬件支持，应用广泛。</li>
<li>适用于：适合用于比较高要求的音乐欣赏。</li>
</ul>
<h4 id="OGG"><a href="#OGG" class="headerlink" title="OGG"></a>OGG</h4><p>Ogg是一种非常有潜力的编码，在各种码率下都有比较惊人的表现，尤其中低码率下。Ogg除了音质好之外，她还是一个完全免费的编码，这对ogg被更多支持打好了基础。Ogg有着非常出色的算法，可以用更小的码率达到更好的音质，128kbps的Ogg比192kbps甚至更高码率的mp3还要出色。Ogg的高音具有一定的金属味道，因此在编码一些高频要求很高的乐器独奏时，Ogg的这个缺陷会暴露出来。OGG具有流媒体的基本特征，但现在还没有媒体服务软件支持，因此基于ogg的数字广播还无法实现。Ogg目前的被支持的情况还不够好，无论是软件的还是硬件的，都无法和mp3相提并论。</p>
<ul>
<li>特点：可以用比mp3更小的码率实现比mp3更好的音质，高中低码率下均具有良好的表现。</li>
<li>适用于：用更小的存储空间获得更好的音质（相对MP3）。</li>
</ul>
<h4 id="MPC"><a href="#MPC" class="headerlink" title="MPC"></a>MPC</h4><p>和OGG一样，MPC的竞争对手也是mp3，在中高码率下，MPC可以做到比竞争对手更好音质，在中等码率下，MPC的表现不逊色于Ogg，在高码率下，MPC的表现更是独孤求败，MPC的音质优势主要表现在高频部分，MPC的高频要比MP3细腻不少，也没有Ogg那种金属味道，是目前最适合用于音乐欣赏的有损编码。由于都是新生的编码，和Ogg际遇相似，也缺乏广泛的软件和硬件支持。MPC有不错的编码效率，编码时间要比OGG和LAME短不少。</p>
<ul>
<li>特点：中高码率下，具有有损编码中最佳的音质表现，高码率下，高频表现极佳。</li>
<li>适用于：在节省大量空间的前提下获得最佳音质的音乐欣赏。<h4 id="WMA"><a href="#WMA" class="headerlink" title="WMA"></a>WMA</h4>微软开发的WMA同样也是不少朋友所喜爱的，在低码率下，有着好过mp3很多的音质表现，WMA的出现，立刻淘汰了曾经风靡一时的VQF编码。有微软背景的WMA获得了很好的软件及硬件支持，Windows Media Player就能够播放WMA，也能够收听基于WMA编码技术的数字电台。因为播放器几乎存在于每一台PC上，越来越多的音乐网站都乐意使用WMA作为在线试听的首选了。除了支持环境好之外，WMA在64-128kbps码率下也具有相当出色的表现，虽然不少要求较高的朋友并不够满意，但更多要求不高的朋友接受了这种编码，WMA很快的普及开了。</li>
<li>特点：低码率下的音质表现难有对手。</li>
<li>适用于：数字电台架设、在线试听、低要求下的音乐欣赏。</li>
</ul>
<h4 id="mp3PRO"><a href="#mp3PRO" class="headerlink" title="mp3PRO"></a>mp3PRO</h4><p>作为mp3的改良版本的mp3PRO表现出了相当不错的素质，高音丰满，虽然mp3PRO是通过SBR技术在播放过程中插入的，但实际听感相当不错，虽然显得有点单薄，但在64kbps的世界里已经没有对手了，甚至超过了128kbps的mp3，但很遗憾的是，mp3PRO的低频表现也象mp3一样的破，所幸的是，SBR的高频插值可以或多或少的掩盖掉这个缺陷，因此mp3PRO的低频弱势反而不如WMA那么明显。大家可以在使用RCA mp3PRO Audio Player的PRO开关来切换PRO模式和普通模式时深深的感觉到。整体而言，64kbps的mp3PRO达到了128kbps的mp3的音质水平，在高频部分还略有胜出。</p>
<ul>
<li>特点：低码率下的音质之王。</li>
<li>适用于：低要求下的音乐欣赏。</li>
</ul>
<h4 id="APE"><a href="#APE" class="headerlink" title="APE"></a>APE</h4><p>一种新兴的无损音频编码，可以提供50-70%的压缩比，虽然比起有损编码来太不值得一提了，但对于追求完美音质的朋友简直是天大的福音。APE可以做到真正的无损，而不仅是听起来无损，压缩比也要比类似的无损格式要好。</p>
<ul>
<li>特点：音质非常好。</li>
<li>适用于：最高品质的音乐欣赏及收藏。</li>
</ul>
<h3 id="相关算法"><a href="#相关算法" class="headerlink" title="相关算法"></a>相关算法</h3><h4 id="音量计算"><a href="#音量计算" class="headerlink" title="音量计算"></a>音量计算</h4><p>音量值计算：<code>db=20*lg(x/2^15)</code>，其中x表示样点幅度值，db表示分贝值。</p>
<p>声音是一种波，PCM数据是波形的描述，音量值表示波的能量，和波的振幅相关，和各点的相对差值有关。16bits的采样值表示范围是-32768~32767。把每个点依次连接起来就是声音的波形了。求音量先将数据转换成-1~1之间（因为位率是是精度表示，要转换成相对最大值的比例），进行傅立叶变换，提高速度使用快速傅立叶变换（FFT），求出当时的频谱图，就是各个频率的音量大小。求平均就是总音量。</p>
<p>FFT变换有实部数据和虚部数据，其能量值是（实部<em>实部+虚部</em>虚部）的开方，而声音的大小是分贝，20<em>lg(能量值)，所以频谱图各音量是10</em>lg(实部<em>实部+虚部</em>虚部)</p>
<h3 id="音频编码技术比较"><a href="#音频编码技术比较" class="headerlink" title="音频编码技术比较"></a>音频编码技术比较</h3><p><img src="http://images.wodekouwei.com/technology/audio_cmp.png" alt="image">
说明：质量评价共五个等级（1、2、3、4、5），其中5.0为最高分。
上表中各种算法、应用领域中缩略语的中文和英文全称参见下面说明。</p>
<ul>
<li>PCM：Pulse Code Modulation，脉冲编码调制。</li>
<li>ADPCM：Adaptive Differential Pulse Code Modulation，自适应差分脉冲编码调制。</li>
<li>SB-ADPCM：Subband Adaptive Differential Pulse Code Modulation，子带-自适应差分脉冲编码调制。</li>
<li>LPC：Linear Predictive Coding，线性预测编码。</li>
<li>CELPC：Code Excited Linear Predictive Coding，码激励线性预测编码。</li>
<li>VSELPC：Vector Sum Excited Linear Predictive Coding，矢量和激励线性预测编码。</li>
<li>RPE-LTP：Regular Pulse Excited-Long Term Predictive，规则脉冲激励长时预测。</li>
<li>LD-CELP：Low Delay-Code Excited Linear Predictive，低时延码激励线性预测。</li>
<li>MPE：Multi-Pulse Excited，多脉冲激励。</li>
<li>PSTN：Public Switched Telephone Network，公共交换电话网。</li>
<li>ISDN：Integrated Services Digital Network，综合业务数字网。</li>
</ul>
<h3 id="Android-Audio相关api"><a href="#Android-Audio相关api" class="headerlink" title="Android Audio相关api"></a>Android Audio相关api</h3><ul>
<li>AcousticEchoCanceler:回声消除器</li>
<li>AutomaticGainControl:自动增强控制器</li>
<li>NoiseSuppressor:噪音抑制器</li>
<li>BassBoost:重低音调节器</li>
<li>Equalizer:均衡器</li>
<li>PresetReverb:预设音场控制器</li>
<li>Visualizer:示波器</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://images.wodekouwei.com/avatar/winnle_the_pooh.jpg"
               alt="轻口味" />
          <p class="site-author-name" itemprop="name">轻口味</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">124</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">60</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/qingkouwei" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/LightTaste" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.douban.com/people/turnpp/" target="_blank" title="豆瓣">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  豆瓣
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/shen-jun-wei-9/" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="https://github.com/ossrs/srs" title="SRS" target="_blank">SRS</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">轻口味</span>
</div>

<div>
<a href="http://www.miitbeian.gov.cn/">京ICP备17018543号</a>

        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      本站访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  
    
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "bb46b146831e4e34808d09cd94c85f50",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

</body>
</html>
