<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[关于QT QAudioOutput push模式问题]]></title>
    <url>%2F2017%2F09%2F13%2Ftips-qt-audiooutput%2F</url>
    <content type="text"><![CDATA[在MAC上基于QT和ffmpeg实现最简单播放器,在完成声音播放模块后导致无法正常播放,QAudioOutput start后state仍是idel,将QAudioOutput buffer写满后就不能继续写数据了,导致播放被卡死. QAudioOutput创建代码: 12345678910111213141516171819QAudioFormat fmt;fmt.setSampleRate(this-&gt;sampleRate);fmt.setSampleSize(this-&gt;sampleSize);fmt.setChannelCount(this-&gt;channel);fmt.setCodec(&quot;audio/pcm&quot;);fmt.setByteOrder(QAudioFormat::LittleEndian);fmt.setSampleType(QAudioFormat::UnSignedInt);QAudioDeviceInfo info = QAudioDeviceInfo::defaultOutputDevice();printf(&quot;deviceName:%s\n&quot;,qPrintable(info.deviceName()));if (!info.isFormatSupported(fmt)) &#123; printf(&quot;default format not supported try to use nearest\n&quot;); fmt = info.nearestFormat(fmt);&#125;output = new QAudioOutput(fmt);//connect(output, SIGNAL(stateChanged(QAudio::State)), this, SLOT(handleStateChanged(QAudio::State)));io = output-&gt;start();printf(&quot;io:%p\n&quot;, io);//printf(&quot;state:%s&quot;,output-&gt;state());QAudio::State stat = output-&gt;state(); 在stackoverflow Qt QAudioOutput push mode看到别人的问题和解释,发现是setSampleType引起的,代码在window上可以正常跑,只是在mac上失败. 问题I’ve got a question about using QAudioOutput to directly write samples at a specific sample rate to the sound output device. I’m writing an emulator that emualates sound chips on a per-frame basis, and then gets a buffer containing a frame’s worth of audio samples, which I would like to write to the audio output. Currently, to test my audio output routine, I allocate a huge (5 minute) buffer to put random numbers into, like so: Header: 1234uint16_t *audio_outputBuffer;uint32_t audio_bytesRemainingToRead;QAudioOutput *audio_outputStream;QIODevice *audio_outputDevice; Implementation: 12345678910audio_outputBuffer = (uint16_t *) malloc((96000 * 4) * 300);int i = 0;uint16_t *tempAudioBuffer = audio_outputBuffer;for(i = 0; i &lt; ((96000 * 4) * 150); i++) &#123; *tempAudioBuffer = (uint16_t) rand() &amp; 0xFFFF; tempAudioBuffer++;&#125;audio_bytesRemainingToRead = (96000 * 4) * 300; Next, I set up my audio device with some basic parameters: 1234567891011121314151617// Set up the formatQAudioFormat format;format.setFrequency(96000); // Usually this is specified through an UI optionformat.setChannels(2);format.setSampleSize(16);format.setCodec(&quot;audio/pcm&quot;);format.setByteOrder(QAudioFormat::LittleEndian);format.setSampleType(QAudioFormat::UnSignedInt);// There&apos;s code here to notify the user of inability to match the format and choose an action, which is omitted for clarity// Create audio output stream, set up signalsaudio_outputStream = new QAudioOutput(format, this);connect(audio_outputStream, SIGNAL(stateChanged(QAudio::State)), this, SLOT(audio_stateChanged(QAudio::State)));audio_outputDevice = audio_outputStream-&gt;start(); Then, in my timer tick routine, which is called by a QTimer at 60 FPS, I do the following code to write a ‘chunk’ of audio data to the QAudioOutput’s buffer: 123456if(audio_outputDevice-&gt;isOpen() &amp;&amp; audio_outputStream-&gt;state() != QAudio::StoppedState) &#123; qint64 bytesOfAudioWrittenToDevice = audio_outputDevice-&gt;write((char *) audio_outputBuffer, audio_outputStream-&gt;periodSize()); audio_bytesRemainingToRead -= bytesOfAudioWrittenToDevice; qDebug() &lt;&lt; &quot;Wrote&quot; &lt;&lt; bytesOfAudioWrittenToDevice &lt;&lt; &quot;bytes of audio to output device. Remaining bytes to read:&quot; &lt;&lt; audio_bytesRemainingToRead; qDebug() &lt;&lt; &quot;Free bytes in audio output:&quot; &lt;&lt; audio_outputStream-&gt;bytesFree();&#125; Once I start the audio output process, I get the following output on the console: 12345678910Current audio state: 3 Error: 0Wrote 2048 bytes of audio to output device. Remaining bytes to read: 115197952Free bytes in audio output: 6144Current audio state: 0 Error: 0Wrote 2048 bytes of audio to output device. Remaining bytes to read: 115195904Free bytes in audio output: 4096Wrote 2048 bytes of audio to output device. Remaining bytes to read: 115193856Free bytes in audio output: 2048Wrote 2048 bytes of audio to output device. Remaining bytes to read: 115191808Free bytes in audio output: 0 (This and the above line is repeated forever) To me, it looks like QAudioOutput isn’t flushing it’s internal buffer to the sound card, which goes along with the entire “no sound coming out of my computer” thing. What would cause this issue, and how could I fix it? (By the way, I’m compiling my code against Qt 4.8.1, on Mac OS X 10.7.4.) Thanks for any answers. 解释Upfront just wanna point out: This is not a Qt bug. Why? The answer is that in the WAV spec’, 8-bit samples are always unsigned, whereas 16-bit samples are always signed. Any other combination does not work. This is device related, the framework can not do anything about it. So this will not work because you have set 16 bit sample size and unsigned integer format. And yes, the solution is: you have to set the sample type to signed for 16-bit resolution: 1format.setSampleType(QAudioFormat::SignedInt); Inversely for 8-bit samples you would have to put: 1format.setSampleType(QAudioFormat:UnsignedInt); Also this very similar question (same problem but with 8-bit) shows you that it is not a particular problem of using signed or unsigned samples in Qt, but that it is the combination of samples size and type that matters (for the audio device, not for Qt ;) QAudioOutput in Qt5 is not producing any sound IMHO the fact that Qt does not take care of handling these cases by forcing the correct format is a flaw but not a lack in functionnality. You can learn more about this in the notes section of this page: https://ccrma.stanford.edu/courses/422/projects/WaveFormat/ I’ve figured this out — apparently Qt has issues with UNSIGNED samples. If you set the sample type to signed, everything works fine, regardless of platform.]]></content>
      <categories>
        <category>QT</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>QT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FFMPEG api使用流程]]></title>
    <url>%2F2017%2F09%2F13%2Fffmpeg-arch%2F</url>
    <content type="text"><![CDATA[ffmpeg接口使用流程比较固定: av_register_all():注册所有模块 int ret = avformat_open_input(&amp;ic, ofn, 0, 0);:获取AVFormatContext ic(ofn为输入文件地址) for(i = 0; i &lt; ic-&gt;nb_streams; i++):遍历AVFormatContext中所有stream,分别找到Audio与Video对应的AVCodecContext; AVCodec *codec = avcodec_find_decoder(enc-&gt;codec_id);:根据AVCodecContext中codec_id获取到AVCodec; avcodec_open2(enc, codec,NULL):打开AVCodec; 接下来分配AVPacket与AVFrame]]></content>
      <categories>
        <category>FFMPEG</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>FFMPEG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FFMPEG编解码]]></title>
    <url>%2F2017%2F09%2F13%2Fffmpeg-codec%2F</url>
    <content type="text"><![CDATA[解码ffmpeg3版本的解码接口做了调整,之前的视频解码接口avcodec_decode_video2和音频解码接口avcodec_decode_audio4被设置为deprecated,对这两个接口做了合并,使用同一的接口.并且将音视频解码步骤分成了两步,第一步avcodec_send_packet,第二步avcodec_receive_frame, 旧版本avcodec_decode_video2旧版本avcodec_decode_audio4123456789101112int got_picture; ret = avcodec_decode_audio4(enc, pcm,&amp;got_picture, pkt); if ( ret &lt; 0 ) &#123; char buf[1024] = &#123;0&#125;; av_strerror(err, buf, sizeof(buf)); printf(buf); printf(&quot;avcodec_decode_audio4 failed:%d&quot; ,got_picture); av_packet_unref(pkt); av_frame_free(&amp;pcm); if(ic) avformat_close_input(&amp;ic); return -1; &#125; 将AVPacket的pkt解码成AVFrame的pcm 新版本avcodec_send_packet接口源码:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Supply raw packet data as input to a decoder. * * Internally, this call will copy relevant AVCodecContext fields, which can * influence decoding per-packet, and apply them when the packet is actually * decoded. (For example AVCodecContext.skip_frame, which might direct the * decoder to drop the frame contained by the packet sent with this function.) * * @warning The input buffer, avpkt-&gt;data must be AV_INPUT_BUFFER_PADDING_SIZE * larger than the actual read bytes because some optimized bitstream * readers read 32 or 64 bits at once and could read over the end. * * @warning Do not mix this API with the legacy API (like avcodec_decode_video2()) * on the same AVCodecContext. It will return unexpected results now * or in future libavcodec versions. * * @note The AVCodecContext MUST have been opened with @ref avcodec_open2() * before packets may be fed to the decoder. * * @param avctx codec context * @param[in] avpkt The input AVPacket. Usually, this will be a single video * frame, or several complete audio frames. * Ownership of the packet remains with the caller, and the * decoder will not write to the packet. The decoder may create * a reference to the packet data (or copy it if the packet is * not reference-counted). * Unlike with older APIs, the packet is always fully consumed, * and if it contains multiple frames (e.g. some audio codecs), * will require you to call avcodec_receive_frame() multiple * times afterwards before you can send a new packet. * It can be NULL (or an AVPacket with data set to NULL and * size set to 0); in this case, it is considered a flush * packet, which signals the end of the stream. Sending the * first flush packet will return success. Subsequent ones are * unnecessary and will return AVERROR_EOF. If the decoder * still has frames buffered, it will return them after sending * a flush packet. * * @return 0 on success, otherwise negative error code: * AVERROR(EAGAIN): input is not accepted right now - the packet must be * resent after trying to read output * AVERROR_EOF: the decoder has been flushed, and no new packets can * be sent to it (also returned if more than 1 flush * packet is sent) * AVERROR(EINVAL): codec not opened, it is an encoder, or requires flush * AVERROR(ENOMEM): failed to add packet to internal queue, or similar * other errors: legitimate decoding errors */int avcodec_send_packet(AVCodecContext *avctx, const AVPacket *avpkt); 参数分析 AVCodecContext *avctx：第一个参数与旧的接口一致，是视频解码的上下文，包含解码器。 const AVPacket *avpkt： 编码的音视频帧数据 为什么要传递空的avpkt 这里有一个说明是可以传递NULL，什么情况下需要传递NULL，你平时看一些视频播放器，播放经常会少最后几帧，很多情况就是因为没有处理好缓冲帧的问题，ffmpeg内部会缓冲几帧，要想取出来就需要传递空的AVPacket进去。 新版本avcodec_receive_frame接口源码12345678910111213141516171819/** * Return decoded output data from a decoder. * * @param avctx codec context * @param frame This will be set to a reference-counted video or audio * frame (depending on the decoder type) allocated by the * decoder. Note that the function will always call * av_frame_unref(frame) before doing anything else. * * @return * 0: success, a frame was returned * AVERROR(EAGAIN): output is not available right now - user must try * to send new input * AVERROR_EOF: the decoder has been fully flushed, and there will be * no more output frames * AVERROR(EINVAL): codec not opened, or it is an encoder * other negative values: legitimate decoding errors */int avcodec_receive_frame(AVCodecContext *avctx, AVFrame *frame); 参数分析 AVCodecContext *avctx：第一个参数视频解码的上下文，与上面接口一致。 AVFrame *frame：解码后的视频帧数据。 空间申请和释放问题解码后图像空间由函数内部申请，你所做的只需要分配 AVFrame 对象空间，如果你每次调用avcodec_receive_frame传递同一个对象，接口内部会判断空间是否已经分配，如果没有分配会在函数内部分配。 avcodec_send_packet和avcodec_receive_frame调用关系并不一定是一对一的，比如一些音频数据一个AVPacket中包含了1秒钟的音频，调用一次avcodec_send_packet之后，可能需要调用25次 avcodec_receive_frame才能获取全部的解码音频数据，所以要做如下处理： 12345678910int re = avcodec_send_packet(codec, pkt);if (re != 0)&#123; return;&#125;while( avcodec_receive_frame(codec, frame) == 0)&#123; //读取到一帧音频或者视频 //处理解码后音视频 frame&#125; 参考 send/receive encoding and decoding API overview]]></content>
      <categories>
        <category>FFMPEG</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>FFMPEG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FFMPEG工具方法]]></title>
    <url>%2F2017%2F09%2F13%2Fffmpeg-util%2F</url>
    <content type="text"><![CDATA[av_strerror1234int av_strerror ( int errnum,char * errbuf,size_t errbuf_size)]]></content>
      <categories>
        <category>FFMPEG</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>FFMPEG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[音视频参数之Profile]]></title>
    <url>%2F2017%2F09%2F12%2Ftips-media-profile%2F</url>
    <content type="text"><![CDATA[前两天发现公司产品 触发 有导入从微信等下载的外部小视频时失败的情况, 触发 导入视频时使用开源库media-for-mobile对视频进行重新编码.media-for-mobile使用android系统接口android.media.MediaExtractor对mp4文件解复用,使用android硬件编解码接口android.media.MediaCodec对视频解码-处理-编码操作,最后通过Android系统APIandroid.media.MediaMuxer将视频写成文件. 反馈的两个有问题视频,一个视频导入时报BufferOverFlowException错误,另一个视频不报错但是导入时进度一直为0%. 经分析出问题的两个视频的Video profile为High,普通视频为Baseline,报BufferOverFlowException错误的视频的Audio profile为HE-AAC,普通视频的Audio profile为LC. 详细了解了一下H264 与AAC profile: H264各种profile作为行业标准，H.264编码体系定义了4种不同的Profile(类)：Baseline(基线类),Main(主要类), Extended(扩展类)和High Profile(高端类)（它们各自下分成许多个层）： Baseline Profile: 提供I/P帧，仅支持progressive(逐行扫描)和CAVLC； Extended Profile: 提供I/P/B/SP/SI帧，仅支持progressive(逐行扫描)和CAVLC； Main Profile: 提供I/P/B帧，支持progressive(逐行扫描)和interlaced(隔行扫描)，提供CAVLC或CABAC； High Profile: （也就是FRExt）在Main Profile基础上新增：8x8 intra prediction(8x8 帧内预测), custom quant(自定义量化), lossless video coding(无损视频编码), 更多的yuv格式（4:4:4…）； H.264在高清中具有最小体积。在同等图像质量下，采用H.264技术压缩后的数据量只有MPEG2的1/8，MPEG4的1/3，相对Xvid、Divx等属于MPEG4编码而言，其体积优势明显，在互联网上，H.264资源处在爆发趋势。而High Profile是H.264解码中的高端类型，拥有最完善的支持程度、最优秀的特性，可以说是高清视频编码中的劳斯莱斯，只有征服了这个优秀的编码，MP4才能将H.264完全掌控，也才能充分享受到高清视频带来的视觉震撼，意义非凡。 针对当前高清是视频会议行业的主流发展趋势，而目前高清普及的主要阻力之一就是带宽的限制。而High Profile H.264技术在同等视频质量的情况下可节省50%的带宽，为客户节省大量的网络带宽成本。据业内专家介绍，此次H.264 High Profile的推出是视频技术的一个巨大进步，其意义不亚于2003年从H.263向H.264过渡的价值。它将为目前正在推广的高清视频通信应用扫清了令人头疼的网络带宽障碍，不仅如此，这些变化对于包括CIF、标清等品质的视频通信应用也同样适用。 鉴于High Profile H.264对于视频会议行业的非凡影响，目前已有国内外厂商尝鲜，宣布其旗下产品全面支持该项技术，包括宝利通、华平、华腾网讯。从这一趋势来看，未来视频会议产品全面支持High Profile H.264也经成为不可逆转的潮流，而高清普及也在技术层面更近了一步。 参考: H264 各种profile AAC各种profileAAC共有9种规格，以适应不同的场合的需要： MPEG-2 AAC LC: 低复杂度规格（Low Complexity）–比较简单，没有增益控制，但提高了编码效率，在中等码率的编码效率以及音质方面，都能找到平衡点 MPEG-2 AAC Main: 主规格 MPEG-2 AAC SSR: 可变采样率规格（Scaleable Sample Rate） MPEG-4 AAC LC: 低复杂度规格（Low Complexity）——现在的手机比较常见的MP4文件中的音频部份就包括了该规格音频文件 MPEG-4 AAC Main: 主规格 ——包含了除增益控制之外的全部功能，其音质最好 MPEG-4 AAC SSR: 可变采样率规格（Scaleable Sample Rate） MPEG-4 AAC LTP: 长时期预测规格（Long Term Predicition） MPEG-4 AAC LD: 低延迟规格（Low Delay） MPEG-4 AAC HE: 高效率规格（High Efficiency）—–这种规格适合用于低码率编码，有Nero ACC 编码器支持 14496-3标准，里面定义的profile除了上述的一些规格，还有如Scalable 、 TwinVQ、 CELP、 HVXC等更多其他的profile。 目前听到用的比较多的应该是LC和HE(适合低码率)。流行的Nero AAC的命令行编码程序就支持LC，HE，HEv2这三种，试用后，用MediaInfo分析了编码后的AAC音频，发现规格显示都是LC，当时就感到奇怪，不是说支持三种规格吗？然后才又查资料发现，原来HE其实就是AAC（LC）+SBR技术，HEv2就是AAC（LC）+SBR+PS技术，难怪用MediaInfo分析后，HE规格的文件即显示: 123格式简介:LC格式设置,SBR:是格式设置,PS:否 HE与HEv2 HE：“high efficiency”（高效性）。HE-AAC v1（又称AACPlusV1，SBR)用容器的方法加了原AAC（LC）+SBR技术。SBR其实代表的是Spectral Band Replication(频段复制)。简单概括一下，音乐的主要频谱集中在低频段，高频段幅度很小（但很重要，决定了音质），如果对整个频段编码，要么为了保护高频造成低频段编码过细以致文件巨大，要么为了保存了低频的主要成分而失去高频成分以致丧失音质。SBR把频谱切割开来，低频单独编码保存主要成分，高频单独放大编码保存音质，“统筹兼顾”了，在减少文件大小的情况下还保存了音质，完美的化解了一对矛盾 HEv2 它用容器的方法包含了HE-AAC v1和PS技术。PS指“parametric stereo”（参数立体声）。这个其实好理解，原来的立体声文件，文件大小是一个声道的两倍。但是两个声道的声音存在某种相似性，根据香农信息熵编码定理，相关性应该被去掉才能减小文件大小。所以PS技术存储了一个声道的全部信息，然后，花很少的字节用参数描述另一个声道和它不同的地方 这样，HEv1和HEv2用个图简单表示下就是：(图中的AAC即指的是原来的AAC-LC) 由于NERO AAC编码后产生的是经过MP4容器封装后的，而我们的decoder需要处理的是未经封装的AAC流，因此还需要处理从MP4封装格式中extract出AAC流的步骤；哦，这里提到了MP4容器封装，就再把我看到的一些关于MP4容器的心得插入在此也说下： 其实.mp4格式规范是MPEG4 Part 1标准定义的。但是这个格式本身相当通用，并不是只能用来存贮MPEG4视频格式。举个例子，一个.mp4文件中包含的可能是H.263的视频轨及AMR的音频轨。这样它和MPEG4视频压缩算法就半点边都沾不上。但它绝对是一个合法的.mp4文件。从这个意义上讲，.mp4是一个独立的封包格式。也许它的原始设计意图是仅用于MPEG4，但事实上大家觉得它很好用，已经把它扩展成可以包容其它格式了。现在市场上比如某产品号称“支持MP4播放”，到底是什么意思呢？如果它是指可以播放.mp4这种文件，那里面的音频和视频格式它能支持多少种组合呢？没说清楚吧。举个极端的例子，假设一台设备仅支持“视频为未压缩YUV以及不带音频轨的.mp4文件，但它的文件名确实可以是.mp4，是不是也可以在盒子上印上“支持MP4”呢？那么，买回去，复制一个网上下载的.mp4文件（MPEG4视频和AAC音频应该是个比较流行的组合），结果却发现根本不能播放。就算不举这么极端的例子，一般.mp4文件中常见的视频音频格式也有多种，一个产品要做到支持所有的格式是很难的。所以，如果要准确的描述，应该写清楚类似“支持视频格式为MPEG4或H.264/AVC，音频为AMR或AAC的*.mp4文件”。其实更严格一些，还应该写清楚MPEG4支持到哪种profile, AMR是NB还是WB，AAC是LC还是HE等更多细节。当然，这种误导型的说明应该在减少，不过如果有比较确切的格式需求，最好还是先搞清楚这些细节。看到网上还有人说到N73，其实只支持视频为MPEG4 Simple Profile / Advanced Simple Profile及H.263 Profile 0 &amp; 3，音频为AMR-NB/WB或者AAC-LC, HE-AAC的mp4文件。如果你放一个视频格式为H.264/AVC的mp4上去，是无法播放出画面来的。 在网上找了一些工具，如MP4UI,MP4BOX,Yamb(mp4box的GUI程序),采用它们进行extract操作后发现，原来的SBR和PS等信息咋没有了，都变成LC规格的AAC文件啦。好容易准备的测试流，难道还是不能用？于是一番苦寻发现，可能是SBR和PS等信息在ADTS头中是无法体现的，所以分析ADTS格式头的AAC，就无法判别是否是HE和HEv2啦。但是我总觉得SBR和PS等技术信息在AAC流中应该还是存在的。因为我还在一个国外的论坛上看到这么几句话：There’s no requirement for MP4 with AAC to have SBR indicated in the headers. It’s still correct not to have it marked and have SBR or PS data in the stream anyway. Likewise, decoding a frame and not seeing any SBR or PS info doesn’t mean you can’t find it further up in the stream anyway（我理解就是说SBR OR PS信息不一定在Header中有，但是并不意味着你不能进一步在stream中发现它）。 HE-AAC的.mp4码流，经过extract出AAC(ADTS)后，44.1KHZ的变成了22.05KHZ。HEv2-AAC的.mp4码流，经过extract出AAC(ADTS)后，不但44.1KHZ的变成了22.05KHZ(一半)，连2channels也变成了1channels，这个问题更奇怪了，在论坛上找，发现也有人有此问题：“I get 22050Hz, 1 channel for audio that is in fact 44100Hz, 2channels and having both SBR and PS”。 后来看到MSDN中的AAC Decoder的描述中有这么一小段话： The media type gives the sample rate and number of channels prior to the application of spectral band replication (SBR) and parametric stereo (PS) tools, if present. The effect of the SBR tool is to double the decoded sample rate relative to the core AAC-LC sample rate. The effect of the PS tool is to decode stereo from a mono-channel core AAC-LC stream. 我的理解是AAC的decoder如果支持SBR和PS，会将AAC-HEV1(SBR)中的sample rate提高一倍，而会将AAC-HEV2(SBR+PS)中不仅sample rate提高一倍，单声道也提高至双声道了。结合前面提到的SBR(频段复制)和PS(参数立体声）技术的简单介绍，好像觉得这样是有点儿道理的哦~~ 用IPP example提供的解码工具simple_player简单试了下，对于44.1khz，stereo的HEv2-AAC的.mp4码流，经过extract出22.05KHZ，mono 的AAC(ADTS)后，再使用simple_player进行音频解码测试，解完后，果然发现又恢复了44.1khz和stereo。（但目前也测试了好几种extract出的HE和HEv2的aac码流，有的能将sample rate和channel 又double回来，有的又不能，这个具体原因是不是由于Ipp example提供的解码器的问题还不确定）。 另外，用simple_player如果直接decoder编码出的经过封装的.mp4格式的AAC音频的话，发现：其它都正常，只AAC-HEv2格式的.mp4音频解码后变成了单声道。难道是解码器中的PS tools没能发挥作用？初步估计应该是IPP 的那个小解码器的问题吧。 关于ADTS&amp;ADIF上面说到了ADTS头格式的AAC。其实，AAC的音频文件格式有以下两种： ADIF：Audio Data Interchange Format 音频数据交换格式。这种格式的特征是可以确定的找到这个音频数据的开始，不需进行在音频数据流中间开始的解码，即它的解码必须在明确定义的开始处进行。故这种格式常用在磁盘文件中。 ADTS：Audio Data Transport Stream 音频数据传输流。这种格式的特征是它是一个有同步字的比特流，解码可以在这个流中任何位置开始。它的特征类似于mp3数据流格式。 简单说，ADTS可以在任意帧解码，也就是说它每一帧都有头信息。ADIF只有一个统一的头，所以必须得到所有的数据后解码。且这两种的header的格式也是不同的，具体的组织结构在这里就不详说了。 参考: AAC的各种规格 问题原因分析首先BufferOverFlowException问题,由于音频采用AAC-HE导致MediaExtractor解析出的音频采样率为本身采样率的一半,同时创建出的解码OutputBuffer的大小是编码InputBuffer的2倍,而media-for-mobile开源项目直接将从解码器OutputBuffer取出的数据塞入编码器InputBuffer中,2倍的数据放入一倍的Buffer导致溢出,暂时的解决办法手动指定编码器InputBuffer max-size为一个较大值(10*1024).同时,由于MediaExtractor不能获取正确的audio profile,也无法确认获取到的采样率是否不正确采样率的一半,所以采用ffmpeg接口获取profile,但是ffmpeg调用avcodec_open2获取到的profile为-99,而且采样率依然为正常采样率一半,只有在解码一帧音频后才能得到正确的profile与采样率. 其次,转码进度不增加的问题,主要是High的H264视频,media-for-mobile使用一个MediaExtractor一次抽取音视频帧,如果是音频则交音频解码器,如果为视频则交视频解码器,解码后将解码帧转交编码器,编码后将数据写入MediaMuxer,将数据写入MediaMuxer的前提是吊用过addTrack,将音视频track加入到Muxer中,而addTrack需要在音视频解码若干帧后产生INFO_OUTPUT_FORMAT_CHANGED,若没有addTrack会导致编码后数据如法写入Muxer,卡死编码器,Baseline视频只需要少量帧就可以产生INFO_OUTPUT_FORMAT_CHANGED,但High视频产生INFO_OUTPUT_FORMAT_CHANGED需要更多的帧,而media-for-mobile中,MediaExtractor首先一直获取到的是音频数据,音频一直解码编码,但是输出的muxer时,videotrack未被添加,所以无法将音频编码器的数据写入muxer,音频解码器被卡死,而mediasource一直被产生的audio数据无法被消费,无法获取到视频数据导致视频INFO_OUTPUT_FORMAT_CHANGED一直无法产生,最终产生死锁,导致audio等待video的INFO_OUTPUT_FORMAT_CHANGED,video等待audio被读完后读到video解码产生INFO_OUTPUT_FORMAT_CHANGED. 最后将audio和video使用两个MediaExtractor各读取各自内容.]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>音视频</tag>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android自动化测试(一):UiAutomator官方介绍]]></title>
    <url>%2F2017%2F08%2F30%2Fat-android-start%2F</url>
    <content type="text"><![CDATA[了解android测试需要查询android官方文档,android官方培训教程Getting Started with Testing介绍了android提供的测试类型,测试接口等,相较与网上总结的android自动化测试框架,官方文档显然分类更合理,定位更准确. 两种测试类型在使用Android Studio创建模块时会在src下生成androidTest和test两个用于测试的的目录,对应下面两种测试类型: 本地单元测试(Local unit tests)位于module-name/src/test/java/.下,运行在PC端本地的JVM虚拟机上,并且不能访问Android框架的接口. 参考Building Local Tests 设备化测试位于module-name/src/androidTest/java/.下,必须运行在Android物理设备和虚拟机上.]]></content>
      <categories>
        <category>autotest</category>
      </categories>
      <tags>
        <tag>android</tag>
        <tag>autotest</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android自动化测试(N):UiAutomator官方介绍]]></title>
    <url>%2F2017%2F08%2F30%2Fat-android-uiautomator-official%2F</url>
    <content type="text"><![CDATA[Automating User Interface TestsUser interface (UI) testing lets you ensure that your app meets its functional requirements and achieves a high standard of quality such that it is more likely to be successfully adopted by users. One approach to UI testing is to simply have a human tester perform a set of user operations on the target app and verify that it is behaving correctly. However, this manual approach can be time-consuming, tedious, and error-prone. A more efficient approach is to write your UI tests such that user actions are performed in an automated way. The automated approach allows you to run your tests quickly and reliably in a repeatable manner. Note: It is strongly encouraged that you use Android Studio for building your test apps, because it provides project setup, library inclusion, and packaging conveniences. This class assumes you are using Android Studio. To automate UI tests with Android Studio, you implement your test code in a separate Android test folder (src/androidTest/java). The Android Plug-in for Gradle builds a test app based on your test code, then loads the test app on the same device as the target app. In your test code, you can use UI testing frameworks to simulate user interactions on the target app, in order to perform testing tasks that cover specific usage scenarios. For testing Android apps, you typically create these types of automated UI tests: UI tests that span a single app: This type of test verifies that the target app behaves as expected when a user performs a specific action or enters a specific input in its activities. It allows you to check that the target app returns the correct UI output in response to user interactions in the app’s activities. UI testing frameworks like Espresso allow you to programmatically simulate user actions and test complex intra-app user interactions. UI tests that span multiple apps: This type of test verifies the correct behavior of interactions between different user apps or between user apps and system apps. For example, you might want to test that your camera app shares images correctly with a 3rd-party social media app, or with the default Android Photos app. UI testing frameworks that support cross-app interactions, such as UI Automator, allow you to create tests for such scenarios. The lessons in this class teach you how to use the tools and APIs in the Android Testing Support Library to build these types of automated tests. Before you begin building tests using these APIs, you must install the Android Testing Support Library, as described in Downloading the Android Testing Support Library. UI TestingIn addition to unit testing the individual components that make up your Android application (such as activities, services, and content providers), it is also important that you test the behavior of your application’s user interface (UI) when it is running on a device. UI testing ensures that your application returns the correct UI output in response to a sequence of user actions on a device, such as entering keyboard input or pressing toolbars, menus, dialogs, images, and other UI controls. Functional or black-box UI testing does not require testers to know the internal implementation details of the app, only its expected output when a user performs a specific action or enters a specific input. This approach allows for better separation of development and testing roles in your organization. One common approach to UI testing is to run tests manually and verify that the app is behaving as expected. However, this approach can be time-consuming, tedious, and error-prone. A more efficient and reliable approach is to automate the UI testing with a software testing framework. Automated testing involves creating programs to perform testing tasks (test cases) to cover specific usage scenarios, and then using the testing framework to run the test cases automatically and in a repeatable manner. Overviewhe Android SDK provides the following tools to support automated, functional UI testing on your application: uiautomatorviewer - A GUI tool to scan and analyze the UI components of an Android application. uiautomator - A Java library containing APIs to create customized functional UI tests, and an execution engine to automate and run the tests. To use these tools, you must have the following versions of the Android development tools installed: Android SDK Tools, Revision 21 or higher Android SDK Platform, API 16 or higher Workflow for the the uiautomator testing framework Here’s a short overview of the steps required to automate UI testing: Prepare to test by installing the app on a test device, analyzing the app’s UI components, and ensuring that your application is accessible by the test automation framework. Create automated tests to simulate specific user interactions on your application. Compile your test cases into a JAR file and install it on your test device along with your app. Run the tests and view the test results. Correct any bugs or defects discovered in testing. Analyzing Your Application’s UIBefore you start writing your test cases, it’s helpful to familiarize yourself with the UI components (including the views and controls) of the targeted application. You can use the uiautomatorviewer tool to take a snapshot of the foreground UI screen on any Android device that is connected to your development machine. The uiautomatorviewer tool provides a convenient visual interface to inspect the layout hierarchy and view the properties of the individual UI components that are displayed on the test device. Using this information, you can later create uiautomator tests with selector objects that target specific UI components to test. To analyze the UI components of the application that you want to test: Connect your Android device to your development machine. Open a terminal window and navigate to /tools/. Run the tool with this command: 1$ uiautomatorviewer To capture a screen for analysis, click the Device Screenshot button in the GUI of the uiautomatorviewer tool. Note: If you have more than one device connected, specify the device for screen capture by setting the ANDROID_SERIAL environment variable: a. Find the serial numbers for your connected devices by running this command: 123$ adb devices``` b. Set the ANDROID_SERIAL environment variable to select the device to test: #In Windows: set ANDROID_SERIAL= #In UNIX: export ANDROID_SERIAL= 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657If you are connected to only a single device, you do not need to set the ANDROID_SERIAL environment variable.5. View the UI properties for your application:- Hover over the snapshot in the left-hand panel to see the UI components identified by the uiautomatorviewer tool. You can view the component’s properties listed in the lower right-hand panel, and the layout hierarchy in the upper right-hand panel.- Optionally, click on the Toggle NAF Nodes button to see UI components that are not accessible to the uiautomator testing framework. Only limited information may be available for these components.#### Preparing to TestBefore using the uiautomator testing framework, complete these pre-flight tasks:##### Load the application to a deviceIf you are reading this document, chances are that the Android application that you want to test has not been published yet. If you have a copy of the APK file, you can install the APK onto a test device by using the adb tool. To learn how to install an APK file using the adb tool, see the [adb](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/help/adb.html#move) documentation.##### Identify the application’s UI componentsBefore writing your `uiautomator` tests, first identify the UI components in the application that you want to test. Typically, good candidates for testing are UI components that are visible and that users can interact with. The UI components should also have visible text labels, `android:contentDescription` values, or both.You can inspect the visible screen objects in an application conveniently by using the `uiautomatorviewer` tool. For more information about how to analyze an application screen with this tool, see the section [Analyzing Your Application’s UI](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/testing/testing_ui.html#uianalaysis). For more information about the common types of UI components provided by Android, see [User Interface](https://stuff.mit.edu/afs/sipb/project/android/docs/guide/topics/ui/index.html).##### Ensure that the application is accessibleThis step is required because the `uiautomator` tool depends on the accessibility features of the Android framework to execute your functional UI tests. You should include these minimum optimizations to support the `uiautomator` tool:- Use the `android:contentDescription` attribute to label the `ImageButton`, `ImageView`, `CheckBox` and other user interface controls.- Provide an `android:hint` attribute instead of a content description for `EditText` fields- Associate an `android:hint` attribute with any graphical icons used by controls that provide feedback to the user (for example, status or state information).- Make sure that all the user interface elements are accessible with a directional controller, such as a trackball or D-pad.- Use the `uiautomatorviewer` tool to ensure that the UI component is accessible to the testing framework. You can also test the application by turning on accessibility services like TalkBack and Explore by Touch, and try using your application using only directional controls.For more information about implementing and testing accessibility, see [Making Applications Accessible](https://stuff.mit.edu/afs/sipb/project/android/docs/guide/topics/ui/accessibility/apps.html).&gt; Note: To identify the non-accessible components in the UI, click on the Toggle NAF Nodes option in the `uiautomatorviewer` tool.Generally, Android application developers get accessibility support for free, courtesy of the `View` and `ViewGroup` classes. However, some applications use custom view components to provide a richer user experience. Such custom components won&apos;t get the accessibility support that is provided by the standard Android UI components. If this applies to your application, ensure that the application developer exposes the custom drawn UI components to Android accessibility services, by implementing the `AccessibilityNodeProvider` class. For more information about making custom view components accessible, see [Making Applications Accessible](https://stuff.mit.edu/afs/sipb/project/android/docs/guide/topics/ui/accessibility/apps.html#custom-views).##### Configure your development environmentIf you&apos;re developing in Eclipse, the Android SDK provides additional tools that help you write test cases using `uiautomator` and buiild your JAR file. In order to set up Eclipse to assist you, you need to create a project that includes the `uiautomator` client library, along with the Android SDK library. To configure Eclipse:1. Create a new Java project in Eclipse, and give your project a name that is relevant to the tests you’re about to create (for example, &quot;MyAppNameTests&quot;). In the project, you will create the test cases that are specific to the application that you want to test.2. From the Project Explorer, right-click on the new project that you created, then select Properties &gt; Java Build Path, and do the following: - Click Add Library &gt; JUnit then select JUnit3 to add JUnit support. - Click Add External JARs... and navigate to the SDK directory. Under the platforms directory, select the latest SDK version and add both the uiautomator.jar and android.jar files.If you did not configure Eclipse as your development environment, make sure that the `uiautomator.jar` and `android.jar` files from the `&lt;android-sdk&gt;/platforms/&lt;sdk&gt;` directory are in your Java class path.Once you have completed these prerequisite tasks, you&apos;re almost ready to start creating your `uiautomator` tests.#### Creating uiautomator TestsTo build a test that runs in the `uiautomator` framework, create a test case that extends the `UiAutomatorTestCase` class. In Eclipse, the test case file goes under the `src` directory in your project. Later, you will build the test case as a JAR file, then copy this file to the test device. The test JAR file is not an APK file and resides separately from the application that you want to test on the device.Because the `UiAutomatorTestCase` class extends `junit.framework.TestCase`, you can use the `JUnit` Assert class to test that UI components in the app return the expected results. To learn more about JUnit, you can read the documentation on the `junit.org` home page.The first thing your test case should do is access the device that contains the target app. It’s also good practice to start the test from the Home screen of the device. From the Home screen (or some other starting location you’ve chosen in the target app), you can use the classes provided by the `uiautomator` API to simulate user actions and to test specific UI components. For an example of how to put together a `uiautomator` test case, see the sample test case.##### uiautomator APIThe `uiautomator` API is bundled in the `uiautomator.jar` file under the `&lt;android-sdk&gt;/platforms/` directory. The API includes these key classes that allow you to capture and manipulate UI components on the target app:- [UiDevice](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/help/uiautomator/UiDevice.html)Represents the device state. In your tests, you can call methods on the UiDevice instance to check for the state of various properties, such as current orientation or display size. Your tests also can use the UiDevice instance to perform device level actions, such as forcing the device into a specific rotation, pressing the d-pad hardware button, or pressing the Home and Menu buttons.To get an instance of UiDevice and simulate a Home button press: getUiDevice().pressHome(); 12- [UiSelector](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/help/uiautomator/UiSelector.html)Represents a search criteria to query and get a handle on specific elements in the currently displayed UI. If more than one matching element is found, the first matching element in the layout hierarchy is returned as the target UiObject. When constructing a UiSelector, you can chain together multiple properties to refine your search. If no matching UI element is found, a `UiAutomatorObjectNotFoundException` is thrown. You can use the childSelector() method to nest multiple UiSelector instances. For example, the following code example shows how to specify a search to find the first ListView in the currently displayed UI, then search within that ListView to find a UI element with the text property Apps. UiObject appItem = new UiObject(new UiSelector() .className(“android.widget.ListView”).instance(1) .childSelector(new UiSelector().text(“Apps”))); 1234- [UiObject](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/help/uiautomator/UiObject.html)Represents a UI element. To create a UiObject instance, use a UiSelector that describes how to search for, or select, the UI element.The following code example shows how to construct UiObject instances that represent a Cancel button and a OK button in your application. UiObject cancelButton = new UiObject(new UiSelector().text(“Cancel”)); UiObject okButton = new UiObject(new UiSelector().text(“OK”)); 12You can reuse the UiObject instances that you have created in other parts of your app testing, as needed. Note that the `uiautomator` test framework searches the current display for a match every time your test uses a UiObject instance to click on a UI element or query a property.In the following code example, the `uiautomator` test framework searches for a UI element with the text property OK. If a match is found and if the element is enabled, the framework simulates a user click action on the element. if(okButton.exists() &amp;&amp; okButton.isEnabled()) { okButton.click(); } 1You can also restrict the search to find only elements of a specific class. For example, to find matches of the Button class: UiObject cancelButton = new UiObject(new UiSelector().text(“Cancel”) .className(“android.widget.Button”)); UiObject okButton = new UiObject(new UiSelector().text(“OK”) .className(“android.widget.Button”)); 12- [UiCollection](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/help/uiautomator/UiCollection.html)Represents a collection of items, for example songs in a music album or a list of emails in an inbox. Similar to a UiObject, you construct a UiCollection instance by specifying a UiSelector. The UiSelector for a UiCollection should search for a UI element that is a container or wrapper of other child UI elements (such as a layout view that contains child UI elements). For example, the following code snippet shows how to construct a UiCollection to represent a video album that is displayed within a FrameLayout: UiCollection videos = new UiCollection(new UiSelector() .className(“android.widget.FrameLayout”)); 1If the videos are listed within a LinearLayout view, and you want to to retrieve the number of videos in this collection: int count = videos.getChildCount(new UiSelector() .className(“android.widget.LinearLayout”)); 1If you want to find a specific video that is labeled with the text element Cute Baby Laughing from the collection and simulate a user-click on the video: UiObject video = videos.getChildByText(new UiSelector() .className(“android.widget.LinearLayout”), “Cute Baby Laughing”); video.click(); 12Similarly, you can simulate other user actions on the UI object. For example, if you want to simulate selecting a checkbox that is associated with the video: UiObject checkBox = video.getChild(new UiSelector() .className(“android.widget.Checkbox”)); if(!checkBox.isSelected()) checkbox.click(); 123- [UiScrollable](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/help/uiautomator/UiScrollable.html)Represents a scrollable collection of UI elements. You can use the UiScrollable class to simulate vertical or horizontal scrolling across a display. This technique is helpful when a UI element is positioned off-screen and you need to scroll to bring it into view.For example, the following code shows how to simulate scrolling down the Settings menu and clicking on an About tablet option: UiScrollable settingsItem = new UiScrollable(new UiSelector() .className(“android.widget.ListView”)); UiObject about = settingsItem.getChildByText(new UiSelector() .className(“android.widget.LinearLayout”), “About tablet”); about.click() 1234For more information about these APIs, see the uiautomator reference.##### A sample uiautomator test caseThe following code example shows a simple test case which simulates a user bringing up the Settings app in a stock Android device. The test case mimics all the steps that a user would typically take to perform this task, including opening the Home screen, launching the All Apps screen, scrolling to the Settings app icon, and clicking on the icon to enter the Settings app. package com.uia.example.my; // Import the uiautomator libraries import com.android.uiautomator.core.UiObject; import com.android.uiautomator.core.UiObjectNotFoundException; import com.android.uiautomator.core.UiScrollable; import com.android.uiautomator.core.UiSelector; import com.android.uiautomator.testrunner.UiAutomatorTestCase; public class LaunchSettings extends UiAutomatorTestCase { public void testDemo() throws UiObjectNotFoundException { // Simulate a short press on the HOME button. getUiDevice().pressHome(); // We’re now in the home screen. Next, we want to simulate // a user bringing up the All Apps screen. // If you use the uiautomatorviewer tool to capture a snapshot // of the Home screen, notice that the All Apps button’s // content-description property has the value “Apps”. We can // use this property to create a UiSelector to find the button. UiObject allAppsButton = new UiObject(new UiSelector() .description(&quot;Apps&quot;)); // Simulate a click to bring up the All Apps screen. allAppsButton.clickAndWaitForNewWindow(); // In the All Apps screen, the Settings app is located in // the Apps tab. To simulate the user bringing up the Apps tab, // we create a UiSelector to find a tab with the text // label “Apps”. UiObject appsTab = new UiObject(new UiSelector() .text(&quot;Apps&quot;)); // Simulate a click to enter the Apps tab. appsTab.click(); // Next, in the apps tabs, we can simulate a user swiping until // they come to the Settings app icon. Since the container view // is scrollable, we can use a UiScrollable object. UiScrollable appViews = new UiScrollable(new UiSelector() .scrollable(true)); // Set the swiping mode to horizontal (the default is vertical) appViews.setAsHorizontalList(); // Create a UiSelector to find the Settings app and simulate // a user click to launch the app. UiObject settingsApp = appViews.getChildByText(new UiSelector() .className(android.widget.TextView.class.getName()), &quot;Settings&quot;); settingsApp.clickAndWaitForNewWindow(); // Validate that the package name is the expected one UiObject settingsValidation = new UiObject(new UiSelector() .packageName(&quot;com.android.settings&quot;)); assertTrue(&quot;Unable to detect Settings&quot;, settingsValidation.exists()); }} 1234#### Building and Deploying Your uiautomator Tests1. Once you have coded your test, follow these steps to build and deploy your test JAR to your target Android test device:Create the required build configuration files to build the output JAR. To generate the build configuration files, open a terminal and run the following command: /tools/android create uitest-project -n -t 1 -p 12345The &lt;name&gt; is the name of the project that contains your uiautomator test source files, and the &lt;path&gt; is the path to the corresponding project directory.2. From the command line, set the ANDROID_HOME variable:- In Windows:`set ANDROID_HOME=&lt;path_to_your_sdk&gt;`- In UNIX:`export ANDROID_HOME=&lt;path_to_your_sdk&gt;`3. Go to the project directory where your build.xml file is located and build your test JAR. ant build 14. Deploy your generated test JAR file to the test device by using the adb push command: adb push /data/local/tmp/ 1Here’s an example: adb push ~/dev/workspace/LaunchSettings/bin/LaunchSettings.jar /data/local/tmp/ 123#### Running uiautomator TestsHere’s an example of how to run a test that is implemented in the `LaunchSettings.jar` file. The tests are bundled in the `com.uia.example.my` package: adb shell uiautomator runtest LaunchSettings.jar -c com.uia.example.my.LaunchSettings 123456789101112131415To learn more about the syntax, subcommands, and options for uiautomator, see the [uiautomator](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/help/uiautomator/index.html) reference.#### Best PracticesHere are some best practices for functional UI testing with the uiautomator framework:- Ensure that you validate the same UI functions on your application across the various types of devices that your application might run on (for example, devices with different screen densities).- You should also test your UI against common scenarios such as in-coming phone calls, network interruptions, and user-initiated switching to other applications on the device.### [uiautomator tools](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/help/uiautomator/index.html)The uiautomator testing framework lets you test your user interface (UI) efficiently by creating automated functional UI testcases that can be run against your app on one or more devices.For more information on testing with the uiautomator framework, see [UI Testing](https://stuff.mit.edu/afs/sipb/project/android/docs/tools/testing/testing_ui.html).#### SyntaxTo run your testcases on the target device, you can use the `adb shell` command to invoke the `uiautomator` tool. The syntax is: adb shell uiautomator runtest -c [options] 1Here’s an example: adb shell uiautomator runtest LaunchSettings.jar -c com.uia.example.my.LaunchSettings ``` Command-line OptionsThe following table describes the subcommands and options for uiautomator. Table 1. Command-line options for uiautomator Subcommand Option Description runtest &lt;jar&gt; Required. The argument is the name of one or more JAR files that you deployed to the target device which contain your uiautomator testcases. You can list more than one JAR file by using a space as a separator. -c &lt;test_class_or_method&gt; Required. The argument is a list of one or more specific test classes or test methods from the JARs that you want uiautomator to run.Each class or method must be fully qualified with the package name, in one of these formats: package_name.class_name package_name.class_name#method_name You can list multiple classes or methods by using a space as a separator. –nohup Runs the test to completion on the device even if its parent process is terminated (for example, if the device is disconnected). -e Specify other name-value pairs to be passed to test classes. May be repeated.Note: The -e options cannot be combined; you must prefix each option with a separate -e flag. -e debug [true false] Wait for debugger to connect before starting. dump [file] Generate an XML file with a dump of the current UI hierarchy. If a filepath is not specified, by default, the generated dump file is stored on the device in this location /storage/sdcard0/window_dump.xml. events Prints out accessibility events to the console until the connection to the device is terminated UiAutomation apiClass for interacting with the device’s UI by simulation user actions and introspection of the screen content. It relies on the platform accessibility APIs to introspect the screen and to perform some actions on the remote view tree. It also allows injecting of arbitrary raw input events simulating user interaction with keyboards and touch devices. One can think of a UiAutomation as a special type of AccessibilityService which does not provide hooks for the service life cycle and exposes other APIs that are useful for UI test automation. 这是一个通过模拟用户操作来与设备用户界面交互以及获取屏幕内容的类。它依赖于平台的辅助功能APIs来在远程的控件树上获取屏幕内容以及执行一些操作。同时它也允许通过注入原生事件(译者注:指的就是InputEvent. KeyEvent也是继承于InputEvent的，所以说它是原生事件)来模拟用户的按键和触屏操作。我们可以认为UiAutomation就是一个特殊类型的AccessibilityService,其既不会为控制服务的生命周期而提供钩子函数，也不会暴露任何其他可以直接用于用户界面测试自动化的APIs. The APIs exposed by this class are low-level to maximize flexibility when developing UI test automation tools and libraries. Generally, a UiAutomation client should be using a higher-level library or implement high-level functions. For example, performing a tap on the screen requires construction and injecting of a touch down and up events which have to be delivered to the system by a call to injectInputEvent(InputEvent, boolean). 这个类暴露出来的APIs是很低层的，目的就是为了在开发用户界面测试自动化框架和库时提供最大的弹性。总的来说，一个UiAutomation客户端应该使用一些（基于UiAutomation的)更高层次的库或者实现更高层次的方法。比如，模拟一个用户在屏幕上的点击事件需要构造并注入一个按下和一个弹起事件，然后必须调用UiAutomation的一个injectInputEvent(InputEvent, boolean)的调用来发送给操作系统。 The APIs exposed by this class operate across applications enabling a client to write tests that cover use cases spanning over multiple applications. For example, going to the settings application to change a setting and then interacting with another application whose behavior depends on that setting. 这个类暴露出来的APIs可以跨应用，这样用户就可以编写可以跨越多个应用的测试用例脚本了。比如，打开系统的设置应用去修改一些设置然后再与另外一个依赖于该设置的应用进行交互（译者注：这个在instrumentation这个框架可以做不到的）]]></content>
      <categories>
        <category>autotest</category>
      </categories>
      <tags>
        <tag>android</tag>
        <tag>autotest</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android自动化测试之(N):测试支持库]]></title>
    <url>%2F2017%2F08%2F30%2Fat-android-support-library%2F</url>
    <content type="text"><![CDATA[Android官方文档-测试支持库 Android 测试支持库提供了大量用于测试 Android 应用的框架。此库提供了一组 API，让您可以为应用快速构建何运行测试代码，包括 JUnit 4 和功能性用户界面 (UI) 测试。您可以从 Android Studio IDE 或命令行运行使用这些 API 创建的测试。 Android 测试支持库通过 Android SDK 管理器提供。如需了解详细信息，请参阅测试支持库设置 本页介绍了 Android 测试支持库提供了哪些工具、如何在测试环境中使用这些工具，以及库版本的相关信息。 测试支持库功能Android 测试支持库包括以下自动化测试工具： AndroidJUnitRunner：适用于 Android 且与 JUnit 4 兼容的测试运行器 Espresso：UI 测试框架；适合应用中的功能性 UI 测试 UI Automator：UI 测试框架；适合跨系统和已安装应用的跨应用功能性 UI 测试 AndroidJUnitRunnerAndroidJUnitRunner 类是一个 JUnit 测试运行器，可让您在 Android 设备上运行 JUnit 3 或 JUnit 4 样式测试类，包括使用 Espresso 和 UI Automator 测试框架的设备。测试运行器可以将测试软件包和要测试的应用加载到设备、运行测试并报告测试结果。此类将替换 InstrumentationTestRunner 类，后者仅支持 JUnit 3 测试。 此测试运行器的主要功能包括： JUnit 支持 访问仪器信息 测试筛选 测试分片 要求 Android 2.2（API 级别 8）或更高版本。 JUnit 支持测试运行器与 JUnit 3 和 JUnit 4（最高版本为 JUnit 4.10）测试兼容。不过，请勿在同一软件包中混用 JUnit 3 和 JUnit 4 测试代码，因为这可能会导致意外结果。如果要创建一个 JUnit 4 仪器测试类以在设备或模拟器上运行，则测试类必须以 @RunWith(AndroidJUnit4.class) 注解作为前缀。 以下代码段显示了如何编写 JUnit 4 仪器测试来验证 CalculatorActivity 类中的 add 操作是否正常工作。 1234567891011121314151617181920212223242526272829import android.support.test.runner.AndroidJUnit4;import android.support.test.runner.AndroidJUnitRunner;import android.test.ActivityInstrumentationTestCase2;@RunWith(AndroidJUnit4.class)public class CalculatorInstrumentationTest extends ActivityInstrumentationTestCase2&lt;CalculatorActivity&gt; &#123; @Before public void setUp() throws Exception &#123; super.setUp(); // Injecting the Instrumentation instance is required // for your test to run with AndroidJUnitRunner. injectInstrumentation(InstrumentationRegistry.getInstrumentation()); mActivity = getActivity(); &#125; @Test public void typeOperandsAndPerformAddOperation() &#123; // Call the CalculatorActivity add() method and pass in some operand values, then // check that the expected value is returned. &#125; @After public void tearDown() throws Exception &#123; super.tearDown(); &#125;&#125; 访问仪器信息您可以使用 InstrumentationRegistry 类访问与测试运行相关的信息。此类包括 Instrumentation 对象、目标应用 Context 对象、测试应用 Context 对象，以及传递到测试中的命令行参数。使用 UI Automator 框架编写测试或编写依赖于 Instrumentation 或 Context 对象的测试时，此数据非常有用。 测试筛选在 JUnit 4.x 测试中，您可以使用注解对测试运行进行配置。此功能可将向测试中添加样板文件和条件代码的需求降至最低。除了 JUnit 4 支持的标准注解外，测试运行器还支持 Android 特定的注解，包括： @RequiresDevice：指定测试仅在物理设备而不在模拟器上运行。 @SdkSupress：禁止在低于给定级别的 Android API 级别上运行测试。例如，要禁止在低于 18 的所有 API 级别上运行测试，请使用注解 - - @SDKSupress(minSdkVersion=18)。 @SmallTest、@MediumTest 和 @LargeTest：指定测试的运行时长以及运行频率。测试分片测试运行器支持将单一测试套件拆分成多个碎片，因此您可以将属于同一碎片的测试作为一个组在同一 Instrumentation 实例下运行。每个分片由一个索引号进行标识。运行测试时，使用 -e numShards 选项指定要创建的独立分片数量，并使用 -e shardIndex 选项指定要运行哪个分片。 例如，要将测试套件拆分成 10 个分片，且仅运行第二个碎片中的测试，请使用以下命令： adb shell am instrument -w -e numShards 10 -e shardIndex 2 要详细了解如何使用此测试运行器，请参阅 API 参考。 EspressoEspresso 测试框架提供了一组 API 来构建 UI 测试，用于测试应用中的用户流。利用这些 API，您可以编写简洁、运行可靠的自动化 UI 测试。Espresso 非常适合编写白盒自动化测试，其中测试代码将利用所测试应用的实现代码详情。 Espresso 测试框架的主要功能包括： 灵活的 API，用于目标应用中的视图和适配器匹配。如需了解详细信息，请参阅视图匹配。 一组丰富的操作 API，用于自动化 UI 交互。如需了解详细信息，请参阅操作 API。 UI 线程同步，用于提升测试可靠性。如需了解详细信息，请参阅 UI 线程同步。 要求 Android 2.2（API 级别 8）或更高版本。 视图匹配利用 Espresso.onView() 方法，您可以访问目标应用中的 UI 组件并与之交互。此方法接受 Matcher 参数并搜索视图层次结构，以找到符合给定条件的相应 View 实例。您可以通过指定以下条件来优化搜索： 视图的类名称 视图的内容描述 视图的 R.id 在视图中显示的文本 例如，要找到 ID 值为 my_button 的按钮，可以指定如下匹配器：1onView(withId(R.id.my_button)); 如果搜索成功，onView() 方法将返回一个引用，让您可以执行用户操作并基于目标视图对断言进行测试。 适配器匹配在 AdapterView 布局中，布局在运行时由子视图动态填充。如果目标视图位于某个布局内部，而该布局是从 AdapterView（例如 ListView 或 GridView）派生出的子类，则 onView() 方法可能无法工作，因为只有布局视图的子集会加载到当前视图层次结构中。 因此，请使用 Espresso.onData() 方法访问目标视图元素。Espresso.onData() 方法将返回一个引用，让您可以执行用户操作并根据 AdapterView 中的元素对断言进行测试。 操作 API通常情况下，您可以通过根据应用的用户界面执行某些用户交互来测试应用。借助 ViewActions API，您可以轻松地实现这些操作的自动化。您可以执行多种 UI 交互，例如： 视图点击 滑动 按下按键和按钮 键入文本 打开链接 例如，要模拟输入字符串值并按下按钮以提交该值，您可以像下面一样编写自动化测试脚本。ViewInteraction.perform() 和 DataInteraction.perform() 方法采用一个或多个 ViewAction 参数，并以提供的顺序运行操作。123456// Type text into an EditText view, then close the soft keyboardonView(withId(R.id.editTextUserInput)) .perform(typeText(STRING_TO_BE_TYPED), closeSoftKeyboard());// Press the button to submit the text changeonView(withId(R.id.changeTextBt)).perform(click()); UI 线程同步由于计时问题，Android 设备上的测试可能随机失败。此测试问题称为测试不稳定。在 Espresso 之前，解决方法是在测试中插入足够长的休眠或超时期或添加代码，以便重试失败的操作。Espresso 测试框架可以处理 Instrumentation 与 UI 线程之间的同步；这就消除了对之前的计时解决方法的需求，并确保测试操作与断言更可靠地运行。 要详细了解如何使用 Espresso，请参阅 API 参考和测试单个应用的 UI 培训。 UI AutomatorUI Automator 测试框架提供了一组 API 来构建 UI 测试，用于在用户应用和系统应用中执行交互。利用 UI Automator API，您可以执行在测试设备中打开“设置”菜单或应用启动器等操作。UI Automator 测试框架非常适合编写黑盒自动化测试，其中的测试代码不依赖于目标应用的内部实现详情。 UI Automator 测试框架的主要功能包括： 用于检查布局层次结构的查看器。如需了解详细信息，请参阅 UI Automator 查看器。 在目标设备上检索状态信息并执行操作的 API。如需了解详细信息，请参阅访问设备状态。 支持跨应用 UI 测试的 API。如需了解详细信息，请参阅 UI Automator API。 要求 Android 4.3（API 级别 18）或更高版本。 UI Automator 查看器uiautomatorviewer 工具提供了一个方便的 GUI，可以扫描和分析 Android 设备上当前显示的 UI 组件。您可以使用此工具检查布局层次结构，并查看在设备前台显示的 UI 组件属性。利用此信息，您可以使用 UI Automator（例如，通过创建与特定可见属性匹配的 UI 选择器）创建控制更加精确的测试。 uiautomatorviewer 工具位于 &lt;android-sdk&gt;/tools/目录中。 访问设备状态UI Automator 测试框架提供了一个 UiDevice 类，用于在目标应用运行的设备上访问和执行操作。您可以调用其方法来访问设备属性，如当前屏幕方向或显示尺寸。UiDevice 类还可用于执行以下操作： 更改设备旋转 按 D-pad 按钮 按“返回”、“主屏幕”或“菜单”按钮 打开通知栏 对当前窗口进行屏幕截图 例如，要模拟按下“主屏幕”按钮，请调用 UiDevice.pressHome() 方法。 UI Automator API利用 UI Automator API，您可以编写稳健可靠的测试，而无需了解目标应用的实现详情。您可以使用这些 API 在多个应用中捕获和操作 UI 组件： UiCollection：枚举容器的 UI 元素以便计算子元素个数，或者通过可见的文本或内容描述属性来指代子元素。 UiObject：表示设备上可见的 UI 元素。 UiScrollable：为在可滚动 UI 容器中搜索项目提供支持。 UiSelector：表示在设备上查询一个或多个目标 UI 元素。 Configurator：允许您设置运行 UI Automator 测试所需的关键参数。 例如，以下代码显示了如何编写可在设备中调用默认应用启动器的测试脚本：12345678910111213// Initialize UiDevice instancemDevice = UiDevice.getInstance(getInstrumentation());// Perform a short press on the HOME buttonmDevice.pressHome();// Bring up the default launcher by searching for// a UI component that matches the content-description for the launcher buttonUiObject allAppsButton = mDevice .findObject(new UiSelector().description(&quot;Apps&quot;));// Perform a click on the button to bring up the launcherallAppsButton.clickAndWaitForNewWindow(); 要详细了解如何使用 UI Automator，请参阅 API 参考和测试多个应用的 UI 培训。 测试支持库设置Android 测试支持库软件包在最新版本的 Android 支持存储库中提供，后者可作为辅助组件通过 Android SDK 管理器下载。 要通过 SDK 管理器下载 Android 支持存储库，请执行以下操作： 启动 Android SDK 管理器。 在 SDK 管理器窗口中，滚动到 Packages 列表末尾，找到 Extras 文件夹并展开（如有必要）以显示其内容。 选择 Android Support Repository 项。 点击 Install packages… 按钮。 下载后，此工具会将支持存储库文件安装到您现有的 Android SDK 目录中。库文件位于 SDK 的以下子目录中：&lt;sdk&gt;/extras/android/m2repository 目录。 Android 测试支持库的类位于 android.support.test 软件包中。 要在 Gradle 项目中使用 Android 测试支持库，请在 build.gradle 文件中添加这些依赖关系： 123456789dependencies &#123; androidTestCompile &apos;com.android.support.test:runner:0.4&apos; // Set this dependency to use JUnit 4 rules androidTestCompile &apos;com.android.support.test:rules:0.4&apos; // Set this dependency to build and run Espresso tests androidTestCompile &apos;com.android.support.test.espresso:espresso-core:2.2.1&apos; // Set this dependency to build and run UI Automator tests androidTestCompile &apos;com.android.support.test.uiautomator:uiautomator-v18:2.1.2&apos;&#125; 要将 AndroidJUnitRunner 设置为 Gradle 项目中的默认测试仪器运行器，请在 build.gradle 文件中指定此依赖关系： `` android { defaultConfig { testInstrumentationRunner “android.support.test.runner.AndroidJUnitRunner” } } ```]]></content>
      <categories>
        <category>autotest</category>
      </categories>
      <tags>
        <tag>android</tag>
        <tag>autotest</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android自动化测试之(N):adb工具]]></title>
    <url>%2F2017%2F08%2F29%2Fat-android-adb%2F</url>
    <content type="text"><![CDATA[Android手机自动化测试过程离不开adb工具,介绍几个常用的adb命令. 1. adb forward命令示例: 1adb forward tcp:8000 tcp:9000 作用: 把PC端8000端口的数据, 转发到Android端的9000端口上,PC端的8000端口会被 adb 监听, 这个时候我们只需要往8000端口写数据, 这个数据就会发送到手机端的9000端口上. 2. adb connect命令示例: 1adb connect + IP 作用: 通过无线网络在PC端adb连接手机端 注意: 要链接的IP ，必须和自己的PC的网络在同一个局域网内，adb 不能跨局域网链接设备 如果通过usb链接Android设备，通过adb devices 可以看见设备列表，但是使用不了，可以参考下面的命令说明手机端的服务未开启,需要连接usb开启手机服务,默认端口5555:adb tcpip 5555,开启后拔掉usb通过adb connect 192.168.0.101:5555即可连接]]></content>
      <categories>
        <category>autotest</category>
      </categories>
      <tags>
        <tag>android</tag>
        <tag>autotest</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RecyclerView实现单选列表]]></title>
    <url>%2F2017%2F08%2F24%2Ftips-recyclerview-selectable%2F</url>
    <content type="text"><![CDATA[常规方法： 在Javabean里增加一个boolean isSelected字段， 并在Adapter里根据这个字段的值设置“CheckBox”的选中状态。 在每次选中一个新优惠券时，改变数据源里的isSelected字段， 并notifyDataSetChanged()刷新整个列表。 这样实现起来很简单，代码量也很少，唯一不足的地方就是性能有损耗，不是最优雅。 So作为一个有追求 今天比较闲 的程序员，我决心分享一波优雅方案。 本文会列举分析一下在ListView和RecyclerView中, 列表实现单选的几种方案，并推荐采用定向刷新 部分绑定的方案，因为更高效and优雅 1常规方案:常规方案 请光速阅读，直接上码： Bean结构： 1234567public class TestBean extends SelectedBean &#123; private String name; public TestBean(String name,boolean isSelected) &#123; this.name = name; setSelected(isSelected); &#125;&#125; 我项目里有好多单选需求，懒得写isSelected字段，所以弄了个父类供子类继承。 123456789public class SelectedBean &#123; private boolean isSelected; public boolean isSelected() &#123; return isSelected; &#125; public void setSelected(boolean selected) &#123; isSelected = selected; &#125;&#125; Acitivity 和Adapter其他方法都是最普通的不再赘述。 Adapter的onBindViewHolder()如下： 1234567891011121314151617181920212223242526272829Log.d(&quot;TAG&quot;, &quot;onBindViewHolder() called with: holder = [&quot; + holder + &quot;], position = [&quot; + position + &quot;]&quot;); holder.ivSelect.setSelected(mDatas.get(position).isSelected());//“CheckBox” holder.tvCoupon.setText(mDatas.get(position).getName());//TextView holder.ivSelect.setOnClickListener(new View.OnClickListener() &#123; @Override public void onClick(View view) &#123; //实现单选，第一种方法，十分简单， Lv Rv通用,因为它们都有notifyDataSetChanged()方法 // 每次点击时，先将所有的selected设为false，并且将当前点击的item 设为true， 刷新整个视图 for (TestBean data : mDatas) &#123; data.setSelected(false); &#125; mDatas.get(position).setSelected(true); notifyDataSetChanged(); &#125; &#125;);ViewHolder： public static class CouponVH extends RecyclerView.ViewHolder &#123; private ImageView ivSelect; private TextView tvCoupon; public CouponVH(View itemView) &#123; super(itemView); ivSelect = (ImageView) itemView.findViewById(R.id.ivSelect); tvCoupon = (TextView) itemView.findViewById(R.id.tvCoupon); &#125; &#125; 方案优点：简单粗暴 方案缺点： 其实需要修改的Item只有两项： 一个当前处于选中状态的Item-&gt;普通状态 再将当前手指点击的这个Item-&gt;选中状态 但采用普通方案，则会刷新整个一屏可见的Item，重走他们的getView()/onBindViewHolder()方法。 其实一个屏幕一般最多可见10+个Item，遍历一遍也无伤大雅。 但咱们还是要有追求优雅的心，所以我们继续往下看。 2 利用Rv的notifyItemChanged()定向刷新:本方案可以中速阅读 本方案需要在Adapter里新增一个字段： 1private int mSelectedPos = -1;//实现单选 方法二，变量保存当前选中的position 在设置数据集时(构造函数，setData()方法等：)，初始化 mSelectedPos 的值。 123456//实现单选方法二： 设置数据集时，找到默认选中的posfor (int i = 0; i &lt; mDatas.size(); i++) &#123; if (mDatas.get(i).isSelected()) &#123; mSelectedPos = i; &#125;&#125; onClick里代码如下： 1234567891011//实现单选方法二： notifyItemChanged() 定向刷新两个视图//如果勾选的不是已经勾选状态的Itemif (mSelectedPos!=position)&#123; //先取消上个item的勾选状态 mDatas.get(mSelectedPos).setSelected(false); notifyItemChanged(mSelectedPos); //设置新Item的勾选状态 mSelectedPos = position; mDatas.get(mSelectedPos).setSelected(true); notifyItemChanged(mSelectedPos);&#125; 本方案由于调用了notifyItemChanged()，所以还会伴有“白光一闪”的动画。 方案优点： 本方案，较优雅了，不会重走一屏可见的Item的getView()/onBindViewHolder()方法， 但仍然会重走需要修改的两个Item的getView()/onBindViewHolder()方法， 方案缺点： 我们实际上需要修改的，只是里面“CheckBox”的值， 按照在DiffUtil一文学习到的姿势，术语应该是“Partial bind “， （安利时间,没听过DiffUtil和Partial bind的 戳-&gt;：【Android】详解7.0带来的新工具类：DiffUtil） 我们需要的只是部分绑定。 一个疑点： 使用方法2 在第一次选中其他Item时，切换selected状态时， 查看log，并不是只重走了新旧Item的onBindViewHolder()方法，还走了两个根本不在屏幕范围里的Item的onBindViewHolder()方法， 如，本例中 在还有item 0-3 在屏幕里，默认勾选item1，我选中item0后，log显示postion 4,5,0,1 依次执行了onBindViewHolder()方法。 但是再次切换其他Item时， 会符合预期：只走需要修改的两个Item的getView()/onBindViewHolder()方法。 原因未知，有朋友知道烦请告知，多谢。 3 Rv 实现部分绑定（推荐）:利用RecyclerView的 findViewHolderForLayoutPosition()方法，获取某个postion的ViewHolder，按照源码里这个方法的注释，它可能返回null。所以我们需要注意判空，（空即在屏幕不可见）。 与方法2只有onClick里的代码不一样，核心还是利用mSelectedPos 字段搞事情。 1234567891011121314//实现单选方法三： RecyclerView另一种定向刷新方法：不会有白光一闪动画 也不会重复onBindVIewHolderCouponVH couponVH = (CouponVH) mRv.findViewHolderForLayoutPosition(mSelectedPos);if (couponVH != null) &#123;//还在屏幕里 couponVH.ivSelect.setSelected(false);&#125;else &#123; //add by 2016 11 22 for 一些极端情况，holder被缓存在Recycler的cacheView里， //此时拿不到ViewHolder，但是也不会回调onBindViewHolder方法。所以add一个异常处理 notifyItemChanged(mSelectedPos);&#125;mDatas.get(mSelectedPos).setSelected(false);//不管在不在屏幕里 都需要改变数据//设置新Item的勾选状态mSelectedPos = position;mDatas.get(mSelectedPos).setSelected(true);holder.ivSelect.setSelected(true); 方案优点： 定向刷新两个Item，只修改必要的部分，不会重走onBindViewHolder()，属于手动部分绑定。代码量也适中，不多。 方案缺点： 没有白光一闪动画？？？（如果这算缺点） 4 Rv 利用payloads实现部分绑定(不推荐):本方案属于开拓思维，是在方案2的基础上，利用payloads和notifyItemChanged(int position, Object payload)搞事情。 不知道payloads是什么的，看不懂此方案的，我又要安利：（戳-&gt;：【Android】详解7.0带来的新工具类：DiffUtil） onClick代码如下： 123456789101112131415//实现单选方法四：if (mSelectedPos != position) &#123; //先取消上个item的勾选状态 mDatas.get(mSelectedPos).setSelected(false); //传递一个payload Bundle payloadOld = new Bundle(); payloadOld.putBoolean(&quot;KEY_BOOLEAN&quot;, false); notifyItemChanged(mSelectedPos, payloadOld); //设置新Item的勾选状态 mSelectedPos = position; mDatas.get(mSelectedPos).setSelected(true); Bundle payloadNew = new Bundle(); payloadNew.putBoolean(&quot;KEY_BOOLEAN&quot;, true); notifyItemChanged(mSelectedPos, payloadNew);&#125; 需要重写三参数的onBindViewHolder() 方法： 123456789101112@Overridepublic void onBindViewHolder(CouponVH holder, int position, List&lt;Object&gt; payloads) &#123; if (payloads.isEmpty()) &#123; onBindViewHolder(holder, position); &#125; else &#123; Bundle payload = (Bundle) payloads.get(0); if (payload.containsKey(&quot;KEY_BOOLEAN&quot;)) &#123; boolean aBoolean = payload.getBoolean(&quot;KEY_BOOLEAN&quot;); holder.ivSelect.setSelected(aBoolean); &#125; &#125;&#125; 方案优点： 同方法3 方案缺点： 代码量多，实现效果和方法三一样，仅做开拓思维用，所以选择方法三。 作者：张旭童 链接：http://www.jianshu.com/p/1ac13f74da63 來源：简书]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Activity任务和返回栈]]></title>
    <url>%2F2017%2F08%2F24%2Ftips-activity-stack%2F</url>
    <content type="text"><![CDATA[问题:实例化launcher activity 这个问题表现: 在package installers 安装界面安装完一个应用后，直接打开app，然后进入了 Activity_1, 此时再通过此activity用startActivity(intent)的方法打开 Activity_2. 然后按home键返回桌面，在桌面点击app图标进入，你觉得应该进入的是 Activity_2 ，实际上却是launcher Activity_1 . 然而还没完，这时候你按 back 返回键，会发现返回到了之前打开的 Activity_2，再按返回，又出现 launcherActivity_1. 也就是说系统重复实例化了Activity_1. 退出app后再次点击桌面图标进入，反复试验，没有再出现这个问题。也就是说，这个问题（bug ？）只出现在操作步骤（1）后才会产生. 以上问题我在一些知名厂商的app 上发现也存在这个BUG ： 百度云 陌陌 去哪儿旅行 …QQ没有出现这个问题 另外，如果以root方式静默安装的话不会出现这个问题，在eclipse里直接发布到模拟器上运行也没有出现这个问题 解决方案: 在super.onCreate(…)方法之后插入代码： 123456789if(!this.isTaskRoot()) &#123; //判断该Activity是不是任务空间的源Activity，“非”也就是说是被系统重新实例化出来 //如果你就放在launcher Activity中话，这里可以直接return了 Intent mainIntent=getIntent(); String action=mainIntent.getAction(); if(mainIntent.hasCategory(Intent.CATEGORY_LAUNCHER) &amp;&amp; action.equals(Intent.ACTION_MAIN)) &#123; finish(); return;//finish()之后该活动会继续执行后面的代码，你可以logCat验证，加return避免可能的exception &#125; &#125; 来源google ： https://code.google.com/p/android/issues/detail?id=14262 https://code.google.com/p/android/issues/detail?id=2373#c40 还有另一种方案： http://stackoverflow.com/questions/3042420/home-key-press-behaviour/4782423#4782423 以下引用自google官方文档 应用通常包含多个 Activity。每个 Activity 均应围绕用户可以执行的特定操作设计，并且能够启动其他 Activity。 例如，电子邮件应用可能有一个 Activity 显示新邮件的列表。用户选择某邮件时，会打开一个新 Activity 以查看该邮件。 一个 Activity 甚至可以启动设备上其他应用中存在的 Activity。例如，如果应用想要发送电子邮件，则可将 Intent 定义为执行“发送”操作并加入一些数据，如电子邮件地址和电子邮件。 然后，系统将打开其他应用中声明自己处理此类 Intent 的 Activity。在这种情况下，Intent 是要发送电子邮件，因此将启动电子邮件应用的“撰写”Activity（如果多个 Activity 支持相同 Intent，则系统会让用户选择要使用的 Activity）。发送电子邮件时，Activity 将恢复，看起来好像电子邮件 Activity 是您的应用的一部分。 即使这两个 Activity 可能来自不同的应用，但是 Android 仍会将 Activity 保留在相同的任务中，以维护这种无缝的用户体验。 任务是指在执行特定作业时与用户交互的一系列 Activity。 这些 Activity 按照各自的打开顺序排列在堆栈（即返回栈）中。 设备主屏幕是大多数任务的起点。当用户触摸应用启动器中的图标（或主屏幕上的快捷方式）时，该应用的任务将出现在前台。 如果应用不存在任务（应用最近未曾使用），则会创建一个新任务，并且该应用的“主”Activity 将作为堆栈中的根 Activity 打开。 当前 Activity 启动另一个 Activity 时，该新 Activity 会被推送到堆栈顶部，成为焦点所在。 前一个 Activity 仍保留在堆栈中，但是处于停止状态。Activity 停止时，系统会保持其用户界面的当前状态。 用户按“返回”按钮时，当前 Activity 会从堆栈顶部弹出（Activity 被销毁），而前一个 Activity 恢复执行（恢复其 UI 的前一状态）。 堆栈中的 Activity 永远不会重新排列，仅推入和弹出堆栈：由当前 Activity 启动时推入堆栈；用户使用“返回”按钮退出时弹出堆栈。 因此，返回栈以“后进先出”对象结构运行。 图 1 通过时间线显示 Activity 之间的进度以及每个时间点的当前返回栈，直观呈现了这种行为。 图 1. 显示任务中的每个新 Activity 如何向返回栈添加项目。 用户按“返回”按钮时，当前 Activity 随即被销毁，而前一个 Activity 恢复执行。 如果用户继续按“返回”，堆栈中的相应 Activity 就会弹出，以显示前一个 Activity，直到用户返回主屏幕为止（或者，返回任务开始时正在运行的任意 Activity）。 当所有 Activity 均从堆栈中移除后，任务即不复存在。 任务是一个有机整体，当用户开始新任务或通过“主页”按钮转到主屏幕时，可以移动到“后台”。 尽管在后台时，该任务中的所有 Activity 全部停止，但是任务的返回栈仍旧不变，也就是说，当另一个任务发生时，该任务仅仅失去焦点而已，如图 2 中所示。然后，任务可以返回到“前台”，用户就能够回到离开时的状态。 例如，假设当前任务（任务 A）的堆栈中有三个 Activity，即当前 Activity 下方还有两个 Activity。 用户先按“主页”按钮，然后从应用启动器启动新应用。 显示主屏幕时，任务 A 进入后台。新应用启动时，系统会使用自己的 Activity 堆栈为该应用启动一个任务（任务 B）。与该应用交互之后，用户再次返回主屏幕并选择最初启动任务 A 的应用。现在，任务 A 出现在前台，其堆栈中的所有三个 Activity 保持不变，而位于堆栈顶部的 Activity 则会恢复执行。 此时，用户还可以通过转到主屏幕并选择启动该任务的应用图标（或者，通过从概览屏幕选择该应用的任务）切换回任务 B。这是 Android 系统中的一个多任务示例。 图 2. 两个任务：任务 B 在前台接收用户交互，而任务 A 则在后台等待恢复。 注：后台可以同时运行多个任务。但是，如果用户同时运行多个后台任务，则系统可能会开始销毁后台 Activity，以回收内存资源，从而导致 Activity 状态丢失。请参阅下面有关 Activity 状态的部分。 由于返回栈中的 Activity 永远不会重新排列，因此如果应用允许用户从多个 Activity 中启动特定 Activity，则会创建该 Activity 的新实例并推入堆栈中（而不是将 Activity 的任一先前实例置于顶部）。 因此，应用中的一个 Activity 可能会多次实例化（即使 Activity 来自不同的任务），如图 3 所示。因此，如果用户使用“返回”按钮向后导航，则会按 Activity 每个实例的打开顺序显示这些实例（每个实例的 UI 状态各不相同）。 但是，如果您不希望 Activity 多次实例化，则可修改此行为。 具体操作方法将在后面的管理任务部分中讨论。 图 3. 一个 Activity 将多次实例化。 Activity 和任务的默认行为总结如下： 当 Activity A 启动 Activity B 时，Activity A 将会停止，但系统会保留其状态（例如，滚动位置和已输入表单中的文本）。如果用户在处于 Activity B 时按“返回”按钮，则 Activity A 将恢复其状态，继续执行。 用户通过按“主页”按钮离开任务时，当前 Activity 将停止且其任务会进入后台。 系统将保留任务中每个 Activity 的状态。如果用户稍后通过选择开始任务的启动器图标来恢复任务，则任务将出现在前台并恢复执行堆栈顶部的 Activity。 如果用户按“返回”按钮，则当前 Activity 会从堆栈弹出并被销毁。 堆栈中的前一个 Activity 恢复执行。销毁 Activity 时，系统不会保留该 Activity 的状态。 即使来自其他任务，Activity 也可以多次实例化。 保存 Activity 状态正如上文所述，当 Activity 停止时，系统的默认行为会保留其状态。 这样一来，当用户导航回到上一个 Activity 时，其用户界面与用户离开时一样。 但是，在 Activity 被销毁且必须重建时，您可以而且应当主动使用回调方法保留 Activity 的状态。 系统停止您的一个 Activity 时（例如，新 Activity 启动或任务转到前台），如果系统需要回收系统内存资源，则可能会完全销毁该 Activity。 发生这种情况时，有关该 Activity 状态的信息将会丢失。如果发生这种情况，系统仍会知道该 Activity 存在于返回栈中，但是当该 Activity 被置于堆栈顶部时，系统一定会重建 Activity（而不是恢复 Activity）。 为了避免用户的工作丢失，您应主动通过在 Activity 中实现 onSaveInstanceState() 回调方法来保留工作。 如需了解有关如何保存 Activity 状态的详细信息，请参阅 Activity 文档。 管理任务Android 管理任务和返回栈的方式（如上所述，即：将所有连续启动的 Activity 放入同一任务和“后进先出”堆栈中）非常适用于大多数应用，而您不必担心 Activity 如何与任务关联或者如何存在于返回栈中。 但是，您可能会决定要中断正常行为。 也许您希望应用中的 Activity 在启动时开始新任务（而不是放置在当前任务中）；或者，当启动 Activity 时，您希望将其现有实例上移一层（而不是在返回栈的顶部创建新实例）；或者，您希望在用户离开任务时，清除返回栈中除根 Activity 以外的所有其他 Activity。 通过使用 清单文件元素中的属性和传递给 startActivity() 的 Intent 中的标志，您可以执行所有这些操作以及其他操作。 在这一方面，您可以使用的主要 属性包括： taskAffinity launchMode allowTaskReparenting clearTaskOnLaunch alwaysRetainTaskState finishOnTaskLaunch 您可以使用的主要 Intent 标志包括： FLAG_ACTIVITY_NEW_TASK FLAG_ACTIVITY_CLEAR_TOP FLAG_ACTIVITY_SINGLE_TOP 在下文中，您将了解如何使用这些清单文件属性和 Intent 标志定义 Activity 与任务的关联方式，以及 Activity 在返回栈中的行为方式。 此外，我们还单独介绍了有关如何在概览屏幕中显示和管理任务与 Activity 的注意事项。 如需了解详细信息，请参阅概览屏幕。 通常，您应该允许系统定义任务和 Activity 在概览屏幕中的显示方法，并且无需修改此行为。 注意：大多数应用都不得中断 Activity 和任务的默认行为： 如果确定您的 Activity 必须修改默认行为，当使用“返回”按钮从其他 Activity 和任务导航回到该 Activity 时，请务必要谨慎并确保在启动期间测试该 Activity 的可用性。请确保测试导航行为是否有可能与用户的预期行为冲突。 定义启动模式启动模式允许您定义 Activity 的新实例如何与当前任务关联。 您可以通过两种方法定义不同的启动模式： 使用清单文件 在清单文件中声明 Activity 时，您可以指定 Activity 在启动时应该如何与任务关联。 使用 Intent 标志 调用 startActivity() 时，可以在 Intent 中加入一个标志，用于声明新 Activity 如何（或是否）与当前任务关联。 因此，如果 Activity A 启动 Activity B，则 Activity B 可以在其清单文件中定义它应该如何与当前任务关联（如果可能），并且 Activity A 还可以请求 Activity B 应该如何与当前任务关联。如果这两个 Activity 均定义 Activity B 应该如何与任务关联，则 Activity A 的请求（如 Intent 中所定义）优先级要高于 Activity B 的请求（如其清单文件中所定义）。 注：某些适用于清单文件的启动模式不可用作 Intent 标志，同样，某些可用作 Intent 标志的启动模式无法在清单文件中定义。 使用清单文件在清单文件中声明 Activity 时，您可以使用 元素的 launchMode 属性指定 Activity 应该如何与任务关联。 launchMode 属性指定有关应如何将 Activity 启动到任务中的指令。您可以分配给 launchMode 属性的启动模式共有四种： “standard”（默认模式） 默认。系统在启动 Activity 的任务中创建 Activity 的新实例并向其传送 Intent。Activity 可以多次实例化，而每个实例均可属于不同的任务，并且一个任务可以拥有多个实例。 “singleTop” 如果当前任务的顶部已存在 Activity 的一个实例，则系统会通过调用该实例的 onNewIntent() 方法向其传送 Intent，而不是创建 Activity 的新实例。Activity 可以多次实例化，而每个实例均可属于不同的任务，并且一个任务可以拥有多个实例（但前提是位于返回栈顶部的 Activity 并不是 Activity 的现有实例）。 例如，假设任务的返回栈包含根 Activity A 以及 Activity B、C 和位于顶部的 D（堆栈是 A-B-C-D；D 位于顶部）。收到针对 D 类 Activity 的 Intent。如果 D 具有默认的 “standard” 启动模式，则会启动该类的新实例，且堆栈会变成 A-B-C-D-D。但是，如果 D 的启动模式是 “singleTop”，则 D 的现有实例会通过 onNewIntent() 接收 Intent，因为它位于堆栈的顶部；而堆栈仍为 A-B-C-D。但是，如果收到针对 B 类 Activity 的 Intent，则会向堆栈添加 B 的新实例，即便其启动模式为 “singleTop” 也是如此。 注：为某个 Activity 创建新实例时，用户可以按“返回”按钮返回到前一个 Activity。 但是，当 Activity 的现有实例处理新 Intent 时，则在新 Intent 到达 onNewIntent() 之前，用户无法按“返回”按钮返回到 Activity 的状态。 “singleTask” 系统创建新任务并实例化位于新任务底部的 Activity。但是，如果该 Activity 的一个实例已存在于一个单独的任务中，则系统会通过调用现有实例的 onNewIntent() 方法向其传送 Intent，而不是创建新实例。一次只能存在 Activity 的一个实例。 注：尽管 Activity 在新任务中启动，但是用户按“返回”按钮仍会返回到前一个 Activity。 “singleInstance”. 与 “singleTask” 相同，只是系统不会将任何其他 Activity 启动到包含实例的任务中。该 Activity 始终是其任务唯一仅有的成员；由此 Activity 启动的任何 Activity 均在单独的任务中打开。 我们再来看另一示例，Android 浏览器应用声明网络浏览器 Activity 应始终在其自己的任务中打开（通过在 元素中指定 singleTask 启动模式）。这意味着，如果您的应用发出打开 Android 浏览器的 Intent，则其 Activity 与您的应用位于不同的任务中。相反，系统会为浏览器启动新任务，或者如果浏览器已有任务正在后台运行，则会将该任务上移一层以处理新 Intent。 无论 Activity 是在新任务中启动，还是在与启动 Activity 相同的任务中启动，用户按“返回”按钮始终会转到前一个 Activity。 但是，如果启动指定 singleTask 启动模式的 Activity，则当某后台任务中存在该 Activity 的实例时，整个任务都会转移到前台。此时，返回栈包括上移到堆栈顶部的任务中的所有 Activity。 图 4 显示了这种情况。 图 4. 显示如何将启动模式为“singleTask”的 Activity 添加到返回栈。 如果 Activity 已经是某个拥有自己的返回栈的后台任务的一部分，则整个返回栈也会上移到当前任务的顶部。 如需了解有关在清单文件中使用启动模式的详细信息，请参阅 元素文档，其中更详细地讨论了 launchMode 属性和可接受的值。 注：使用 launchMode 属性为 Activity 指定的行为可由 Intent 附带的 Activity 启动标志替代，下文将对此进行讨论。 使用 Intent 标志启动 Activity 时，您可以通过在传递给 startActivity() 的 Intent 中加入相应的标志，修改 Activity 与其任务的默认关联方式。可用于修改默认行为的标志包括： FLAG_ACTIVITY_NEW_TASK 在新任务中启动 Activity。如果已为正在启动的 Activity 运行任务，则该任务会转到前台并恢复其最后状态，同时 Activity 会在 onNewIntent() 中收到新 Intent。 正如前文所述，这会产生与 “singleTask”launchMode 值相同的行为。 FLAG_ACTIVITY_SINGLE_TOP 如果正在启动的 Activity 是当前 Activity（位于返回栈的顶部），则 现有实例会接收对 onNewIntent() 的调用，而不是创建 Activity 的新实例。 正如前文所述，这会产生与 “singleTop”launchMode 值相同的行为。 FLAG_ACTIVITY_CLEAR_TOP 如果正在启动的 Activity 已在当前任务中运行，则会销毁当前任务顶部的所有 Activity，并通过 onNewIntent() 将此 Intent 传递给 Activity 已恢复的实例（现在位于顶部），而不是启动该 Activity 的新实例。 产生这种行为的 launchMode 属性没有值。 FLAG_ACTIVITY_CLEAR_TOP 通常与 FLAG_ACTIVITY_NEW_TASK 结合使用。一起使用时，通过这些标志，可以找到其他任务中的现有 Activity，并将其放入可从中响应 Intent 的位置。 注：如果指定 Activity 的启动模式为 “standard”，则该 Activity 也会从堆栈中移除，并在其位置启动一个新实例，以便处理传入的 Intent。 这是因为当启动模式为 “standard” 时，将始终为新 Intent 创建新实例。 处理关联“关联”指示 Activity 优先属于哪个任务。默认情况下，同一应用中的所有 Activity 彼此关联。 因此，默认情况下，同一应用中的所有 Activity 优先位于相同任务中。 不过，您可以修改 Activity 的默认关联。 在不同应用中定义的 Activity 可以共享关联，或者可为在同一应用中定义的 Activity 分配不同的任务关联。 可以使用 元素的 taskAffinity 属性修改任何给定 Activity 的关联。 taskAffinity 属性取字符串值，该值必须不同于在 元素中声明的默认软件包名称，因为系统使用该名称标识应用的默认任务关联。 在两种情况下，关联会起作用： 启动 Activity 的 Intent 包含 FLAG_ACTIVITY_NEW_TASK 标志。 默认情况下，新 Activity 会启动到调用 startActivity() 的 Activity 任务中。它将推入与调用方相同的返回栈。 但是，如果传递给 startActivity() 的 Intent 包含 FLAG_ACTIVITY_NEW_TASK 标志，则系统会寻找其他任务来储存新 Activity。这通常是新任务，但未做强制要求。 如果现有任务与新 Activity 具有相同关联，则会将 Activity 启动到该任务中。 否则，将开始新任务。 如果此标志导致 Activity 开始新任务，且用户按“主页”按钮离开，则必须为用户提供导航回任务的方式。 有些实体（如通知管理器）始终在外部任务中启动 Activity，而从不作为其自身的一部分启动 Activity，因此它们始终将 FLAG_ACTIVITY_NEW_TASK 放入传递给 startActivity() 的 Intent 中。请注意，如果 Activity 能够由可以使用此标志的外部实体调用，则用户可以通过独立方式返回到启动的任务，例如，使用启动器图标（任务的根 Activity 具有 CATEGORY_LAUNCHER Intent 过滤器；请参阅下面的启动任务部分）。 Activity 将其 allowTaskReparenting 属性设置为 “true”。 在这种情况下，Activity 可以从其启动的任务移动到与其具有关联的任务（如果该任务出现在前台）。 提示：如果从用户的角度来看，一个 .apk 文件包含多个“应用”，则您可能需要使用 taskAffinity 属性将不同关联分配给与每个“应用”相关的 Activity。 清理返回栈如果用户长时间离开任务，则系统会清除所有 Activity 的任务，根 Activity 除外。 当用户再次返回到任务时，仅恢复根 Activity。系统这样做的原因是，经过很长一段时间后，用户可能已经放弃之前执行的操作，返回到任务是要开始执行新的操作。 您可以使用下列几个 Activity 属性修改此行为： alwaysRetainTaskState 如果在任务的根 Activity 中将此属性设置为 “true”，则不会发生刚才所述的默认行为。即使在很长一段时间后，任务仍将所有 Activity 保留在其堆栈中。 clearTaskOnLaunch 如果在任务的根 Activity 中将此属性设置为 “true”，则每当用户离开任务然后返回时，系统都会将堆栈清除到只剩下根 Activity。 换而言之，它与 alwaysRetainTaskState 正好相反。 即使只离开任务片刻时间，用户也始终会返回到任务的初始状态。 finishOnTaskLaunch 此属性类似于 clearTaskOnLaunch，但它对单个 Activity 起作用，而非整个任务。 此外，它还有可能会导致任何 Activity 停止，包括根 Activity。 设置为 “true” 时，Activity 仍是任务的一部分，但是仅限于当前会话。如果用户离开然后返回任务，则任务将不复存在。启动任务通过为 Activity 提供一个以 “android.intent.action.MAIN” 为指定操作、以 “android.intent.category.LAUNCHER” 为指定类别的 Intent 过滤器，您可以将 Activity 设置为任务的入口点。 例如：1234567&lt;activity ... &gt; &lt;intent-filter ... &gt; &lt;action android:name=&quot;android.intent.action.MAIN&quot; /&gt; &lt;category android:name=&quot;android.intent.category.LAUNCHER&quot; /&gt; &lt;/intent-filter&gt; ...&lt;/activity&gt; 此类 Intent 过滤器会使 Activity 的图标和标签显示在应用启动器中，让用户能够启动 Activity 并在启动之后随时返回到创建的任务中。 第二个功能非常重要：用户必须能够在离开任务后，再使用此 Activity 启动器返回该任务。 因此，只有在 Activity 具有 ACTION_MAIN 和 CATEGORY_LAUNCHER 过滤器时，才应该使用将 Activity 标记为“始终启动任务”的两种启动模式，即 “singleTask” 和 “singleInstance”。例如，我们可以想像一下如果缺少过滤器会发生什么情况： Intent 启动一个 “singleTask” Activity，从而启动一个新任务，并且用户花了些时间处理该任务。然后，用户按“主页”按钮。 任务现已发送到后台，而且不可见。现在，用户无法返回到任务，因为该任务未显示在应用启动器中。 如果您并不想用户能够返回到 Activity，对于这些情况，请将 元素的 finishOnTaskLaunch 设置为 “true”（请参阅清理堆栈）。 有关如何在概览屏幕中显示和管理任务与 Activity 的更多信息，请参阅概览屏幕。]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[home后从service重新启动activity延迟问题]]></title>
    <url>%2F2017%2F08%2F24%2Ftips-starting-an-activity-from-a-service-after-home-button%2F</url>
    <content type="text"><![CDATA[问题: 在Activity界面，按下HOME键后，点击悬浮层按钮，再启动Activity， Activity要延时5S后才出来。 经验证，这个问题不是应用自身的BUG。那怕该Activity是空的，也会有这个问题 Android为了避免应用在按下HOME键退出后还可以强制把自己启动，特意加的限制。 在现有的API情况下，不能解决这个问题，除非你的应用是一个启动器（Launcher）， 添加了home/ launcher intent filter。如果你不是启动器，又要从悬浮层启动一个Activity，就把该Activity也改成悬浮层吧 1. 不从后台启动 Activity 准则：在谷歌的 Android API Guides 中，特意提醒开发者不要在后台启动 activity，包括在 Service 和 BroadcastReceiver 中，这样的设计是为了避免在用户毫不知情的情况下突然中断用户正在进行的工作，在 http://developer.android.com/guide/practices/seamlessness.html#interrupt 中有如下解释： That is, don’t call startActivity() from BroadcastReceivers or Services running in the background. Doing so will interrupt whatever application is currently running, and result in an annoyed user. Perhaps even worse, your Activity may become a “keystroke bandit” and receive some of the input the user was in the middle of providing to the previous Activity. Depending on what your application does, this could be bad news. 2. 需要违反“不从后台启动 Activity”准则的特例：特例：即便如此，手机厂商的开发者们在开发基于系统级的应用的时候，可能仍然需要有从 Service 或 BroadcastReceiver 中 startActivity 的需求，往往这样的前提是连这样的 Service 或 BroadcastReceiver 也是由用户的某些操作而触发的，Service 或 BroadcastReceiver 只是充当了即将启动 activity 之前的一些代理参数检查工作以便决定是否需要 start 该 activity。 除非是上述笔者所述的特殊情况，应用开发者都应该遵循 “不要从后台启动 Activity”准则。 一个需要特别注意的问题是，特例中所述的情况还会遇到一个问题，就是当通过 home 键将当前 activity 置于后台时，任何在后台startActivity 的操作都将会延迟 5 秒，除非该应用获取了 “android.permission.STOP_APP_SWITCHES” 权限。 关于延迟 5 秒的操作在 com.android.server.am.ActivityManagerService 中的 stopAppSwitches() 方法中，系统级的应用当获取了 “android.permission.STOP_APP_SWITCHES” 后将不会调用到这个方法来延迟通过后台启动 activity 的操作，事实上 android 原生的 Phone 应用就是这样的情况，它是一个获取了”android.permission.STOP_APP_SWITCHES” 权限的系统级应用，当有来电时，一个从后台启动的 activity 将突然出现在用户的面前，警醒用户有新的来电，这样的设计是合理的。 所以，当你需要开发类似 Phone 这样的应用时，需要做如下工作： root 你的手机； 在 AndroidManifest.xml 中添加 “android.permission.STOP_APP_SWITCHES” 用户权限； 将你开发的应用程序 push 到手机的 /system/app 目录中。 3. 参考资料：无缝的设计之——不要中断你的用户 stackoverflow 中关于后台 startActivity 被延迟 5 秒的探讨]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tips-c-language]]></title>
    <url>%2F2017%2F08%2F06%2Ftips-c-language%2F</url>
    <content type="text"><![CDATA[c预定义宏__func__代表这条语句所在的函数的函数名]]></content>
      <categories>
        <category>language</category>
      </categories>
      <tags>
        <tag>tips</tag>
        <tag>language</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FQA]]></title>
    <url>%2F2017%2F07%2F25%2FFQA%2F</url>
    <content type="text"><![CDATA[ssh连接服务器出错12345678910111213@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed.The fingerprint for the RSA key sent by the remote host isda:f7:3e:ba:f7:00:e6:44:76:f2:58:6e:48:**.Please contact your system administrator.Add correct host key in /用户home目录/.ssh/known_hosts to get rid of this message.Offending RSA key in /用户home目录/.ssh/known_hosts:1RSA host key for ip地址 has changed and you have requested strict checking.Host key verification failed. 出现这个问题的原因是,第一次使用SSH连接时，会生成一个认证，储存在客户端的known_hosts中. 可使用以下指令查看: 1ssh-keygen -l -f ~/.ssh/known_hosts 由于服务器重新安装系统了，所以会出现以上错误。 解决办法: 1ssh-keygen -R 服务器端的ip地址 或者直接在known_hosts中删除对应ip的行]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webrtc线程模型]]></title>
    <url>%2F2017%2F07%2F23%2Fwebrtc-source-thread%2F</url>
    <content type="text"><![CDATA[webrtc的base的 thread，是我见过的封装最帅的c++线程库，根据比qt的还好用,发个例子给你 1234567891011121314151617181920212223242526 using namespace webrtc;using namespace rtc;//std::cout&lt;&lt;&quot;Thread::Current()：&quot; &lt;&lt; Thread::Current()-&gt;GetId();//Thread::Current()-&gt;Start(); 不能调用start，因为不是我创建的，他已经开始了 //Thread::Current()-&gt;Run(); //阻塞当前线程进入线程循环Thread * thread = new Thread();//MyRunnable run;//thread-&gt;Start(&amp;run);//可以带一个Runnable参数运行,运行完就结束，否则运行Thread::Run进入消息循环thread-&gt;Start();//std::cout &lt;&lt; &quot;Thread::Invoke()：&quot;&lt;&lt; thread-&gt;Invoke&lt;bool&gt;(RTC_FROM_HERE, &amp;task)&lt;&lt; &quot; at &quot; &lt;&lt; Thread::Current()-&gt;GetId() &lt;&lt; std::endl;thread-&gt;Post(RTC_FROM_HERE, Bind(task2));//将最常用的auto handler= new MessageClient;//thread-&gt;PostAt(RTC_FROM_HERE, (int64_t)3000,handler);//thread-&gt;PostDelayed(RTC_FROM_HERE, (int64_t)5000, handler);//thread-&gt;Stop();Thread * thread2 = new Thread();thread2-&gt;Start();thread2-&gt;Post(RTC_FROM_HERE, Bind(task2));//将最常用的//thread2-&gt;Invoke() 非常有用，在任何地方可以指定我的代码运行在某个线程//api下的proxy机制，实际上就是设置要执行的线程，然后加锁等待线程执行结果。这是我设计对外接口可以在任何线程调用而不出错的常用方法//base的asyncinvoker与proxy类似的机制。 有ios的gdc，android的handler异曲同工 因为编写复杂稳定的多线程C++项目实在太难，所以一个好的跨平台C++基础库是我最求的目标,目前比较欣赏的项目有： Boost:大而全，缺少一些可以直接上手的东西如线程消息队列，智能指针并非线程安全。 QT core：非常好 C++11：也需要线程消息队列，线程安全智能指针。 chromium的base库：太大了 当我看到webrtc的base时，非常惊讶的发现它正是我想要的,特点： 小：只有几M 纯：基于c++标准库和各操作系统sdk 跨平台 对智能指针、线程、socket封装非常好。 不断更新（需要一直跟踪官方代码） 移植出来单独使用，方案有三： 把源码拷贝出来用通用的编译工具（makefile，cmake，qmake）管理。（makefile较复杂，cmake简单，qmake最简单） 把源码拷贝出来用基于自带的gn管理 在webrtc项目里面编译和合并需要的静态库和pdb 因为google官方说了：引用计数+引用计数的智能化（scoped_ref_ptr）+弱引用就可以解决问题。 shared_ptr不是线程安全的，因为shared_ptr有两个成员：引用计数，和源对象指针。没办法对两个成员同时实现原子操作。 但unique_ptr是个好东西 智能指针的使用： 不用再使用delete。 尽量使用unique_ptr。 多个线程读写同一个 shared_ptr 对象，那么需要加锁。 shared_ptr 和weak_ptr配合解决循环引用的问题。 weak_ptr必须，oc，swift的ViewControler和控件都是weak关系 内存管理模型的三种级别： 1 手动内存管理(c/c++的malloc与free，new与delete)：容易出错。 2 自动内存管理（oc的arc，c++的智能指针，scoped_ptr）：存在循环引用问题，通过程序员自己管理强弱引用关系解决。 3 垃圾回收机制（如java,python）：后台GC降低了程序效率，好的程序员仍然好考虑java的强引用[表情]引用/软引用/ 3 线程模型 1 生产者消费模型（mutex，condition）：最最常用的模型。 2 线程池模型：解决大量请求分配太多线程的问题。比如一个android和ios的app，http请求会很多很多。 3 (着重强调）串行模型：ios有GCD(Grand Central Dispatch，global queue是线程池），android有looper， win32有PostMessage，boost有strand 读写锁：特别只有写才会不安全的情况。 再结合其他的手段会让程序简洁优美易读：java的handler，oc的delegate和block、swift的闭包，mvc模式 ，c++的function/bind/lambda，python和javascript的function 而串行模型就成了解决这类多线程问题的首选，就是线程消息模型。 在android 系统里面，无数这样的例子。 模块处理线程Call构造方法中创建module_process_thread与pacer_thread两个ProcessThread.接着为module_process_thread注册CallStats, ReceiveSideCongestionController, SendSideCongestionController模块,为pacer_thread注册PacedSender, RemoteBitrateEstimator模块. Call::CreateVideoSendStream创建VideoSendStream时,将module_process_thread做构造参数传入,调用RegisterProcessThread方法,注册所有的rtc_rtcp模块到module_process_thread线程.同样的为VideoReceiveStream中设置.]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AndroidStudio常见问题]]></title>
    <url>%2F2017%2F07%2F23%2Ftips-androidstudio%2F</url>
    <content type="text"><![CDATA[如何解决Unsupported major.minor version 52.0问题？ http://www.jianshu.com/p/5eebd3c609d6 运行./gradlew :PandaAndroidDemo:release出现如下错误： 12345678910111213FAILURE: Build failed with an exception.* Where:Build file &apos;/Users/shitianci/work/Lab/panda.android/PandaAndroidDemo/build.gradle&apos; line: 1* What went wrong:A problem occurred evaluating project &apos;:PandaAndroidDemo&apos;.&gt; java.lang.UnsupportedClassVersionError: com/android/build/gradle/AppPlugin : Unsupported major.minor version 52.0* Try:Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.BUILD FAILED 直接点击 run按钮 或者 Build→Generate Build APK 却运行正常。 这里面有两个问题： 为什么出现Unsupported major.minor version 52.0？ 为什么gradle命令和android studio按钮运行结果不一样？ 问题一：为什么出现Unsupported major.minor version 52.0？在网上找了一圈，最后在stackoverflow找到了本质原因 12345You get this error because a Java 7 VM tries to load a class compiled for Java 8Java 8 has the class file version 52.0 but a Java 7 VM can only load class files up to version 51.0In your case the Java 7 VM is your gradle build and the class is com.android.build.gradle.AppPlugin 简单来说，就是java的编译环境版本太低，java 8 class file的版本是52，Java 7虚拟机只能支持到51。所以需要升级到java 8 vm才行。 问题二：为什么gradle命令和android studio按钮运行结果不一样？从问题1来看，肯定Android Studio按钮调用的是java 8 vm，所以查找一下系统配置，最终在Project Structure找到了如下设置： Android Studio 2.2.2使用了自带的JDK环境，其地址为 1/Applications/Android Studio.app/Contents/jre/jdk/Contents/Home 而gradle命令的执行环境是在gradle.properties配置的，其指向为： 1org.gradle.java.home=/Library/Java/JavaVirtualMachines/jdk1.7.0_71.jdk/Contents/home 将其修改为： 1org.gradle.java.home=/Applications/Android Studio.app/Contents/jre/jdk/Contents/Home INSTALL_PARSE_FAILED_NO_CERTIFICATES安装问题Android studio 更新到25后打包问题，打包后的应用安装提示：INSTALL_PARSE_FAILED_NO_CERTIFICATES Android N 引入一项新的应用签名方案 APK Signature Scheme v2，它能提供更快的应用安装时间和更多针对未授权 APK 文件更改的保护。 在默认情况下，Android Studio 2.2 和 Android Gradle 2.2 插件会使用 APK Signature Scheme v2 和传统签名方案来签署您的应用。 脏的解决方式：使用v1打包]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>tips</tag>
        <tag>AndroidStudio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RFC3550 RTP中文版]]></title>
    <url>%2F2017%2F07%2F22%2Fp-rfc-3550-zh%2F</url>
    <content type="text"><![CDATA[RFC3550 RTP：实时应用程序传输协议 摘要 本文描述RTP（real-time transport protocol），实时传输协议。RTP在多点传送（多播）或单点传送（单播）的网络服务上，提供端对端的网络传输功能，适合应用程序传输实时数据，如：音频，视频或者仿真数据。RTP没有为实时服务提供资源预留的功能，也不能保证QoS（服务质量）。数据传输功能由一个控制协议（RTCP）来扩展，通过扩展，可以用一种方式对数据传输进行监测控制，该协议（RTCP）可以升级到大型的多点传送（多播）网络，并提供最小限度的控制和鉴别功能。RTP和RTCP被设计成和下面的传输层和网络层无关。协议支持RTP标准的转换器和混合器的使用。 本文的大多数内容和旧版的RFC1889相同。在线路里传输的数据包格式没有改变，唯一的改变是使用协议的规则和控制算法。为了最小化传输，发送RTCP数据包时超过了设定的速率，而在这时，很多的参与者同时加入了一个会话，在这样的情况下，一个新加入到（用于计算的可升级的）计时器算法中的元素是最大的改变。 目录（Table of Contents） 1 引言 （Introduction） 1 1 术语（Terminology） 2 RTP使用场景（RTP Use Scenarios） 2 1 简单多播音频会议（ Simple Multicast Audio Conference） 2 2 音频和视频会议（Audio and Video Conference） 2 3 混频器和转换器（Mixers and Translators） 2 4 分层编码（Layered Encodings） 3 定义（Definitions） 4 字节序，校正和时间格式（Byte Order, Alignment, and Time Format） 5 RTP数据传输协议（RTP Data Transfer Protocol） 5 1 RTP固定头域（RTP Fixed Header Fields） 5 2 多路复用RTP会话（Multiplexing RTP Sessions） 5 3 RTP头的配置文件详细变更（Profile-Specific Modifications to the RTP Header） 5 3 1 RTP报头扩展（RTP Header Extension） 6 RTP控制协议（RTP Control Protocol） – RTCP 6 1 RTCP包格式（RTCP Packet Format） 6 2 RTCP传输间隔（RTCP Transmission Interval） 6 2 1 维护会话成员数目（Maintaining the number of session members） 6 3 RTCP包的发送与接收规则（RTCP Packet Send and Receive Rules） 6 3 1 计算RTCP传输间隔（Computing the RTCP Transmission Interval） 6 3 2 初始化（Initialization） 6 3 3 接收RTP或RTCP（非BYE)包（Receiving an RTP or Non-BYE RTCP Packet） 6 3 4 接收RTCP（BYE）包（Receiving an RTCP BYE Packet） 6 3 5 SSRC计时失效（Timing Out an SSRC） 6 3 6 关于传输计时器的到期（Expiration of Transmission Timer） 6 3 7 传输一个 BYE 包（Transmitting a BYE Packet） 6 3 8 更新we_sent（Updating we_sent） 6 3 9 分配源描述带宽（Allocation of Source Description Bandwidth） 6 4 发送方和接收方报告（Sender and Receiver Reports） 6 4 1 SR：发送方报告的RTCP包（SR: Sender report RTCP packet） 6 4 2 RR：接收方报告的RTCP包（RR: Receiver Report RTCP Packet） 6 4 3 扩展发送方和接收方报告（Extending the Sender and Receiver Reports ） 6 4 4 分析发送方和接收方报告（Analyzing Sender and Receiver Reports ） 6 5 SDES：源描述RTCP包（SDES: Source description RTCP packet） 6 5 1 CNAME：规范终端标识符的SDES数据项(CNAME: Canonical End-Point Identifier SDES Item） 6 5 2 NAME：用户名的SDES数据项（NAME: User name SDES item) 6 5 3 EMAIL:电子邮件地址的SDES数据项（EMAIL: Electronic Mail Address SDES Item） 6 5 4 PHONE：电话号码的SDES数据项（PHONE: Phone Number SDES Item） 6 5 5 LOC:地理用户地址的SDES数据项（LOC: Geographic User Location SDES Item） 6 5 6 TOOL：应用程序或工具名字的SDES数据项（TOOL: Application or Tool Name SDES Item） 6 5 7 NOTE：通知/状态的SDES数据项（NOTE: Notice/Status SDES Item） 6 5 8 PRIV:私有扩展的SDES数据项（PRIV: Private Extensions SDES Item） 6 6 BYE：Goodbye RTCP包（BYE: Goodbye RTCP packet） 6 7 APP:定义应用程序的RTCP包（APP: Application-Defined RTCP Packet） 7 RTP转换器和混频器（RTP Translators and Mixers） 7 1 概述（General Description ） 7 2 在转换器中的RTCP数据处理（RTCP Processing in Translators） 7 3 在混频器中的RTCP数据处理（RTCP Processing in Mixers ） 7 4 级联混频器（Cascaded Mixers） 8 SSRC标识符的分配和使用（SSRC Identifier Allocation and Use） 8 1 冲突概率（Probability of Collision ） 8 2 冲突解决和循环检测（Collision Resolution and Loop Detection） 8 3 在分层编码中使用（Use with Layered Encodings） 9 安全（Security ） 9 1 机密性（Confidentiality） 9 2 身份验证和消息完整性（Authentication and Message Integrity） 10 拥塞控制（Congestion Control） 11 网络和传输协议之上的RTP（RTP over Network and Transport Protocols） 12 协议常量摘要（Summary of Protocol Constants） 12 1 RTCP 包类型（RTCP Packet Types） 12 2 SDES 类型（SDES Types） 13 RTP概况和负载格式详细说明 （RTP Profiles and Payload Format Specifications） 14 安全考虑（Security Considerations） 15 IANA考虑（IANA Considerations） 16 知识产权声明（Intellectual Property Rights Statement） 17 鸣谢（Acknowledgments） 附录 A 算法（Algorithms） 附录 A 1 RTP数据头有效性检查（RTP Data Header Validity Checks ） 附录 A 2 RTCP数据头有效性检查（RTCP Header Validity Checks） 附录 A 3 确定RTP包预期数目和丢失数目（Determining Number of Packets Expected and Lost） 附录 A 4 生成SDES RTCP包（Generating RTCP SDES Packets） 附录 A 5 解析RTCP SDES包（Parsing RTCP SDES Packets） 附录 A 6 生成32位随机标识符（Generating a Random 32-bit Identifier 附录 A 7 计算RTCP传输间隔（Computing the RTCP Transmission Interval） 附录 A 8 估测两次到达间隔的抖动（Estimating the Interarrival Jitter） 附录 B 与RFC1889不同之外（Changes from RFC 1889） 参考书目（References） 标准化引用（Normative References ） 资料性引用（Informative References） 作者地址 完整的版权声明 1.绪论 本文详细的介绍实时传输协议RTP，RTP提供带有实时特性的端对端数据传输服务，传输的数据如：交互式的音频和视频。那些服务包括有效载荷类型定义，序列号，时间戳和传输监测控制。应用程序在UDP上运行RTP来使用它的多路技术和checksum服务。2种协议都提供传输协议的部分功能。不过，RTP可能被其他适当的下层网络和传输协议使用（见11节）。如果下层网络支持，RTP支持数据使用多播分发机制转发到多个目的地。 注意RTP本身没有提供任何的机制来确保实时的传输或其他的服务质量保证，而是由低层的服务来完成。它不保证传输或防止乱序传输，它不假定下层网络是否可靠，是否按顺序传送数据包。RTP包含的序列号允许接受方重构发送方的数据包顺序，但序列号也用来确定一个数据包的正确位置，例如，在视频解码的时候不用按顺序的对数据包进行解码。 但是RTP原先的设计是用来满足多参与者的多媒体会议的需要，它没有限定于专门的应用。连续数据的储存，交互分布式仿真，动态标记，以及控制和测量应用程序也可能会适合使用RTP。 该文档定义RTP，由2个密切联系的部分组成： ○实时传输协议RTP，用于实时传输数据。 ○RTP控制协议RTCP，用于监控服务质量和传达关于在一个正在进行的会议中的参与者的信息。后者对“宽松控制”的会议可能已经足够，但是并没有必要去支持一个应用程序所有的通讯控制条件。这个功能可能充分的或者部分的被一个单独的会议控制协议所包含，这超过了本文档的范围。 RTP表现了协议的一种新的类型，该类型由Clark和Tennenhouse提出[10]，遵循应用级（framing）框架和（integrated layer processing）统一层处理的原则。就是说，RTP被规定为可扩展的，用来提供一个专门的应用程序需要的信息，并将会经常性的被归并到应用程序的处理中，而不是作为一个单独的层被实现。RTP只是一个故意不完成的协议框架。本文档详细说明那些功能，希望这些功能能够普遍贯穿于所有适合使用RTP的应用程序。和常规的协议不同，额外的功能可能通过完善协议本身或者增加一个可能需要分析的选项机制来增加，RTP被规定为可以根据需要通过修改和/或增加操作，“剪裁”到报头。具体的例子见5.3和6.4.3节。 因此，除了本文档，用于专门应用程序的RTP完整的说明将还需要一个或者更多的同类文档（见13节）： ○ 一个框架（大致轮廓）的说明文档，该文档定义了一系列的有效载荷类型编码和它们与有效载荷格式之间的映射（例如，媒体编码）。一个框架可能也定义了应用程序对RTP的一些扩展和修改，详细到一个专门的类。典型的情况，一个应用程序将在一个框架下运行。一个用于音频和视频数据的框架可以在同类RFC3551[1]文档里找到。 ○有效载荷格式说明文档，该文档定义了一个像一个音频或者视频编码的特殊载荷，在RTP里是如何被传输的。 一个关于实时服务和算法如何实现的讨论和关于一些RTP设计结果的后台讨论能够在[11]中找到。 1.1术语 在这个文档里的关键词“一定要”，“一定不能”，“必需的”，“会”，“不会”，“应该”，“不应该”，“推荐”，“可能”和“可选” 将会像在BCP 14（Basic Control Program，基本控制程序），RFC2119[2]里描述一样的解释。并指出适合RTP实现的需要的级别。 2 RTP使用场景（RTP Use Scenarios） 2.1 简单多播音频会议（ Simple Multicast Audio Conference） 2.2 音频和视频会议（Audio and Video Conference） 2.3 混频器和转换器（Mixers and Translators） 2.4 分层编码（Layered Encodings） 以下章节描述了用到RTP的一些方面。所举例子用来说明RTP应用的基本操作，但RTP的用途不限于此。在这些例子中，RTP运行于IP和UDP之上，并且遵循RFC3551所描述的音频和视频的配置文件中的约定。 2.1 简单多播音频会议（Simple Multicast Audio Conference） IETF的一个工作组开会讨论最新协议草案时，使用Internet的IP多播服务来进行语音通讯。工作组中心分配到一个多播的组地址和一对端口。一个端口用于音频数据，另一个端口用于控制（RTCP）数据包。该地址和端口信息发布给预定的参与者。如果有私密性要求，则可用章节9.1中说明的方法，对数据和控制包进行加密，这时就需要生成和发布加密密钥。分配和发布机制的精确细节不在RTP的讨论范围之内。 每个与会者所使用的音频会议应用程序，都以小块形式（比方说持续２０秒时间）来发送音频数据。每个音频数据块都前导RTP报头；RTP报头和数据依次包含在UDP包里。RTP报头指明了各个包里音频编码的类型（如PCM,ADPCM,LPC），这样发送方可以在会议过程中改变编码方式，以适应状况的变化，例如，要加进一个低带宽接入的参与者，或是要应付网络拥塞。 Internet，像其他的报文分组网络一样，偶而会丢失和重排包，造成时长不等的延迟。为弥补这个不足，RTP报头里包含计时信息和一个序列号，允许接收方重建来自源的计时信息，比如前文例子中音频块以20s的间隔在扬声器中连续播放。会议中，对每个RTP包的源,单独地实施计时重建。序列号还被接收方用来评估丢失包数目。 由于会议期间不断有工作组成员加入或离开，因此有必要知道任一时刻的实际参与者及他们接收音频数据的状况好坏。出于这个目的，会议中每个音频应用程序的实例，都在RTCP（控制）端口上周期性地多播一个附加用户名的接收报告。接收报告指明了当前说话者被收听到的状况，可用于控制自适应性编码。除了用户名外，通过控制带宽限度，可以包含其他标识信息。一个站点在离开会议时发送RTCP BYE包（章节6.5）。 2.2 音频和视频会议（Audio and Video Conference） 一个会议如果同时使用音频和视频媒体，则二者传输时使用不同的RTP会话。也就是说，两种媒体中RTP包和RTCP包的传输，是使用两个不同的UDP端口对和（或）多播地址。在RTP层次，音频和视频会话没有直接的耦合，下面这种情况除外：一个同时参加两个会话的参与者，在两个会话的RTCP包中，使用了相同的规范名，这样两个会话就发生关联（耦合）了。 这样区隔开来的目的之一，是允许一些会议参与者只接受自己选择的单一媒体（或者音频，或者视频）。更进一步的说明在章节5.2给出。尽管两种媒体区分开来了，但通过两个会话RTCP包内载有的计时信息，同源的音频与视频还是能够同步回放。 2.3 混频器和转换器（Mixers and Translators） 到目前为止，我们皆假设所有站点都收到相同格式的媒体数据。然而这并不总是行得通。考虑一下这种情况，一个地方的参与者只能低速接入会议，而其他大部分参与者都能享受高速连接。与其让强迫大家都忍受低带宽，不如在只能低速接入的地方，放置一个减质量音频编码的RTP层次的中继（称作混频器）。混频器将重新同步输入的音频包，重建发送方产生的20ms固定间隔，混频已重建过的音频流为单一的流，转换音频编码为低带宽格式，最后通过低带宽连接转发数据包流（package stream)。这些包可能被单播到一个接收方，也可能多播到另一个的地址而发给多个接收方。RTP报头为混频器提供了一种方法，使其能辨识出对混频后的包有用的源，从而保证提供给接收方正确的说话者指示。 在音频会议中，一些预定参与者尽管有高带宽连接，但不能通过IP多播直接接入会议。例如，他们可能位于一个不允许任何IP包通过的应用层防火墙后面。对这些站点，可能就不需要混频，而需要另一种称为转换器的RTP层次中继。可以在防火墙两侧分别安装一个转换器，外侧转换器将所有多播包通过安全连接转入内侧转换器，内侧转换器再转发给内部网的一个多播组（multicast group)。 混频器和转换器可以设计成用于各种目的。比如，一个视频混频器在测量多个不同视频流中各人的单独影像后，将它们组合成一个单一视频流来模拟群组场景。又如，在只用IP/UDP和只用ST_II的两个主机群之间通过转换建立连接。再如，在没有重新同步或混频时，用packet-by-packet编码转换来自各个独立源的视频流。混频器和转换器的操作细节见章节7。 2.4 分层编码（Layered Encodings） 为了匹配接收方的能力（容量）以及适应网络拥塞，多媒体应用程序应当能够调整其传输速率。许多应用实现把调适传输速率的责任放在源端。这种做法在多播传输中并不好，因为不同接收方对带宽存在着冲突性需求。这经常导致最小公分母的场景，网格中最小的管道支配了全部实况多媒体“广播”的质量和保真度。 相反地，可以把分层编码和分层传输系统组合起来，从而把调适速率的责任放在接收端。在IP多播之上的RTP上下文中，对一个横跨多个RTP会话（每个会话在独自多播组上开展）的分级表示信号(a hierarchically represented signal)，源能够把它的分层（layers)分割成条。 接收方仅需合并适当的多播组子集，就能适应异种网络和控制接收带宽。 RTP分层编码的细节在章节6.3.9，8.3和11中给出。 3. 定义（definitions) RTP负载（RTP payload）：通过RTP传输的包中的数据，例如，音频样本或压缩好的视频数据。负载格式与解释不在本文讨论范围。 RTP包（RTP packet）：一种数据包，其组成部分有：一个固定RTP报头，一个可能为空的作用源（contributing sources）列表（见下文），以及负载数据。一些下层协议可能要求对RTP包的封装进行定义。一般地，下层协议的一个包包含一个RTP包，但若封装方法允许，也可包含数个RTP包（见章节11）。 RTCP包（RTCP packet）：一种控制包，其组成部分有：一个类似RTP包的固定报头，后跟一个结构化的部分，该部分具体元素依不同RTCP包的类型而定。格式的定义见章节６。一般地，多个RTCP包将在一个下层协议的包中以合成RTCP包的形式传输；这依靠RTCP包的固定报头中的长度字段来实现。 端口（Port）：“传输协议用来在同一主机中区分不同目的地的一种抽象。TCP/IP协议使用正整数来标识不同端口。”[12] OSI传输层使用的传输选择器（TSEL,the transport selectors）等同于这里的端口。RTP需依靠低层协议提供的多种机制，如“端口”用以多路复用会话中的RTP和RTCP包。 传输地址(Transport address)：是网络地址与端口的结合，用来标识一个传输层次的终端，例如一个IP地址与一个UDP端口。包是从源传输地址发送到目的传输地址。 RTP媒体类型（RTP media type）：一个RTP媒体类型是一个单独RTP会话所载有的负载类型的集合。RTP配置文件把RTP媒体类型指派给RTP负载类型。 多媒体会话（Multimedia session）：在一个参与者公共组中，并发的RTP会话的集合。例如，一个视频会议（为多媒体会话）可能包含一个音频RTP会话和一个视频RTP会话。 RTP会话（RTP session）：一群参与者通过RTP进行通信时所产生的关联。一个参与者可能同时参与多个RTP会话。在一个多媒体会话中，除非编码方式把多种媒体多路复用到一个单一数据流中，否则每种媒体都将使用各自的RTCP包，通过单独的RTP会话来传送。通过使用不同的目的传输地址对（一个网络地址加上一对分别用于RTP和RTCP的端口，构成了一个传输地址对）来接收不同的会话，参与者能把多个RTP会话区隔开来。单个RTP会话中的所有参与者，可能共享一个公用目的传输地址对，比如IP多播的情况；也可能各自使用不同的目的传输地址对，比如个体单播网络地址加上一个端口对。对于单播的情况，参与者可能使用相同端口对来收听其他所有参与者，也可能对来其他每个参与者使用不同的端口对来收听。 RTP会话间相互区别的特征，在于每个RTP会话都维护一个用于SSRC标识符的独立完整的空间。RTP会话所包含的参与者组，由能接收SSRC标识符的参与者组成，这些SSRC标识符由RTP（同步源或作用源）或RTCP中的任意参与者传递。例如，考虑下述情况，用单播UDP实现的三方会议，每方都用不同的端口对来收听其他两方。如果收到一方的数据，就只把RTCP反馈发送给那一方，则会议就相当于由三个单独的点到点RTP会话构成；如果收到一方的数据，却把RTCP反馈发送另两方，则会议就是由一个多方（multi-party)RTP会话构成。后者模拟了三方间进行IP多播通信时的行为。 RTP框架允许上述规定发生变化，但一个特定的控制协议或者应用程序在设计时常常对变化作出约束。 同步源(SSRC，Synchronization source)：RTP包流的源，用RTP报头中32位数值的SSRC标识符进行标识，使其不依赖于网络地址。一个同步源的所有包构成了相同计时和序列号空间的一部分，这样接收方就可以把一个同步源的包放在一起，来进行重放。举些同步源的例子，像来自同一信号源的包流的发送方，如麦克风、摄影机、RTP混频器（见下文）就是同步源。一个同步源可能随着时间变化而改变其数据格式，如音频编码。SSRC标识符是一个随机选取的值，它在特定的RTP会话中是全局唯一（globally unique）的（见章节8）。参与者并不需要在一个多媒体会议的所有RTP会话中，使用相同的SSRC标识符；SSRC标识符的绑定通过RTCP（见章节6.5.1）。如果参与者在一个RTP会话中生成了多个流，例如来自多个摄影机，则每个摄影机都必须标识成单独的同步源。 作用源（CSRC，Contributing source )：若一个RTP包流的源，对由RTP混频器生成的组合流起了作用，则它就是一个作用源。对特定包的生成起作用的源，其SSRC标识符组成的列表，被混频器插入到包的RTP报头中。这个列表叫做CSRC表。相关应用的例子如，在音频会议中，混频器向所有的说话人（talker)指出，谁的话语（speech)将被组合到即将发出的包中，即便所有的包都包含在同一个（混频器的）SSRC标识符中，也可让听者（接收者）可以清楚谁是当前说话人。 终端系统（End system)：一种应用程序，它产生发送出的RTP包中内容，或者使用接收到的RTP包中内容。在一个特定的RTP会话中，一个终端系统可以扮演一个或多个同步源角色，但通常是一个。 混频器（Mixer)：一种中间系统，它从一个或多个源中接收RTP包，可能改变其数据格式，再按某种方式把这些包组合成一个新的包，然后转发出去。由于多个输入源的计时一般不会同步，所以混频器会对各个流的计时作出调整，并为组合流生成一个新的计时。因此，混频器将被标识成它所产生所有数据包的同步源。 转换器（Translator)：一种中间系统，它转发RTP包而不改变各包的同步源标识符。转换器的例子如下：不作混频地转变编码的设备，把多播复制到单播的重复装置，以及防火墙里应用层次的过滤器。 监视器(Monitor)：一种应用程序，它接收RTP会话参与者所发送的RTCP包，特别是接收报告（reception report)，而且对当前服务质量进行评估，评估结果用于分配监视任务，故障诊断和长期统计。监视器常常被内建到参与会话的应用程序中，但也可以是一个的独立的应用程序——不参加会话、也不发送或接收RTP数据包（因为它们在不同的端口上）。这些被称作第三方监视器。还有一种情况也是可以接受的，第三方监视器只接收但不发送数据包，或者另外地算入到会话中。 非RTP途径（Non-RTP means)：为提供一个可用的服务，可能还需要其他的协议和机制。特别地，对多媒体会议来说，一个控制协议可以发布多播地址，发布加密密钥，协商所用的加密算法，以及为没有预定义负载类型值的格式，建立负载类型值和其所代表的负载格式之间的动态映射。其他协议的例子如下：会话初始化协议（SIRFC3261[13]），ITU推荐的H.323[14]，还有使用SDP(RFC2327[15])的应用程序，如RTSP(RFC 2326[16]). 对于简单的应用程序，电子邮件或者会议数据库也可能用到。对这些协议和机制的详细说明已经超出了本文档的讨论范围。 5 RTP数据传输协议 5.1 RTP固定头中的各字段 RTP头有以下格式:123456789101112 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|V=2|P|X| CC |M| PT | sequence number |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| timestamp |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| synchronization source (SSRC) identifier |+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+| contributing source (CSRC) identifiers || .... |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ RTP包头格式 前12个字节出现在每个RTP包中，仅仅在被混合器插入时，才出现CSRC识别符列表。这些域有以下意义： 版本(V)：2比特 此域定义了RTP的版本。此协议定义的版本是2。(值1被RTP草案版本使用，值0用在最初”vat”语音工具使用的协议中。) 填充(P)：1比特 若填料比特被设置，则此包包含一到多个附加在末端的填充比特，填充比特不算作负载的一部分。填充的最后一个字节指明可以忽略多少个填充比特。填充可能用于某些具有固定长度的加密算法，或者用于在底层数据单元中传输多个RTP包。 扩展(X)：1比特 若设置扩展比特，固定头(仅)后面跟随一个头扩展。 CSRC计数(CC)：4比特 CSRC计数包含了跟在固定头后面CSRC识别符的数目。 标志(M)：1比特 标志的解释由具体协议规定。它用来允许在比特流中标记重要的事件，如帧边界。 负载类型(PT)：7比特 此域定义了负载的格式，由具体应用决定其解释。协议可以规定负载类型码和负载格式之间一个默认的匹配。其他的负载类型码可以通过非RTP方法动态定义。RTP发送端在任意给定时间发出一个单独的RTP负载类型；此域不用来复用不同的媒体流。 序列号（sequence number）：16比特 每发送一个RTP数据包，序列号加1，接收端可以据此检测丢包和重建包序列。序列号的初始值是随机的(不可预测)，以使即便在源本身不加密时(有时包要通过翻译器，它会这样做)，对加密算法泛知的普通文本攻击也会更加困难。 时间戳（timestamp） 32比特时间戳反映了RTP数据包中第一个字节的采样时间。时钟频率依赖于负载数据格式，并在描述文件（profile）中进行描述。也可以通过RTP方法对负载格式动态描述。 如果RTP包是周期性产生的，那么将使用由采样时钟决定的名义上的采样时刻，而不是读取系统时间。例如，对一个固定速率的音频，采样时钟将在每个周期内增加1。如果一个音频从输入设备中读取含有160个采样周期的块，那么对每个块，时间戳的值增加160。 时间戳的初始值应当是随机的，就像序号一样。几个连续的RTP包如果是同时产生的。如：属于同一个视频帧的RTP包，将有相同的序列号。 不同媒体流的RTP时间戳可能以不同的速率增长。而且会有独立的随机偏移量。因此，虽然这些时间戳足以重构一个单独的流的时间，但直接比较不同的媒体流的时间戳不能进行同步。对于每一个媒体，我们把与采样时刻相关联的RTP时间戳与来自于参考时钟上的时间戳（NTP）相关联。因此参考时钟的时间戳就了数据的采样时间。（即：RTP时间戳可用来实现不同媒体流的同步，NTP时间戳解决了RTP时间戳有随机偏移量的问题。）参考时钟用于同步所有媒体的共同时间。这一时间戳对（RTP时间戳和NTP时间戳），用于判断RTP时间戳和NTP时间戳的对应关系，以进行媒体流的同步。它们不是在每一个数据包中都被发送，而在发送速率更低的RTCP的SR（发送者报告）中。 如果传输的数据是存贮好的，而不是实时采样等到的，那么会使用从参考时钟得到的虚的表示时间线（virtual presentation timeline）。以确定存贮数据中的每个媒体下一帧或下一个单元应该呈现的时间。此种情况下RTP时间戳反映了每一个单元应当回放的时间。真正的回放将由接收者决定。 SSRC：32比特 用以识别同步源。标识符被随机生成，以使在同一个RTP会话期中没有任何两个同步源有相同的SSRC识别符。尽管多个源选择同一个SSRC识别符的概率很低，所有RTP实现工具都必须准备检测和解决冲突。若一个源改变本身的源传输地址，必须选择新的SSRC识别符，以避免被当作一个环路源。 CSRC列表：0到15项，每项32比特 CSRC列表识别在此包中负载的所有贡献源。识别符的数目在CC域中给定。若有贡献源多于15个，仅识别15个。CSRC识别符由混合器插入，并列出所有贡献源的SSRC识别符。例如语音包，混合产生新包的所有源的SSRC标识符都被列出，以在接收端处正确指示参与者。 5.3.1 RTP头扩展 RTP提供扩展机制以允许实现个性化：某些新的与负载格式独立的功能要求的附加信息在RTP数据包头中传输。设计此方法可以使其它没有扩展的交互忽略此头扩展。RTP头扩展的格式如下图所示。 12345670 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | defined by profile | length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | header extension | | .... | 若RTP头中的扩展比特位置1，则一个长度可变的头扩展部分被加到RTP固定头之后。头扩展包含16比特的长度域，指示扩展项中32比特字的个数，不包括4个字节扩展头(因此零是有效值)。RTP固定头之后只允许有一个头扩展。为允许多个互操作实现独立生成不同的头扩展，或某种特定实现有多种不同的头扩展，扩展项的前16比特用以识别标识符或参数。这16比特的格式由具体实现的上层协议定义。基本的RTP说明并不定义任何头扩展本身。 6 RTP控制协议RTCP RTP控制协议(RTCP)向会议中所有成员周期性发送控制包。它使用与数据包相同的传输机制。底层协议必须提供数据包和控制包的复用，例如用不同的UDP端口。RTCP提供以下四个功能：○基本功能是提供数据传输质量的反馈。这是RTP作为一种传输协议的主要作用，它与其他协议的流量和拥塞控制相关。反馈可能对自适应编码有直接作用，并且IP组播的实验表明它对于从接收端得到反馈信息以诊断传输故障也有决定性作用。向所有成员发送接收反馈可以使”观察员”评估这些问题是局部的还是全局的。利用类似多点广播的传输机制，可以使某些实体，诸如没有加入会议的网络业务观察员，接收到反馈信息并作为第三方监视员来诊断网络故障。反馈功能通过RTCP发送者和接收者报告实现。 ○RTCP为每个RTP源传输一个固定的识别符，称为规范名（CNAME）。由于当发生冲突或程序重启时SSRC可能改变，接收者要用CNAME来跟踪每个成员。接收者还要用CNAME来关联一系列相关RTP会话中来自同一个成员的多个数据流，例如同步语音和图像。 ○前两个功能要求所有成员都发送RTCP包，因此必须控制速率以使RTP成员数可以逐级增长。通过让每个成员向所有成员发送控制包，各个成员都可以独立地观察会议中所有成员的数目。此数目可以用来估计发包速率。 ○第四个可选的功能是传输最少的会议控制信息，例如在用户接口中显示参与的成员。这最可能在”松散控制”的会议中起作用，在”松散控制”会议里，成员可以不经过资格控制和参数协商而加入或退出会议。RTCP作为一个延伸到所有成员的方便通路，必须要支持具体应用所需的所有控制信息通信。 ○在RTP用于IP多点广播时，功能1-3是强制的，在所有情况下都推荐使用。建议RTP应用开发商避免使用只能用于单向广播而不能扩充到多用户的方法。 6.1 RTCP包格式这部分定义了几个RTCP包类型，可以传送不同的控制信息： ○SR：发送者报告，描述作为活跃发送者成员的发送和接收统计数字； ○RR：接收者报告，描述非活跃发送者成员的接收统计数字； ○SDES：源描述项，其中包括规范名CNAME。 ○BYE：表明参与者将结束会话。 ○APP：应用描述功能。 在本文中将详细介绍SR和RR。 每个RTCP包的开始部分是与RTP数据包相类似的固定部分，随后是一块结构化单元，它随负载类型不同长度发生变化，但是总以32比特终止。对齐要求和长度域使RTCP包可”堆栈”，即可以将多个RTCP包形成一个复合RTCP包，在底层协议(如UDP)中，通常都是将复合包作为一个包传输的。复合包中的每个RTCP单包可以单独处理，而无需考虑包复合的顺序。然而，为了实现某些协议功能，添加以下限制： ○接收数据的统计信息(在SR或RR中)。只要带宽允许应尽可能经常的发送，以达到统计数字的最大分辨率。因此每个周期发送的RTCP包必须包含一个报告包。 ○新的参与者需要尽快接收一个源的规范名以识别数据源并与媒体建立会话。因此，每个包中必须包含源描述项中的规范名。除非复合包进行了分割以进行部分加密（见9.1节的描述）。 ○必须限制首次在复合包中出现的包类型的数目，以增加在第一个字中常数比特的数目，这样可以增加RTCP包的有效性，以区分误传的RTP包和其他无关的包。因此，所有RTCP包必须以复合包的形式发送。复合包中至少有两个单个的RTCP包。具有以下格式： ○加密前缀：当且仅当复合包被加密时，对每个RTCP复合包加32比特的前缀。 ○SR或RR：复合包中的第一个RTCP包必须是一个报告包。即使没有数据发送和接收，此时发送空的RR包，或者复合包中其他的唯一包是BYE包，也必须发送报告包。 ○附加的RR：若被报告的接收统计源数目超过SR/RR包中最大允许的31个，附加的RR必须跟在最初的报告包后面。 ○源描述SDES ○BYE或APP包 每个RTP参与者在一个报告间隔内应只发送一个RTCP复合包，以便正确估计每个参与者的RTCP带宽。除非像9.1节描述的情况——把一个RTCP复合包分割以进行加密。如果数据源的个数太多，以至于不能把所有的RR包都放到同一个RTCP包中而不超过网络路径的最大传输单元（maximum transport unit MTU），那么可在每个间隔中发送其中的一部分包。在多个发送间隔中，所有的包应该被等概率的选中。这样就可以报告所有数据源的接收数据的情况。如果一个RTCP复合包的长度超过了网络路径的MTU，则它应当被分割为多个更短的RTCP包来传输。这不会影响对RTCP带宽的估计，因为每一个复合包至少代表了一个参与者。要注意的是每个RTCP复合包必须以SR或RR包开头。 1234567891011| |[--------- packet --------][---------- packet ----------][-packet-] | | receiver chunk chunk V reports item item item item -------------------------------------------------------------------- R[SR #sendinfo #site1#site2][SDES #CNAME PHONE #CNAME LOC][BYE##why] -------------------------------------------------------------------- | | |&lt;----------------------- compound packet -----------------------&gt;| |&lt;-------------------------- UDP packet -------------------------&gt;| #: SSRC/CSRC identifier 图1: RTCP复合包举例 6.2 RTCP传输时间间隔 RTP被设计为允许应用自动适应不同的规模的会话――从几个参与者到几千个参与者的会话。 对每一个会话，我们假定数据传输受到一个上限――会话带宽的限制。会话带宽分配给所有的参与者。这个带宽会被预留，并由网络所限制。如果没有预留，基于环境的其他约束将会确定合理的最大带宽供会话使用，这就是会话带宽。会话带宽在一定程度上独立于媒体编码，但媒体编码却依赖于会话带宽。 在涉及媒体应用时，会话带宽参数最好由一个会话控制应用提供。但媒体应用可能设置一个默认参数。此参数由单个发送者选择的编码方式的数据带宽算出。会话管理可能会基于多播范围的规则或其他标准确定带宽限制。所有的参与者应使用相同的会话带宽值以保证计算出相同的RTCP间隔。 控制传输带宽应当是会话带宽的一小部分，这部分所占总的会话带宽的百分比应是已知的。一小部分：传输协议的首要功能是传输数据；已知：控制传输带宽可以被放进带宽描述中提供给资源预留协议，并且使每个参与者都可以独立的计算出他所占有的带宽份额。 控制传输带宽作为额外的一部分加入到会话带宽中。建议RTCP控制传输带宽为RTCP会话带宽的5%。其中的1/4分配给发送者；当发送者的比例超过所有参与者的1/4时，其RTCP控制带宽相应增加。所有的会话参与者必须使用相同的常数（以上提到的百分比），以便计算出相同的发送时间间隔。这些常数应在一个特殊的描述文件中确定。 计算出的RTCP复合包的发送时间间隔应该有一个下限，以免参与者数量较少时大量发送RTCP包。这也使网络暂时断开时，发送间隔不会太小。在应用开始时，一个延迟应加到第一个的TCP复合包发送之前，以便从其他参与者接收RTCP复合包。这样，发送时间间隔能更快的收敛到正确的值。这个延迟可以设为最小时间间隔的一半。固定的时间间隔建议为5秒。 一个实现可能使RTCP最小发送时间间隔与会话带宽参数成比例。则应满足下列约束： ○对多播会话，只有活动的数据发送者使用减小的最小化的值计算RTCP复合包的发送时间间隔。 ○对单播会话，减小的值也可能被不是活动的数据发送者使用，发送初始的RTCP复合包之前的延迟可能是0。 ○对所有会话，在计算参与者的离开时间时，这个固定最小值会被用到。因此，不使用减小的值进行RTCP包的发送，就不会被其他参与者提前宣布超时。 ○减小的最小时间间隔建议为：360/sb(秒)，其中sb：会话带宽（千字节/秒）。当sb&gt;72kb/s时，最小时间间隔将小于5s。 6.3节所描述的算法和附录A.7将实现本节列出的目标： ○计算出的RTCP包的时间间隔与组中参与者的人数成正比。（参与者越多，发送时间间隔越长，每个参与者占有的RTCP带宽越小）。 ○RTCP包的（真实）时间间隔是计算出的时间间隔的0.5～1.5倍之间某个随机的值，以避免所有的参与者意外的同步。 ○RTCP复合包的平均大小将会被动态估计，包括所有发送的包和接收的包。以自动适应携带的控制信息数量的变化。 ○由于计算出的时间间隔依赖于组中的人数。因此，当一个的用户加入一个已经存在的会话或者大量的用户几乎同时加入一个新的会话时，就会有意外的初始化效应。这些新用户将在开始时错误的估计组中的人数（估计太小）。因此他们的RTCP包的发送时间间隔就会太短。如果许多用户同时加入一个会话，这个问题就很重要了。为了处理这处问题考虑了一种叫“时间重估”的算法。这个算法使得组中人数增加时，用户能够支持RTCP包的传输。 当有用户离开会话，不管是发送BYE包还是超时，组中的人数会减少。计算出的时间间隔也应当减少。因此，应用“逆向重估”算法，使组中的成员更快的减少他们的时间间隔，以对组中的人数减少做出响应。 ○BYE包的处理和其他RTCP包的处理不同。BYE包的发送用到一个“放弃支持”算法。以避免大量的BYE包同时发送，使大量参与者同时离开会话。 这个算法适用于所有参与者都允许RTCP包的情况。此时，会话带宽＝每个发送者的带宽×会话中参与者的总人数。详细算法见随后小节，附录A.7给出了算法的一个实现。 6.2.1维持会话成员的人数 当侦听到新的站点的时候，应当把他们加入计数。每一个登录都应在表中创建一条记录，并以SSRC或CSRC进行索引。新的登录直到接收到含有SSRC的包或含有与此SSRC相联系的规范名的SDES包才视为有效（见附录A.1）。当一个与SSRC标识符相对RTCP BYE包收到时，登录会被从表中删除。除非一个“掉队”的数据包到达，使登录重新创建。 如果在几个RTCP报告时间间隔内没有RTP或RTCP包收到，一个参与者可能标记另外一个站点静止，并删除它。这是针对丢包提供的一个很强健的机制。所有站点对这个超时时间间隔乘子应大体相同，以使这种超时机制正常工作。因此这个乘子应在特别的描述文件中确定。 对于一个有大量参与者的会话，维持并存贮一个有所有参与者的SSRC及各项信息的表几乎是不可能的因此，只可以只存贮SSRC。其他算法类似。关键的问题就是，任何算法都不应当低估组的规模，虽然它有可能被高估。 6.3 RTCP包的发送和接收规则 下面列出了如何发送RTCP包，当接收到的TCP包时该干什么的规则。 为执行规则，一个会话参与者就维持下列变量： tp: RTCP包发送的最后时间。 tc: 当前时间。 tn: 估计的下一个RTCP包要发送的时间。 pmembers: tn最后被重新计算时，会计的会话成员的人数。 members: 会话成员人数的当前估计。 senders: 会话成员中发送者人数的估计。 rtcp_bw: 目标RTCP带宽。例如用于会话中所有成员的RTCP带宽。单位bit/s。这将是程序开始时，指定给“会话带宽”参数的一部分。 we_sent: 自当前第二个前面的RTCP发送后，应用程序又发送了数据，则此项为true。 avg_rtcp_size: 此参与者收到的和发送的RTCP复合包的平均大小。单位：bit。按6.2节，此大小包括底层传输层和网络层协议头。 initial: 如果应用程序还未发送RTCP包，则标记为true。 许多规则都用到了RTCP包传输的“计算时间间隔”。此时间间隔将在随后的小节描述。 6.3.1计算RTCP传输时间间隔 一个会话参与者包的平均发送时间间隔应当和所在会话组中人数成正比。这个间隔称为计算时间间隔。它由上面提到的各个状态参量结合起来计算得出。计算时间间隔T的计算如下： 1（1）如果发送者人数≤会话总人数×25%。则T取决于此参与者是否是发送者（we_sent的值）；否则，发送者和接收者将统一处理。 1234567891011121314senders&lt;=25%*memberswe_sentc=avg_rtcp_size/(0.25*rtcp_bw);n=senders;c=avg_rtcp_size/(0.75*rtcp_bw);n=members-senders;c=avg_rtcp_size/rtcp_bw;n=members;notyesyesnot 图：确定c ，n 如6.2节所述，RTP描述文件可能用两个独立的参数（S，R）确定发送者与非发送者。此时，25%和75%只要相应的换成S/(S+R),R/(S+R)即可。注意R＝0的情况。 2 如果initial为true(则未发送过RTCP包)，则设Tmin=2.5s;否则设Tmin=5s。 3 决定性的计算时间间隔（deterministic calculated interval）Td=max(Tmin ,nc)。 4 T=Tdλ；其中λ~U(0.5,1.5)。即λ服从0.5到1.5之间的均匀分布。 5 T=T/(e-0.5)≈T/1.21828，补偿时间重估算法，使之收敛到比计算出的平均RTCP带宽小的一个值。 这个算法产生了一个随机的计算时间间隔，并把至少25%的RTCP带宽分配给发送者，其余的分给接收者。若发送者超过会话总人数的25%，此算法将把带宽平均分给所有的参与者。 6 3.2初始化 一加入会话，参与者的各状态参量初始化为：tp=0； tc=0； senders=0； pmembers=1； members=1； vw_sent=false； rtcp_bw:由会话带宽参数的相应部分得到；initial=true；avg_rtcp_size:初始化为应用程序稍后将发送的RTCP包的可能大小；T：如6.3.1节；tn=T（这意味着，一个计时器将经T时间后被唤醒）；应用程序可以用任何它需要的方式实现计时器。 参与者把它自己的SSRC加到成员列表中。 6.3.3接收到的TP包或一个非BYE的RTCP包 当收到一个参与者的RTP或RTCP包时，若其SSRC不在成员列表中，将其SSRC加入列表；若此参与者被确认有效（如6.2.1节描述），就把列表中成员的值更新。对每个有效的RTP包中的CSRC执行相同的过程。 当收到一个参与者的RTP包时，若其SSRC不在发送者列表中，则将其SSRC加入发送者列表，更新相应的值。 每收到一个RTCP复合包，avg_rtcp_size更新为avg_rtcp_size = 1/16 packet_size + 15/16 avg_rtcp_size ；其中packet_size是刚收到的RTCP复合包的大小。 6.3.4接收RTCP BYE包 除6.3.7小节描述的发送RTCP BYE包之外，如果收到一个RTCP BYE包，则检测成员列表。若SSRC存在；先移除之，并更新成员的值。 另外，为使RTCP包的发送速率与组中人数变化更加协调，当收到一个BYE包使得members的值pmembers时，下面的逆向重估算法应当执行： （1）tn的更新：tn = tc + ( members / pmembers ) ( tn –tc )； （2）tp的更新：tp = tc – ( members / pmembers ) ( tc – tp )；下一个RTCP包将在时刻tn 被发送，比更新前更早一些。 （3）pmembers的更新：pmembers=members； 这个算法并没有防止组的大小被错误的在短时间内估计为0的情况。如：在一个较多人数的会话中，多数参与者几乎同时离开而少数几个参与者没有离开的情况。这个算法并没有使估计迅速返回正确的值。因为这种情况较罕见，且影响不大。 6.3.5 SSRC超时 在随机的时间间隔中，一个参与者必须检测其他参与者是否已经超时。为此，对接收者（we_sent为false），要计算决定性时间间隔Td，如果从时刻Tc-MTd(M为超时因子，默认为5秒)开始，未发送过RTP或RTCP包，则超时。其SSRC将被从列表中移除，成员被更新。在发送者列表中也要进行类似的检测。发送者列表中，任何从时间tc-2T(在最后两个RTCP报告时间间隔内)未发送RTP包的发送者，其SSRC从发送者列表中移除，列表更新。 如果有成员超时，应该执行6.3.4节中的逆向检测算法。每个参与者在一个RTCP包发送时间间隔内至少要进行一次这样的检测。 6.3.6发送时钟到时了 当包传输的发送时钟到时，参与者执行下列操作： （1）按6.3.1节的办法计算T。 （2）更新发送时钟的定时时间，判断是否发送RTCP包，更新pmembers。如图： tp+T&lt;=tc 发送RTCP包 tp=tc; tn=tc+T; initial=false; avg_rtcp_size=1/16 packet_size + 15/16 avg_rtcp_size tn=tp+T Pmemvers=members yes no //不发送RTCP包 图：发送时钟到时的操作 6.3.7发送一个BTE包 当一个参与者离开会话时，应发送BYE包，通知其他参与者。为避免大量参与者同时离开系统时，大量BYE包的发送，若会话人数超过50，则参与者在要离开会话时，应执行下面的算法。这个算法实际上“篡夺”了一般可变成员的角色来统计BYE包。 （1）tp=tc ； members=1； pmembers=1； sinitial=1； we_sent=false； senders=0； rtcp_size:设置为将要发送的RTCP包大小；计算“计算时间间隔”T；tn=tc+T；(BYE包预计在时刻tn被发送)。 (2)每当从另外一个参与者接收到BYE包时，成员人数加1。不管此成员是否存在于成员列表中，也不管SSRC采样何时使用及BYE包的SSRC是否包含在采样之中。如果收到RTP包或甚的RTCP包（除BYE包之外的RTCP包），成员人数不增加。类似，只有在收到BYE包时，avg_rtcp_size才更新。当RTP包到达时，发送者人数senders不更新，保持为0。 （3）在此基础上，BYE包的传输服从上面规定的一般的RTCP包的传输。 （BYE包的传输，是专注于统计会话中发送BYE包的人数的。） 这允许BYE包被立即发送，并控制总的带宽使用。在最坏情况下上，这可能会使RTCP控制包使用两倍于正常水平的带宽，达到10%――其中5%给BYE包的RTCP包，其余5%给BYE包。 一个参与者若不想用上面的机制进行RTCP包的发送，可以直接离开会话，而根本不发送BYE包。他会被其他参与者因超时而删除。 一个参与者想离开会话时，如果组中的人数会计数目小于50，则参与者可以直接发送BYE包。 另外，一个从未发送过RTP或RTCP包的参与者，在离开会话时，不能发送BYE包。 6.3.8更新we_sent变量 如果一个参与者最近发过RTP包，则变量we_sent值为true,否则为false。相同的机制可以管理发送者中的其他参与者。如果参与者发送了TPT包而此时，其对应的we_sent变量值为false,则就把它自己加到发送者列表中，并设置其we_sent变量为true。6.3.4节中描述的逆向重估算法（reverse reconsideration algorithm）应当被执行。以可能减少发送SR包前的延迟。每次发送一个RTP包，其相应的传输时间都会记录在表中。一般发送者的超时算法应用到参与者自身：从tc-2T时开始，一直没有发送RTP包，则此参与者就从发送者列表中将其自身移除，减少发送者总数，并设置we_sent变量值为false。 6.3.9源描述带宽的分配 这里定义了几种源描述项，强制性的规范名（CNAME）除外。例如，个人姓名（NAME）和电子邮件地址（EMAIL）。它也提供了方法定义新的RTCP包的类型。应用程序在给这些额外信息分配带宽时应额外小心。因为这会降低接收报告及CNAME的发送速率，可能破坏协议发挥作用。建议分配给一个参与者用于传输这些额外信息的带宽不超过总的RTCP带宽的20%。另外，并非所有的源描述项都将包含进每一个应用程序中。包含进应用程序的源描述项应根据其用途分配给相应的带宽百分比。建议不要动态会计这些百分比，而应根据一个源描述项的典型长度将所占带宽的百分比的转化为报告间隔。 例如，一个应用程序可能仅发送CNAME，NAME和EMAIL，而不需要其他项。NAME可能会比EMAIL给予更高的优先级。因为NAME可能会在应用程序的用户界面上持续显示，但EMAIL可能仅仅在需要时才会显示。在每一个RTCP时间间隔内，一个包含CNAME项的SDES包和一个RR包将会被发送。最小的会话时间间隔平均为5秒。每经过3个时间间隔（15秒），一个额外的项将会包含进这个SDES包中。7/8的时间是NAME项，每经过8个这样的间隔（15s8=2min）,将会是EMAIL项。 当多个会话考虑使用一个通用的规范名为每个参与者进行绑定时，如在一个RTP会话组成的多媒体会议中，额外的SDES信息可能只在一次RTP会话中被发送。其余的会话将只发送CNAME。特别，这个办法也应该用在分层编码的多个会话中。 6.4 发送者和接收者报告 RTP接收者利用RTCP报告包提供接收质量反馈。根据接收者是否同时还是发送者，RTCP包采取两种不同的形式。发送者报告(SR)和接收者报告(RR)格式中唯一的不同，除包类型码之外，在于发送者报告包括20字节的发送者信息。 SR包和RR包都包括零到多个接收报告块。针对该接收者发出上一个报告块后接收到RTP包的起始同步源，每个源一个块。报告不发送给CSRC列表中的贡献源。每个接收报告块提供从特定数据源接收到数据的统计信息。由于SR/RR包最多允许31个接收报告块，故可以在最初的SR或RR包之后附加RR包，以包含从上一个报告以来的间隔内收听到的所有源的接收报告。如果数据源太多，致使若把所有的RR包放到同一个RTCP复合包中会超出网络的MTU。那么就在一个周期内选择上面RR包的一部分以不超过MTU。这些RR包的选取应让各个包都有同等的几率被取到。这样在几个发送周期间隔中，对所有的数据源就都发送接收报告了。 以下部分定义了两种报告的格式。如果应用程序需要其他信息，他们可以被扩展。 6.4.1 SR：发送者报告RTCP包 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ header |V=2|P| RC | PT=SR=200 | length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SSRC of sender | +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ sender | NTP timestamp, most significant word | info +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | NTP timestamp, least significant word | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | RTP timestamp | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | sender’s packet count | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | sender’s octet count | +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ report | SSRC_1 (SSRC of first source) | block +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 1 | fraction lost | cumulative number of packets lost | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | extended highest sequence number received | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | interarrival jitter | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | last SR (LSR) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | delay since last SR (DLSR) | +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ report | SSRC_2 (SSRC of second source) | block +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 2 : … : +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ | profile-specific extensions | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 发送者报告包由3部分组成，若定义，可能跟随第4个面向协议的扩展部分。 第一部分，头部，8字节长。该域有以下意义： 版本(V)：2比特 RTP版本识别符，在RTCP包内的意义与RTP包中的相同。此协议中定义的版本号为2。 填充(P)：1比特 若设置填充比特，该RTCP包在末端包含一些附加填充比特，并不是控制信息的基本部分。填充的最后一个比特统计了多少个字节必须被忽略。填充可能会用于需要固定长度块的加密算法。在复合RTCP包中，复合包作为一个整体加密，填料比特只能加在最后一个单个RTCP包的后面。 接收报告块计数(RC)：5比特 该包中所含接收报告块的数目。零值有效。 包类型(PT)：8比特 包含常数200，用以识别这个为SR包。 长度：16比特 该RTCP包的长度减1。其单位是32比特字，包括头和任何填充字节。(偏移量1保证零值有效，避免了在扫描RTCP包长度时可能发生的无限循环，同时以32比特为单位避免了对以4为倍数的有效性检测。) SSRC：32比特 SR包发送者的同步源标识符。 第二部分，发送者信息，20字节长。在每个发送者报告包中出现。它概括了从此发送者发出的数据传输情况。此域有以下意义： NTP时间戳：64比特 指示了此报告发送时的背景时钟（wallclock）时刻，它可以与从其它接收者返回的接收报告块中的时间标志结合起来，计算往返每个接收者所花的时间。接收者应让NTP时间戳的精度远大于其他时间戳的精度。时间戳测量的不确定性不可知，因此也无需指示。一个系统可能没有背景时钟的概念，而只有系统指定的时钟，如系统时间（system uptime）。在这样的系统中，此时钟可以作为参考计算相对NTP时间戳。选择一个公用的时名是非常重要的。这样多个独立的应用都可以使用相同的时钟。到2036年，相对和绝对NTP时间戳会产生大的差异。到那时，我们希望不再需要相对时钟。一个发送者，如果不用背景时钟时间或逝去时间，可以设置此项为零。 RTP时间戳：32比特 与以上的NTP时间标志对应同一时刻。与数据包中的RTP时间戳具有相同的单位和偏移量。这个一致性可以用来让NTP时间标志已经同步的源之间进行媒体内/间同步，还可以让与媒体无关的接收者估计名义RTP时钟频率。注意在大多数情况下此时间戳不等于任何临近的RTP包中的时间戳。RTP时间戳可以由相应的NTP时间戳计算得到。依据的是“RTP时间戳计数器”和“在采样时通过周期性检测背景时钟时间得到的实际时间”两者之间的关系。 （在RTCP SR包中有NTP时间戳、RTP时间戳，它们可以计算背景时钟和RTP时钟之间的对应关系，通过这个关系，可以由RTP数据包中的RTP时间戳计算也相应的回放时刻。这样就可以进行多个流的同步了。之所以要有NTP时间戳，是因为不同流的RTP时间戳有不同的随机偏移量，无法直接进行同步：笔者注。） 发送的报文数：32比特 从开始传输到此SR包产生时该发送者发送的RTP数据包总数。若发送者改变SSRC识别符，该计数器重设。 发送的字节文数：32比特 从开始传输到此SR包产生时该发送者在RTP数据包发送的字节总数(不包括头和填充)。若发送者改变SSRC识别符，该计数器重设。此域可以用来估计平均的负载数据发送速率。 第三部分：零到多个接收报告块。块数等于从上一个报告以来该发送者侦听到的其它源（不包括自身）的数目。每个接收报告块传输从某个同步源来的数据包的接收统计信息。若数据源因冲突而改变其SSRC标识符，接收者重新设置统计信息。这些统计信息有： SSRC_n(同步源标识符)：32比特 在此接收报告块中信息所属源的SSRC标识符。 丢包率：8比特 自从前一SR包或RR包发送以来，从SSRC_n传来的RTP数据包的丢失比例。以定点小数的形式表示。该值定义为损失包数／期望接收的包数。若由于包重复而导致包丢失数为负值，丢包率设为零。注意在收到上一个包后，接收者无法知道以后的包是否丢失。如：若在上一个接收报告间隔内从某个源发出的所有数据包都丢失，那么将不为此数据源发送接收报告块。 累计包丢失数：24比特 从开始接收到现在，从源SSRC_n发到本源的RTP数据包的丢包总数。该值定义为：期望接收的包数－实际接收的包数。接收的包括复制的或迟到的。由于迟到的包不算作损失，在发生复制时丢包数可能为负值。期望接收的包数定义为：扩展的上一接收序号(随后定义)减去最初接收序号。 接收到的扩展的最高序列号：32比特 低16比特包含从源SSRC_n来的最高接收序列号，高16比特用相应的序列号周期计数器扩展该序列号。注意在同一会议中的不同接收者，若启动时间明显不同，将产生不同的扩展项。 到达间隔抖动：32比特 RTP数据包到达时刻统计方差的估计值。测量单位同时间戳单位，用无符号整数表达。到达时间抖动定义为一对包中接收者相对发送者的时间间隔差值的平均偏差(平滑后的绝对值)。如以下等式所示，该值等于两个包相对传输时间的差值。相对传输时间是指：包的RTP时间戳和到达时刻接收者时钟时间的差值。若Si是包i中的RTP时间戳，Ri是包i到达时刻（单位为：RTP时间戳单位）。对于两个包i和j，D可以表示为 D(i，j)=(Rj-Sj)-(Ri-Si)； 到达时刻抖动可以在收到从源SSRC_n来的每个数据包i后连续计算。利用该包和前一包i-1的偏差D(按到达顺序，而非序号顺序)，根据公式J=J+(|D(i-1，i)|-J)/16计算。无论何时发送接收报告，都用当前的J值。 此处描述的抖动计算允许与协议独立的监视器对来自不同实现的报告进行有效的解释。 上一SR报文 (LSR)：32比特 接收到的来自源SSRC_n的最新RTCP发送者报告(SR)的64位NTP时间标志的中间32位。若还没有接收到SR，该域值为零。 自上一SR的时间(DLSR)：32比特 是从收到来自SSRC_n的SR包到发送此接收报告块之间的延时，以1/65536秒为单位。若还未收到来自SSRC_n的SR包，该域值为零。 假设SSRC_r为发出此接收报告块的接收者。源SSRC_n可以通过记录收到此接收报告块的时刻A来计算到SSRC_r的环路传输时延。可以利用最新的SR时间标志(LSR)计算整个环路时间A-LSR，然后减去此DLSR域得到环路传输的时延。 如下图所示。 [10 Nov 1995 11:33:25.125 UTC] [10 Nov 1995 11:33:36.5 UTC] n SR(n) A=b710:8000 (46864.500 s) —————————————————————-&gt; v ^ ntp_sec =0xb44db705 v ^ dlsr=0x0005:4000 ( 5.250s) ntp_frac=0x20000000 v ^ lsr =0xb705:2000 (46853.125s) (3024992005.125 s) v ^ r v ^ RR(n) —————————————————————-&gt; |&lt;-DLSR-&gt;| (5.250 s) A 0xb710:8000 (46864.500 s) DLSR -0x0005:4000 ( 5.250 s) LSR -0xb705:2000 (46853.125 s) delay 0x0006:2000 ( 6.125 s) 图2: 往返路程时间的计算举例 可以用此来近似测量到一组接收者的距离，尽管有些连接可能有非常不对称的时延。 6.4.2 RR：接收者报告包 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ header |V=2|P| RC | PT=RR=201 | length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SSRC of packet sender | +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ report | SSRC_1 (SSRC of first source) | block +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 1 | fraction lost | cumulative number of packets lost | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | extended highest sequence number received | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | interarrival jitter | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | last SR (LSR) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | delay since last SR (DLSR) | +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ report | SSRC_2 (SSRC of second source) | block +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 2 : … : +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ | profile-specific extensions | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 接收者报告包(RR)与发送者报告包基本相同，除了包类型域包含常数201和没有发送者信息的5个字(NTP和RTP时间标志和发送者包和字节计数)。余下区域与SR包意义相同。若没有发送和接收据报告，在RTCP复合包头部加入空的RR包(RC=0)。 6.4.3发送者和接收者报告扩展 如果有额外的关于发送者和接收者的信息要周期性的，描述文件（profile）应该定义接收者报告和发送者报告描述文件扩展。此时，应采用这里的办法，而不是定义另外的RTCP包。因为这种办法需要的头部信息更少。 扩展部分是发送报告包和接收报告包的第四部分。如果有的话，应紧跟在接收报告块的后面。如果需要更多的发送者信息，它应当跟在发送者报告的开关，而不应在报告中出现。如果要包含进接收者的信息，它应该以块数组的方式放到接收报告块的后面。即这些块也应被计入RC字段中。 6.4.4分析发送者和接收者报告 接收质量反馈不仅对发送者有用，而且对于其它接收者和第三方监视器也有作用。发送者可以基于反馈修正发送信息量；接收者可以判断问题是本地的，区域内的还是全局的；网络管理者可以利用与协议无关的监视器(只接收RTCP包而不接收相应的RTP包)去评估多点传送网络的性能。 在发送者信息和接收者报告块中都连续统计丢包数，因此可以计算任何两个报告块中的差别。在短时间和长时间内都可以进行测算。最近收到的两个包之间差值可以评估当前传输质量。包中有NTP时间戳，可以用两个报告间隔的差值计算传输速率。由于此时间间隔与数据编码速率独立，因此可以实现与编码及协议独立的质量监视。 一个例子是计算两个报告间隔时间内的丢包率。丢包率＝此间隔内丢失的包／此间隔内期望收到的包。如果此值与“丢失比例”字段中的值相同，说明包是连续的；若否，说明包不是连续的。间隔时间内的丢包率／间隔时间＝每秒的丢包率。 从发送者信息中，第三方监视器可以在一个时间间隔内计算平均负载数据发送速率和平均发包速率，而无需考虑数据接收。两个值的比就是平均负载大小（平均每个包的负载大小）。（即：平均负载大小＝平均负载数据发送速率／平均发包率。）若能假定丢包与包的大小无关，那么某个特定接收者收到的包数乘以平均负载大小(或相应的包大小)就得出接收者可得到的外在吞吐量。 除了累计计数允许利用报告间差值进行长期包损测量外，单个报告的“丢包比例”字段提供一个短时测量数据。当会话规模增加到无法为所有接收者保存接收状态信息，或者报告间隔变得足够长以至于从一个特定接收者只能收到一个报告时，短时测量数据变得更重要。 到达间隔抖动字段提供另一个有关网络阻塞的短时测量量。丢包反映了长期阻塞，抖动测量反映出短时间的阻塞。抖动测量可以在导致丢包前预示阻塞。由于到达间隔抖动字段仅仅是发送报告时刻抖动的一个快照，因此需要在一个网络内在一段时间内分析来自某个接收者的报告，或者分析来自多个接收者的报告。 6.5源描述RTCP包 源描述（SDES）包由一个头及0个或多个块组成。每个块都由块中所标识的数据源的标识符及其后的各个描述构成。 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ header |V=2|P| SC | PT=SDES=202 | length | +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ chunk | SSRC/CSRC_1 | 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SDES items | | … | +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ chunk | SSRC/CSRC_2 | 2 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SDES items | | … | +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ 6.6 BYE（BYE包） 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |V=2|P| SC | PT=BYE=203 | length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | SSRC/CSRC | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ : … : +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+ (opt) | length | reason for leaving … +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ BYE包表明一个或多个源将要离开。如果混合器收到BYE包，混合器应当发送这个BYE包，并保持SSRC/CSRC不变。如果混合器关闭，应向贡献源列表中的所有SSRC，包括它自己的SSRC发送BYE包。BYE包可能会有选择的包含8个字节的统计字段，其后跟上几个字节的文本表明离开的原因。文本字符串编码格式和SDES中描述的相同。 9安全性 底层协议将最终提供由RTP应用要求的所有安全服务，包括真实性、完整性、保密性。这些服务在参考文献[27]中的IP协议有详细描述。由于使用RTP的初始音频和视频应用在IP层可用之前就要求保密性服务，因此，随后的一小节描述了使用RTP和RTCP的保密性服务。新的RTP应用可以实现这里描述的RTP保密性服务，以用于向后兼容，也可以实现替代这里的安全服务。这种安全服务的RTP开销是比较小的。因此，如果这项服务被将来的某种服务所替代，代价也是比较小的。 另一方面，RTP的其他服务，服务的其他实现及其他的算法可能会在将来定义。特别是为RTP负载提供可靠性的实时安全传输协议（ Secure Real-time Transport, SRTP）正在制定中。它可以使RTP头部不被加密。这样，链路层的头部压缩算法可以继续使用。SRTP基于高级企业标准（Advanced Encryption Standard, AES）制定。它比这里描述的服务提供更强健的安全性。 密钥和证书分配超出了本文的范围。 9.1 保密性 保密性意味着只有特定的接收者才能够对收到的包进行解码；对其他人，包里含有的都是无用信息。内容的保密性通过加密来实现。 当用这节指定的方法RTP、RTCP加密时，为了传输而封装的所有字节将在底层的包中作为一个单元加密。对RTCP，每个单元在加密之前必须在前面附加一个32字节的随机数。对RTP，不必在前面加前缀，而是让序列号和时间戳字段都用随机偏移量初始化。由于较差的随机性质。这其实是一个弱的初始化向量（initialization vector, IV）。另外，如果其后的SSRC字段被攻击者得到，则加密算法将出现新的薄弱点。 对RTCP，一个应用程序可能将RTCP复合包中的一个RTCP包分割成两个RTCP复合包。其中，一个在发送时加密，另一个发送时不加密。例如，SDES信息可能会被加密，但接收者报告却不加密，以适用于没有密钥的第三方监视者。如图4所示。源描述信息后必须附加没有报告的空RR包，以满足所有RTCP复合包必须以SR或RR包开头的要求。SDES的CNAME字段包含在加密或未加密的包中之一即可，但并不都需要包含。相同的源描述信息不应在两个包中都携带。否则会使加密算法不安全。 UDP packet UDP packet [random][RR][SDES #CNAME …] [SR #senderinfo #site1 #site2] encrypted not encrypted #: SSRC identifier 图4: 加密的和未加密的RTCP包 接收者加密的使用和正确密钥的使用通过头或负载的有效性检查进行确认。RTP和RTCP头的有效性检查由附录A.1和A.2给出。 为和RFC1889中RTP初始描述中的实现相一致。默认的算法是链式加密块模式（cipher block chaining (CBC) mode）下的数据加密算法，见RFC1423中1.1节的描述。除非出现由5.1节描述指明的填充多个字节的情况，否则，初始的随机向量是0，因为随机值由RTP头或RTCP复合包的随机前缀提供。CBC初始向量的细节见参考文献[30]。支持本节的加密算法的实现也应当支持CBC下的DES算法。因为此算法可实现最大程度的交互可操作性。采用这种方法的原因是，因特网上通过音频、视频工具做实验证明它简便且有效。但DES被发现很容易被破解。建议用更强健的加密算法，例如三层DES加密算法来代替默认的加密算法。另外，安全CBC模式要求每个包的第一个块和一个随机数求异或。对于RTCP，这通过在每个包前附加一个32位的随机数实现。每个包的随机数相互独立。对RTP，时间戳和序列号将从附加的数值开始，但对连续的包，它们并不是被独立的随机化的。应该注意到对RTP和RTCP，这种随机性都受到了限制。高安全性的应用应当考虑其他更加简捷安全的方法。其他加密算法应通过非RTP方法对一个会话动态指定。特别是基于AES的SRTP描述文件（见参考文献[23]）将会是未来的一个不错的选择。以上描述了IP层或RTP层加密。作为它的替代，描述文件可以定义另外的负载类型以用于加密、编码。这些编码必须描述如何填充，以及编码的其他方面如何控制。这种方法可以按照应用的要求，只加密数据，不加密头部。这可能对同时处理解密和解码的硬件服务特别重要。这也可能对RTP和底层头部的链路层的应用很有用。既然头部的加密已经进行了压缩，负载（而不是地址）的保密性就足够了。 9.2 真实性和信息完整性 真实性和信息完整性没有在RTP层定义，因为这些服务离不开密钥管理体系。可以期望真实性和信息完整性将由底层协议完成。 10 拥塞控制 因特网上的所有传输协议都需要通过一些方法进行地址拥塞控制（见参考文献[31]），RTP也不例外。但由于RTP数据传输经常缺少弹性（以固定的或控制好的速率产生包）。因此，RTP的拥塞控制方法和其他的传输协议，如TCP很不相同。在某种程度上，缺乏弹性意味着降低了拥塞的风险。因为RTP流不会像TCP流那样增长到消耗掉所有可用的带宽程度。但是，缺乏弹性也意味着RTP流不能任意减小它在网络上的负载量，以在出现拥塞时消除之。 由于RTP可能会在许多不同的情况下用于相当广的。因此就没有一个全都通用一个拥塞控制机制。因此，拥塞控制应当在描述文件中定义。对于某些描述，可能加上可应用性陈述以限制描述应用在已设计消除拥塞的环境中。对其它描述，可能需要特别的方法，如基于RTCP反馈的自适应数据传输速率。 参考文献： 正式参考文献 [1] Schulzrinne, H. and S. Casner, “音频和视频会议最小控制的RTP描述”, RFC 3551, 2003.6 [2] Bradner, S., “表示需求层的RFC关键字”, BCP 14, RFC 2119, 1997.3 [3] Postel, J., “网络协议”, STD 5, RFC 791, 1981.9 [4] Mills, D., “网络时间协议（第三版）描述、实现和分析”, RFC 1305,1992.3 [5] Yergeau, F., “UTF-8, 一个ISO 10646传输格式”, RFC 2279,1998.1 [6] Mockapetris, P., “域名――概念和工具”, STD 13, RFC 1034,1987.11 [7] Mockapetris, P., “域名――实现和描述”, STD 13, RFC 1035,1987.1 [8] Braden, R., “因特网主机需求――应用和支持”, STD 3, RFC 1123,1989.10 [9] Resnick, P., “因特网信息格式”, RFC 2822,2001.4 非正式参考文献 [10] Clark, D. and D. Tennenhouse, “新一代协议的建构考虑,” 关于通信体系结构和协议的数据通信专业组讨论班, (宾夕法尼亚州，费城), IEEE 计算机通信回顾 卷. 20(4), 200－208页,1990.9 [11] Schulzrinne, H., “关于设计音频、视频会话传输协议及其它多参与者实时应用的讨论”, 1993.10 [12] Comer, D., TCP/IP网络协议 ,卷1. Englewood Cliffs, New Jersey: Prentice Hall, 1991. [13] Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston, A.,Peterson, J., Sparks, R., Handley, M. and E. Schooler, “SIP:会话初始协议”, RFC 3261,2002.6 [14] International Telecommunication Union, “对不保证质量的局域网的可视电话系统和设备”, Recommendation H.323,ITU的无线电通讯标准一节, Geneva, Switzerland, 2003.7 [15] Handley, M. and V. Jacobson, “SDP: 会话描述协议”, RFC 2327,1998.4 [16] Schulzrinne, H., Rao, A. and R. Lanphier, “实时流协议(RTSP)”, RFC 2326,1998.4 [17] Eastlake 3rd, D., Crocker, S. and J. Schiller, “关于安全性的随机化建议”, RFC 1750, 1994.12 [18] Bolot, J.-C., Turletti, T. and I. Wakeman, “因特网多播视频分布的可升级的反馈控制”,关于通信体系结构和协议的数据通信专业组讨论班（英国，伦敦）, ACM,58—67页, 1994.8 [19] Busse, I., Deffner, B. and H. Schulzrinne, “基于RTP的多媒体应用的动态 QoS控制”, 计算机通讯,卷19,49—58页,1996.1 [20] Floyd, S. and V. Jacobson, “周期性路由信息的同步”,关于通信体系结构和协议的数据通信专业组讨论班 (旧金山,加利福尼亚), 33—44页, ACM,1993.9 并参见[34]. [21] Rosenberg, J. and H. Schulzrinne, “RTP中成员组的采样”, RFC 2762,2000.2 [22] Cadzow, J., “纽约数字信号处理和数据分析基础” 纽约: Macmillan, 1987. [23] Hinden, R. and S. Deering, “IPv6地址结构”, RFC 3513,2003.4 [24] Rekhter, Y., Moskowitz, B., Karrenberg, D., de Groot, G. and E.Lear, “保密因特网中的地址分配”, RFC 1918,1996.2 [25] Lear, E., Fair, E., Crocker, D. and T. Kessler, “考虑可能有害的网络10 (一些实现不应成为标准)”, RFC 1627,1994.7 [26] Feller, W.,概率论及其应用入门,卷1. 纽约: John Wiley and Sons , 1968. [27] Kent, S. and R. Atkinson, “因特网协议的安全体系”, RFC 2401,1998.11 [28] Baugher, M., Blom, R., Carrara, E., McGrew, D., Naslund, M.,Norrman, K. and D. Oran, “安全实时传输协议”,2003.4 [29] Balenson, D., “增强因特网电子邮件的保密性:第三部分”, RFC 1423,1993.2 [30] Voydock, V. and S. Kent, “高层网络协议的安全机制”, ACM 计算调查,卷15,135-171页,1983.6 [31] Floyd, S., “拥塞控制原理”, BCP 41, RFC 2914,2000.9 [32] Rivest, R., “MD5通讯――算法摘要”, RFC 1321,1992.4 [33] Stubblebine, S., “多媒体会话的安全服务”, 第16届国际安全会议，(巴尔的摩,马里兰州),391—395页,1993.9 [34] Floyd, S. and V. Jacobson, “周期路由信息同步”, IEEE/ACM 网络传输,卷2,122—136页,1994.4]]></content>
      <categories>
        <category>RFC</category>
      </categories>
      <tags>
        <tag>协议</tag>
        <tag>RTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc-source-android]]></title>
    <url>%2F2017%2F07%2F15%2Fwebrtc-source-android%2F</url>
    <content type="text"><![CDATA[nativeCreateVideoSource 初始化PeerConnectionFactory(pc/peerconnectionfactory) 创建PeerConnection方法中: 12345678910111213141516171819202122232425262728293031rtc::scoped_refptr&lt;PeerConnectionInterface&gt;PeerConnectionFactory::CreatePeerConnection( const PeerConnectionInterface::RTCConfiguration&amp; configuration, std::unique_ptr&lt;cricket::PortAllocator&gt; allocator, std::unique_ptr&lt;rtc::RTCCertificateGeneratorInterface&gt; cert_generator, PeerConnectionObserver* observer) &#123; RTC_DCHECK(signaling_thread_-&gt;IsCurrent()); if (!cert_generator.get()) &#123; // No certificate generator specified, use the default one. cert_generator.reset( new rtc::RTCCertificateGenerator(signaling_thread_, network_thread_)); &#125; if (!allocator) &#123; allocator.reset(new cricket::BasicPortAllocator( default_network_manager_.get(), default_socket_factory_.get())); &#125; network_thread_-&gt;Invoke&lt;void&gt;( RTC_FROM_HERE, rtc::Bind(&amp;cricket::PortAllocator::SetNetworkIgnoreMask, allocator.get(), options_.network_ignore_mask)); rtc::scoped_refptr&lt;PeerConnection&gt; pc( new rtc::RefCountedObject&lt;PeerConnection&gt;(this)); if (!pc-&gt;Initialize(configuration, std::move(allocator), std::move(cert_generator), observer)) &#123; return nullptr; &#125; return PeerConnectionProxy::Create(signaling_thread(), pc);&#125; 构造PeerConnection对象pc,并调用初始化方法Initialize,Initialize中: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182ool PeerConnection::Initialize( const PeerConnectionInterface::RTCConfiguration&amp; configuration, std::unique_ptr&lt;cricket::PortAllocator&gt; allocator, std::unique_ptr&lt;rtc::RTCCertificateGeneratorInterface&gt; cert_generator, PeerConnectionObserver* observer) &#123; TRACE_EVENT0(&quot;webrtc&quot;, &quot;PeerConnection::Initialize&quot;); if (!allocator) &#123; LOG(LS_ERROR) &lt;&lt; &quot;PeerConnection initialized without a PortAllocator? &quot; &lt;&lt; &quot;This shouldn&apos;t happen if using PeerConnectionFactory.&quot;; return false; &#125; if (!observer) &#123; // TODO(deadbeef): Why do we do this? LOG(LS_ERROR) &lt;&lt; &quot;PeerConnection initialized without a &quot; &lt;&lt; &quot;PeerConnectionObserver&quot;; return false; &#125; observer_ = observer; port_allocator_ = std::move(allocator); // The port allocator lives on the network thread and should be initialized // there. if (!network_thread()-&gt;Invoke&lt;bool&gt;( RTC_FROM_HERE, rtc::Bind(&amp;PeerConnection::InitializePortAllocator_n, this, configuration))) &#123; return false; &#125; // Call must be constructed on the worker thread. factory_-&gt;worker_thread()-&gt;Invoke&lt;void&gt;( RTC_FROM_HERE, rtc::Bind(&amp;PeerConnection::CreateCall_w, this)); session_.reset(new WebRtcSession( call_.get(), factory_-&gt;channel_manager(), configuration.media_config, event_log_.get(), factory_-&gt;network_thread(), factory_-&gt;worker_thread(), factory_-&gt;signaling_thread(), port_allocator_.get(), std::unique_ptr&lt;cricket::TransportController&gt;( factory_-&gt;CreateTransportController( port_allocator_.get(), configuration.redetermine_role_on_ice_restart)),#ifdef HAVE_SCTP std::unique_ptr&lt;cricket::SctpTransportInternalFactory&gt;( new cricket::SctpTransportFactory(factory_-&gt;network_thread()))#else nullptr#endif )); stats_.reset(new StatsCollector(this)); stats_collector_ = RTCStatsCollector::Create(this); // Initialize the WebRtcSession. It creates transport channels etc. if (!session_-&gt;Initialize(factory_-&gt;options(), std::move(cert_generator), configuration)) &#123; return false; &#125; // Register PeerConnection as receiver of local ice candidates. // All the callbacks will be posted to the application from PeerConnection. session_-&gt;RegisterIceObserver(this); session_-&gt;SignalState.connect(this, &amp;PeerConnection::OnSessionStateChange); session_-&gt;SignalVoiceChannelCreated.connect( this, &amp;PeerConnection::OnVoiceChannelCreated); session_-&gt;SignalVoiceChannelDestroyed.connect( this, &amp;PeerConnection::OnVoiceChannelDestroyed); session_-&gt;SignalVideoChannelCreated.connect( this, &amp;PeerConnection::OnVideoChannelCreated); session_-&gt;SignalVideoChannelDestroyed.connect( this, &amp;PeerConnection::OnVideoChannelDestroyed); session_-&gt;SignalDataChannelCreated.connect( this, &amp;PeerConnection::OnDataChannelCreated); session_-&gt;SignalDataChannelDestroyed.connect( this, &amp;PeerConnection::OnDataChannelDestroyed); session_-&gt;SignalDataChannelOpenMessage.connect( this, &amp;PeerConnection::OnDataChannelOpenMessage); configuration_ = configuration; return true;&#125; 调用CreateCall_w创建call对象: 12345678910111213141516void PeerConnection::CreateCall_w() &#123; RTC_DCHECK(!call_); const int kMinBandwidthBps = 30000; const int kStartBandwidthBps = 300000; const int kMaxBandwidthBps = 2000000; webrtc::Call::Config call_config(event_log_.get()); call_config.audio_state = factory_-&gt;channel_manager() -&gt;media_engine()-&gt;GetAudioState(); call_config.bitrate_config.min_bitrate_bps = kMinBandwidthBps; call_config.bitrate_config.start_bitrate_bps = kStartBandwidthBps; call_config.bitrate_config.max_bitrate_bps = kMaxBandwidthBps; call_.reset(webrtc::Call::Create(call_config));&#125; 使用call对象以及PeerConnectionFactory中channelmanager(PeerConnectionFactory中Initialize中创建)构造WebRtcSession对象session,调用Initialize方法初始化session,初始化session槽函数等.session_初始化方法中创建WebRtcSessionDescriptionFactory对象webrtc_session_descfactory. 创建ChannelWebRtcSession::SetLocalDescription: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051bool WebRtcSession::SetLocalDescription(SessionDescriptionInterface* desc, std::string* err_desc) &#123; RTC_DCHECK(signaling_thread()-&gt;IsCurrent()); // Takes the ownership of |desc| regardless of the result. std::unique_ptr&lt;SessionDescriptionInterface&gt; desc_temp(desc); // Validate SDP. if (!ValidateSessionDescription(desc, cricket::CS_LOCAL, err_desc)) &#123; return false; &#125; // Update the initial_offerer flag if this session is the initial_offerer. Action action = GetAction(desc-&gt;type()); if (state() == STATE_INIT &amp;&amp; action == kOffer) &#123; initial_offerer_ = true; transport_controller_-&gt;SetIceRole(cricket::ICEROLE_CONTROLLING); &#125; if (action == kAnswer) &#123; current_local_description_.reset(desc_temp.release()); pending_local_description_.reset(nullptr); current_remote_description_.reset(pending_remote_description_.release()); &#125; else &#123; pending_local_description_.reset(desc_temp.release()); &#125; // Transport and Media channels will be created only when offer is set. if (action == kOffer &amp;&amp; !CreateChannels(local_description()-&gt;description())) &#123; // TODO(mallinath) - Handle CreateChannel failure, as new local description // is applied. Restore back to old description. return BadLocalSdp(desc-&gt;type(), kCreateChannelFailed, err_desc); &#125; // Remove unused channels if MediaContentDescription is rejected. RemoveUnusedChannels(local_description()-&gt;description()); if (!UpdateSessionState(action, cricket::CS_LOCAL, err_desc)) &#123; return false; &#125; if (remote_description()) &#123; // Now that we have a local description, we can push down remote candidates. UseCandidatesInSessionDescription(remote_description()); &#125; pending_ice_restarts_.clear(); if (error() != ERROR_NONE) &#123; return BadLocalSdp(desc-&gt;type(), GetSessionErrorMsg(), err_desc); &#125; return true;&#125; action为offer时CreateChannel创建channels: 123456789101112131415161718192021222324252627282930313233343536373839bool WebRtcSession::CreateChannels(const SessionDescription* desc) &#123; const cricket::ContentGroup* bundle_group = nullptr; if (bundle_policy_ == PeerConnectionInterface::kBundlePolicyMaxBundle) &#123; bundle_group = desc-&gt;GetGroupByName(cricket::GROUP_TYPE_BUNDLE); if (!bundle_group) &#123; LOG(LS_WARNING) &lt;&lt; &quot;max-bundle specified without BUNDLE specified&quot;; return false; &#125; &#125; // Creating the media channels and transport proxies. const cricket::ContentInfo* voice = cricket::GetFirstAudioContent(desc); if (voice &amp;&amp; !voice-&gt;rejected &amp;&amp; !voice_channel_) &#123; if (!CreateVoiceChannel(voice, GetBundleTransportName(voice, bundle_group))) &#123; LOG(LS_ERROR) &lt;&lt; &quot;Failed to create voice channel.&quot;; return false; &#125; &#125; const cricket::ContentInfo* video = cricket::GetFirstVideoContent(desc); if (video &amp;&amp; !video-&gt;rejected &amp;&amp; !video_channel_) &#123; if (!CreateVideoChannel(video, GetBundleTransportName(video, bundle_group))) &#123; LOG(LS_ERROR) &lt;&lt; &quot;Failed to create video channel.&quot;; return false; &#125; &#125; const cricket::ContentInfo* data = cricket::GetFirstDataContent(desc); if (data_channel_type_ != cricket::DCT_NONE &amp;&amp; data &amp;&amp; !data-&gt;rejected &amp;&amp; !rtp_data_channel_ &amp;&amp; !sctp_transport_) &#123; if (!CreateDataChannel(data, GetBundleTransportName(data, bundle_group))) &#123; LOG(LS_ERROR) &lt;&lt; &quot;Failed to create data channel.&quot;; return false; &#125; &#125; return true;&#125; CreateChannels中创建三个Channel,其中CreateVideoChannel创建视频Channel: 123456789101112131415161718192021222324252627282930313233343536373839404142bool WebRtcSession::CreateVideoChannel(const cricket::ContentInfo* content, const std::string* bundle_transport) &#123; bool require_rtcp_mux = rtcp_mux_policy_ == PeerConnectionInterface::kRtcpMuxPolicyRequire; std::string transport_name = bundle_transport ? *bundle_transport : content-&gt;name; cricket::DtlsTransportInternal* rtp_dtls_transport = transport_controller_-&gt;CreateDtlsTransport( transport_name, cricket::ICE_CANDIDATE_COMPONENT_RTP); cricket::DtlsTransportInternal* rtcp_dtls_transport = nullptr; if (!require_rtcp_mux) &#123; rtcp_dtls_transport = transport_controller_-&gt;CreateDtlsTransport( transport_name, cricket::ICE_CANDIDATE_COMPONENT_RTCP); &#125; video_channel_.reset(channel_manager_-&gt;CreateVideoChannel( call_, media_config_, rtp_dtls_transport, rtcp_dtls_transport, transport_controller_-&gt;signaling_thread(), content-&gt;name, SrtpRequired(), video_options_)); if (!video_channel_) &#123; transport_controller_-&gt;DestroyDtlsTransport( transport_name, cricket::ICE_CANDIDATE_COMPONENT_RTP); if (rtcp_dtls_transport) &#123; transport_controller_-&gt;DestroyDtlsTransport( transport_name, cricket::ICE_CANDIDATE_COMPONENT_RTP); &#125; return false; &#125; video_channel_-&gt;SignalRtcpMuxFullyActive.connect( this, &amp;WebRtcSession::DestroyRtcpTransport_n); video_channel_-&gt;SignalDtlsSrtpSetupFailure.connect( this, &amp;WebRtcSession::OnDtlsSrtpSetupFailure); SignalVideoChannelCreated(); video_channel_-&gt;SignalSentPacket.connect(this, &amp;WebRtcSession::OnSentPacket_w); return true;&#125; 调用channel_manager的CreateVideoChannel创建BaseChannel基类的cricket::VideoChannel. VideoChannel需要传入VideoMediaChannel作为构造参数: 1234567891011121314151617181920212223242526272829303132//pc/channelmanager.h/ccVideoChannel* ChannelManager::CreateVideoChannel_w( webrtc::Call* call, const cricket::MediaConfig&amp; media_config, DtlsTransportInternal* rtp_dtls_transport, DtlsTransportInternal* rtcp_dtls_transport, rtc::PacketTransportInternal* rtp_packet_transport, rtc::PacketTransportInternal* rtcp_packet_transport, rtc::Thread* signaling_thread, const std::string&amp; content_name, bool srtp_required, const VideoOptions&amp; options) &#123; RTC_DCHECK(initialized_); RTC_DCHECK(worker_thread_ == rtc::Thread::Current()); RTC_DCHECK(nullptr != call); VideoMediaChannel* media_channel = media_engine_-&gt;CreateVideoChannel( call, media_config, options); if (media_channel == NULL) &#123; return NULL; &#125; VideoChannel* video_channel = new VideoChannel( worker_thread_, network_thread_, signaling_thread, media_channel, content_name, rtcp_packet_transport == nullptr, srtp_required); if (!video_channel-&gt;Init_w(rtp_dtls_transport, rtcp_dtls_transport, rtp_packet_transport, rtcp_packet_transport)) &#123; delete video_channel; return NULL; &#125; video_channels_.push_back(video_channel); return video_channel;&#125; VideoMediaChannel实例media_channel由MediaEngineInterface对象media_engine创建,media_engine由ChannelManager构造方法传入并初始化,ChannelManager由PeerConnectionFactory创建,在PeerConnection初始化方法中,media_engine被创建: 123456789101112131415161718192021222324252627282930313233343536373839404142//pc/peerconnectionfactory.ccbool PeerConnectionFactory::Initialize() &#123; RTC_DCHECK(signaling_thread_-&gt;IsCurrent()); rtc::InitRandom(rtc::Time32()); default_network_manager_.reset(new rtc::BasicNetworkManager()); if (!default_network_manager_) &#123; return false; &#125; default_socket_factory_.reset( new rtc::BasicPacketSocketFactory(network_thread_)); if (!default_socket_factory_) &#123; return false; &#125; std::unique_ptr&lt;cricket::MediaEngineInterface&gt; media_engine = worker_thread_-&gt;Invoke&lt;std::unique_ptr&lt;cricket::MediaEngineInterface&gt;&gt;( RTC_FROM_HERE, rtc::Bind(&amp;PeerConnectionFactory::CreateMediaEngine_w, this)); channel_manager_.reset(new cricket::ChannelManager( std::move(media_engine), worker_thread_, network_thread_)); channel_manager_-&gt;SetVideoRtxEnabled(true); if (!channel_manager_-&gt;Init()) &#123; return false; &#125; return true;&#125;std::unique_ptr&lt;cricket::MediaEngineInterface&gt;PeerConnectionFactory::CreateMediaEngine_w() &#123; RTC_DCHECK(worker_thread_ == rtc::Thread::Current()); return std::unique_ptr&lt;cricket::MediaEngineInterface&gt;( cricket::WebRtcMediaEngineFactory::Create( default_adm_.get(), audio_encoder_factory_, audio_decoder_factory_, video_encoder_factory_.get(), video_decoder_factory_.get(), external_audio_mixer_));&#125; WebRtcMediaEngine2继承自CompositeMediaEngine,CompositeMediaEngine父类MediaEngineInterface有WebRtcVoiceEngine voice与WebRtcVideoEngine2 video两个对象 WebRtcVideoEngine2WebRtcVideoEngine2定义在media/engine/webrtcvideoengine2.h下,用于创建WebRtcVideoChannel2(定义在同一头文件),WebRtcVideoChannel2定义了WebRtcVideoSendStream与WebRtcVideoReceiveStream两个内部类.]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebRtc源码分析(1) PeerConnection]]></title>
    <url>%2F2017%2F07%2F11%2Fwebrtc-source-peerconnection%2F</url>
    <content type="text"><![CDATA[ChannelManagerpc/channelmanager.h 12345678// ChannelManager allows the MediaEngine to run on a separate thread, and takes// care of marshalling calls between threads. It also creates and keeps track of// voice and video channels; by doing so, it can temporarily(暂时的) pause all the// channels when a new audio or video device is chosen. The voice and video// channels are stored in separate vectors, to easily allow operations on just// voice or just video channels.// ChannelManager also allows the application to discover what devices it has// using device manager. WebRtcSession构造中通过MediaControllerInterface初始化ChannelManager变量channelmanager , ChannelManager通过构造传入MediaEngineInterface. WebRtcSessionpc/webrtcsession.h 1234567// A WebRtcSession manages general session state. This includes negotiation// of both the application-level and network-level protocols: the former// defines what will be sent and the latter defines how it will be sent. Each// network-level protocol is represented by a Transport object. Each Transport// participates in the network-level negotiation. The individual streams of// packets are represented by TransportChannels. The application-level protocol// is represented by SessionDecription objects. MediaControllerInterfacepc/mediacontroller.h 123// The MediaController currently owns shared state between media channels.// Abstract interface is defined here such that it can be faked/mocked for// tests, but no other real reason. 实现类MediaController,管理ChannelManager,cricket::ChannelManager* const channel_manager_;,在PeerConnection的Initialize方法中,通过PeerConnectionFactory创建.PeerConnectionFactory中Initialize中真正创建ChannelManager,创建ChannelManager之前,先创建出MediaEngine,实际在PeerConnectionFactory::CreateMediaEngine_w中通过cricket::WebRtcMediaEngineFactory::Create创建. 7&gt; Downloading src/resources/voice_engine/audio_tiny44.wav… 4&gt; Downloading src/resources/voice_engine/audio_tiny48.wav… 2&gt; Downloading src/resources/voice_engine/audio_tiny8.wav… Hook ‘download_from_google_storage –directory –recursive –num_threads=10 –no_auth –quiet –bucket chromium-webrtc-resources src/resources’ took 528.97 secs WARNING: ‘src/testing/gmock’ has been moved from DEPS to a higher level checkout. The git folder containing all the local branches has been saved to /Users/shenjunwei/soft-source/webrtc/old_src_testing_gmock.git. If you don’t care about its state you can safely remove that folder to free up space. WARNING: ‘src/testing/gtest’ has been moved from DEPS to a higher level checkout. The git folder containing all the local branches has been saved to /Users/shenjunwei/soft-source/webrtc/old_src_testing_gtest.git. If you don’t care about its state you can safely remove that folder to free up space.]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于内存对齐那些事]]></title>
    <url>%2F2017%2F07%2F09%2Ftips-about-data-structure-alignment%2F</url>
    <content type="text"><![CDATA[Wrote by mutouyun. 1. 内存对齐（Data Structure Alignment）是什么内存对齐，或者说字节对齐，是一个数据类型所能存放的内存地址的属性（Alignment is a property of a memory address）。 这个属性是一个无符号整数，并且这个整数必须是2的N次方（1、2、4、8、……、1024、……）。 当我们说，一个数据类型的内存对齐为8时，意思就是指这个数据类型所定义出来的所有变量，其内存地址都是8的倍数。 当一个基本数据类型（fundamental types）的对齐属性，和这个数据类型的大小相等时，这种对齐方式称作自然对齐（naturally aligned）。 比如，一个4字节大小的int型数据，默认情况下它的字节对齐也是4。 2. 为什么我们需要内存对齐这是因为，并不是每一个硬件平台都能够随便访问任意位置的内存的。 微软的MSDN里有这样一段话： Many CPUs, such as those based on Alpha, IA-64, MIPS, and SuperH architectures, refuse to read misaligned data. When a program requests that one of these CPUs access data that is not aligned, the CPU enters an exception state and notifies the software that it cannot continue. On ARM, MIPS, and SH device platforms, for example, the operating system default is to give the application an exception notification when a misaligned access is requested. 大意是说，有不少平台的CPU，比如Alpha、IA-64、MIPS还有SuperH架构，若读取的数据是未对齐的（比如一个4字节的int在一个奇数内存地址上），将拒绝访问，或抛出硬件异常。 另外，在维基百科里也记载着如下内容： Data alignment means putting the data at a memory offset equal to some multiple of the word size, which increases the system’s performance due to the way the CPU handles memory. 意思是，考虑到CPU处理内存的方式（32位的x86 CPU，一个时钟周期可以读取4个连续的内存单元，即4字节），使用字节对齐将会提高系统的性能（也就是CPU读取内存数据的效率。比如你一个int放在奇数内存位置上，想把这4个字节读出来，32位CPU就需要两次。但对齐之后一次就可以了）。 3. 内存对齐带来的数据结构大小变化因为有了内存对齐，因此数据在内存里的存放就不是紧挨着的，而是可能会出现一些空隙（Data Structure Padding，也就是用于填充的空白内容）。因此对基本数据类型来说可能还好说，对于一个内部有多个基本类型的结构体（struct）或类而言，sizeof的结果往往和想象中不大一样。 让我们来看一个例子： 12345678struct MyStruct &#123; char a; // 1 byte int b; // 4 bytes short c; // 2 bytes long long d; // 8 bytes char e; // 1 byte &#125;; 我们可以看到，MyStruct中有5个成员，如果直接相加的话大小应该是16，但在32位MSVC里它的sizeof结果是32。 之所以结果出现偏差，为了保证这个结构体里的每个成员都应该在它对齐了的内存位置上，而在某些位置插入了Padding。 下面我们尝试考虑内存对齐，来计算一下这个结构体的大小。首先，我们可以假设MyStruct的整体偏移从0x00开始，这样就可以暂时忽略MyStruct本身的对齐。这时，结构体的整体内存分布如下图所示： 我们可以看到，char和int之间；short和long long之间，为了保证成员各自的对齐属性，分别插入了一些Padding。 因此整个结构体会被填充得看起来像这样： 123456789101112struct MyStruct &#123; char a; // 1 byte char pad_0[3]; // Padding 3 int b; // 4 bytes short c; // 2 bytes char pad_1[6]; // Padding 6 long long d; // 8 bytes char e; // 1 byte char pad_2[7]; // Padding 7 &#125;; 注意到上面加了Padding的示意结构体里，e的后面还跟了7个字节的填充。这是因为结构体的整体大小必须可被对齐值整除，所以“char e”的后面还会被继续填充7个字节好让结构体的整体大小是8的倍数32。 我们可以在gcc + 32位linux中尝试计算sizeof(MyStruct)，得到的结果是24。 这是因为gcc中的对齐规则和MSVC不一样，不同的平台下会使用不同的默认对齐值（The default alignment is fixed for a particular target ABI）。在gcc + 32位linux中，大小超过4字节的基本类型仍然按4字节对齐。因此MyStruct的内存布局这时看起来应该像这个样子： 下面我们来确定这个结构体类型本身的内存对齐是多少。为了保证结构体内的每个成员都能够放在它自然对齐的位置上，对这个结构体本身来说最理想的内存对齐数值应该是结构体里内存对齐数值最大的成员的内存对齐数。 也就是说，对于上面的MyStruct，结构体类型本身的内存对齐应该是8。并且，当我们强制对齐方式小于8时，比如设置MyStruct对齐为2，那么其内部成员的对齐也将被强制不能超过2。 为什么？因为对于一个数据类型来说，其内部成员的位置应该是相对固定的。假如上面这个结构体整体按1或者2字节对齐，而成员却按照各自的方式自然对齐，就有可能出现成员的相对偏移量随内存位置而改变的问题。 比如说，我们可以画一下整个结构体按1字节对齐，并且结构体内的每个成员按自然位置对齐的内存布局： 上面的第一种情况，假设MyStruct的起始地址是0x01（因为结构体本身的偏移按1字节对齐），那么char和int之间将会被填充2个字节的Padding，以保证int的对齐还是4字节。 如果第二次分配MyStruct的内存时起始地址变为0x03，由于int还是4字节对齐，则char和int之间将不会填充Padding（填充了反而不对齐了）。 以此类推，若MyStruct按1字节对齐时不强制所有成员的对齐均不超过1的话，这个结构体里的相对偏移方式一共有4种。 因此对于结构体来说，默认的对齐将等于其中对齐最大的成员的对齐值。并且，当我们限定结构体的内存对齐时，同时也限定了结构体内所有成员的内存对齐不能超过结构体本身的内存对齐。 4. 指定内存对齐在C++98/03里，对内存对齐的操作在不同的编译器里可能有不同的方法。 在MSVC中，一般使用#progma pack来指定内存对齐： 12345678910#pragma pack(1) // 指定后面的内容内存对齐为1 struct MyStruct &#123; char a; // 1 byte int b; // 4 bytes short c; // 2 bytes long long d; // 8 bytes char e; // 1 byte &#125;; #pragma pack() // 还原默认的内存对齐 这时，MyStruct由于按1字节对齐，其中的所有成员都将变为1字节对齐，因此sizeof(MyStruct)将等于16。 还有另外一个简单的方法： 12345678__declspec(align(64)) struct MyStruct &#123; char a; // 1 byte int b; // 4 bytes short c; // 2 bytes long long d; // 8 bytes char e; // 1 byte &#125;; __declspec(align(64))将指定内存对齐为64。比较坑的是，这种方法不能指定内存对齐小于默认对齐，也就是说它只能调大不能调小（__declspec(align(#)) can only increase alignment restrictions）。因此下面这样写会忽略掉declspec： 12__declspec(align(1)) struct MyStruct // ... // warning C4359: &apos;MyStruct&apos;: Alignment specifier is less than actual alignment (8), and will be ignored. 微软的__declspec(align(#))，其#的内容可以是预编译宏，但不能是编译期数值： 12345678#define XX 32 struct __declspec(align(XX)) MyStruct_1 &#123;&#125;; // OK template &lt;size_t YY&gt; struct __declspec(align(YY)) MyStruct_2 &#123;&#125;; // error C2059: syntax error: &apos;identifier&apos; static const unsigned ZZ = 32; struct __declspec(align(ZZ)) MyStruct_3 &#123;&#125;; // error C2057: expected constant expression 在Visual C++ Compiler November 2013 CTP之后，微软终于支持编译期数值的写法了： 123template &lt;size_t YY&gt; struct __declspec(align(YY)) MyStruct_2 &#123;&#125;; // OK in 2013 CTP __declspec(align(#))最大支持对齐为8192（Valid entries are integer powers of two from 1 to 8192）。 下面再来看gcc。gcc和MSVC一样，可以使用#pragma pack： 123456#pragma pack(1) struct MyStruct &#123; // ... &#125;; #pragma pack() 另外，也可以使用__attribute__((__aligned__((#))))： 123456789struct __attribute__((__aligned__((1)))) MyStruct_1 &#123; // ... &#125;; struct MyStruct_2 &#123; // ... &#125; __attribute__((__aligned__((1)))); 这东西写上面写下面都是可以的，但是不能写在struct前面。 和MSVC一样，__attribute__也只能把字节对齐改大，不能改小（The aligned attribute can only increase the alignment）。比较坑的是当你试图改小的时候，gcc没有任何编译提示信息。 gcc可以接受一个宏或编译期数值： 12345678910#define XX 1 struct __attribute__((__aligned__((XX)))) MyStruct_1 &#123;&#125;; // OK template &lt;size_t YY&gt; struct __attribute__((__aligned__((YY)))) MyStruct_2 &#123;&#125;; // OK static const unsigned ZZ = 1; struct __attribute__((__aligned__((ZZ)))) MyStruct_3 &#123;&#125;; // ^ // error: requested alignment is not an integer constant gcc的__attribute__((__aligned__((#))))支持的上限受限于链接器（Note that the effectiveness of aligned attributes may be limited by inherent limitations in your linker）。 5. 获得内存对齐同样的，在C++98/03里，不同的编译器可能有不同的方法来获得一个类型的内存对齐。 MSVC使用__alignof操作符获得内存对齐大小： 123MyStruct xx; std::cout &lt;&lt; __alignof(xx) &lt;&lt; std::endl; std::cout &lt;&lt; __alignof(MyStruct) &lt;&lt; std::endl; gcc则使用__alignof__： 123MyStruct xx; std::cout &lt;&lt; __alignof__(xx) &lt;&lt; std::endl; std::cout &lt;&lt; __alignof__(MyStruct) &lt;&lt; std::endl; 需要注意的是，不论是__alignof还是__alignof__，对于对齐的计算都发生在编译期。因此像下面这样写： 123int a; char&amp; c = reinterpret_cast&lt;char&amp;&gt;(a); std::cout &lt;&lt; __alignof__(c) &lt;&lt; std::endl; 得到的结果将是1。 如果需要在运行时动态计算一个变量的内存对齐，比如根据一个void*指针指向的内存地址来判断这个地址的内存对齐是多少，我们可以用下面这个简单的方法： 1234__declspec(align(128)) long a = 0; size_t x = reinterpret_cast&lt;size_t&gt;(&amp;a); x &amp;= ~(x - 1); // 计算a的内存对齐大小 std::cout &lt;&lt; x &lt;&lt; std::endl; 用这种方式得到的内存对齐大小可能比实际的大，因为它是切实的获得这个内存地址到底能被多大的2^N整除。 6. 堆内存的内存对齐我们在讨论内存对齐的时候很容易忽略掉堆内存。我们经常会使用malloc分配内存，却不理会这块内存的对齐方式，仿佛堆内存不需要考虑内存对齐一样。 实际上，malloc一般使用当前平台默认的最大内存对齐数对齐内存。比如MSVC在32位下一般是8字节对齐；64位下则是16字节（In Visual C++, the fundamental alignment is the alignment that’s required for a double, or 8 bytes. In code that targets 64-bit platforms, it’s 16 bytes）。这样对于常规的数据都是没有问题的。 但是如果我们自定义的内存对齐超出了这个范围，则是不能直接使用malloc来获取内存的。 当我们需要分配一块具有特定内存对齐的内存块时，在MSVC下应当使用_aligned_malloc；而在gcc下一般使用memalign等函数。 其实自己实现一个简易的aligned_malloc是很容易的： 1234567891011121314151617181920212223242526#include &lt;assert.h&gt; inline void* aligned_malloc(size_t size, size_t alignment) &#123; // 检查alignment是否是2^N assert(!(alignment &amp; (alignment - 1))); // 计算出一个最大的offset，sizeof(void*)是为了存储原始指针地址 size_t offset = sizeof(void*) + (--alignment); // 分配一块带offset的内存 char* p = static_cast&lt;char*&gt;(malloc(offset + size)); if (!p) return nullptr; // 通过“&amp; (~alignment)”把多计算的offset减掉 void* r = reinterpret_cast&lt;void*&gt;(reinterpret_cast&lt;size_t&gt;(p + offset) &amp; (~alignment)); // 将r当做一个指向void*的指针，在r当前地址前面放入原始地址 static_cast&lt;void**&gt;(r)[-1] = p; // 返回经过对齐的内存地址 return r; &#125; inline void aligned_free(void* p) &#123; // 还原回原始地址，并free free(static_cast&lt;void**&gt;(p)[-1]); &#125; 7. C++11中对内存对齐的操作C++11标准里统一了内存对齐的相关操作。 指定内存对齐使用alignas说明符： 12345678910alignas(32) long long a = 0; #define XX 1 struct alignas(XX) MyStruct_1 &#123;&#125;; // OK template &lt;size_t YY = 1&gt; struct alignas(YY) MyStruct_2 &#123;&#125;; // OK static const unsigned ZZ = 1; struct alignas(ZZ) MyStruct_3 &#123;&#125;; // OK 注意到MyStruct_3编译是OK的。在C++11里，只要是一个编译期数值（包括static const）都支持alignas（the assignment-expression shall be an integral constant expression，参考ISO/IEC-14882:2011，7.6.2 Alignment specifier，第2款）。 但是需要小心的是，目前微软的编译器（Visual C++ Compiler November 2013 CTP）在MyStruct_3的情况下仍然会报error C2057。 另外，alignas同前面介绍的__declspec、__attribute__一样，只能改大不能改小（参考ISO/IEC-14882:2011，7.6.2 Alignment specifier，第5款）。如果需要改小，比如设置对齐为1的话，仍然需要使用#pragma pack。或者，可以使用C++11里#pragma的等价物_Pragma（微软暂不支持这个）： 12345678910_Pragma(&quot;pack(1)&quot;) struct MyStruct &#123; char a; // 1 byte int b; // 4 bytes short c; // 2 bytes long long d; // 8 bytes char e; // 1 byte &#125;; _Pragma(&quot;pack()&quot;) 除了这些之外，alignas比__declspec、这个char就按int的方式对齐了。 获取内存对齐使用alignof操作符：强大的地方在于它还可以这样用： 1alignas(int) char c; 这个char就按int的方式对齐了。 获取内存对齐使用alignof操作符： 123MyStruct xx; std::cout &lt;&lt; alignof(xx) &lt;&lt; std::endl; std::cout &lt;&lt; alignof(MyStruct) &lt;&lt; std::endl; 相关注意点和前面介绍的__alignof、__alignof__并无二致。 除了alignas和alignof，C++11中还提供了几个有用的工具。 A. std::alignment_of 功能是编译期计算类型的内存对齐。 std里提供这个是为了补充alignof的功能。alignof只能返回一个size_t，而alignment_of则继承自std::integral_constant，因此拥有value_type、type、operator()等接口（或者说操作）。 B. std::aligned_storage 这是个好东西。我们知道，很多时候需要分配一块单纯的内存块，比如new char[32]，之后再使用placement new在这块内存上构建对象： 12char xx[32]; ::new (xx) MyStruct; 但是char[32]是1字节对齐的，xx很有可能并不在MyStruct指定的对齐位置上。这时调用placement new构造内存块，可能会引起效率问题或出错，这时我们应该使用std::aligned_storage来构造内存块： 12std::aligned_storage&lt;sizeof(MyStruct), alignof(MyStruct)&gt;::type xx; ::new (&amp;xx) MyStruct; 需要注意的是，当使用堆内存的时候我们可能还是需要aligned_malloc。因为现在的编译器里new并不能在超出默认最大对齐后，还能保证内存的对齐是正确的。比如在MSVC 2013里，下面的代码： 1234567891011struct alignas(32) MyStruct &#123; char a; // 1 byte int b; // 4 bytes short c; // 2 bytes long long d; // 8 bytes char e; // 1 byte &#125;; void* p = new MyStruct; // warning C4316: &apos;MyStruct&apos; : object allocated on the heap may not be aligned 32 将会得到一个编译警告。 C. std::max_align_t 返回当前平台的最大默认内存对齐类型。malloc返回的内存，其对齐和max_align_t类型的对齐大小应当是一致的。 我们可以通过下面这个方式获得当前平台的最大默认内存对齐数： 12std::cout &lt;&lt; alignof(std::max_align_t) &lt;&lt; std::endl; D. std::align 这货是一个函数，用来在一大块内存当中获取一个符合指定内存要求的地址。 看下面这个例子： 1234char buffer[] = &quot;------------------------&quot;; void * pt = buffer; std::size_t space = sizeof(buffer) - 1; std::align(alignof(int), sizeof(char), pt, space); 意思是，在buffer这个大内存块中，指定内存对齐为alignof(int)，找一块sizeof(char)大小的内存，并在找到这块内存后，将地址放入pt，将buffer从pt开始的长度放入space。 关于这个函数的更多信息，可以参考这里。 关于内存对齐，该说的就是这么多了。我们经常会看到内存对齐的应用，是在网络收发包中。一般用于发送的结构体，都是1字节对齐的，目的是统一收发双方（可能处于不同平台）之间的数据内存布局，以及减少不必要的流量消耗。 C++11中为我们提供了不少有用的工具，可以让我们方便的操作内存对齐。但是在堆内存方面，我们很可能还是需要自己想办法。不过在平时的应用中，因为很少会手动指定内存对齐到大于系统默认的对齐数，所以倒也不比每次new/delete的时候都提心吊胆。 参考文章： Data structure alignment About Data Alignment #pragma pack align (C++) __alignof Operator 6.57.8 Structure-Packing Pragmas 5.32 Specifying Attributes of Types C/C++ Data alignment 及 struct size深入分析 C++ 内存对齐 结构/类对齐的声明方式 字节对齐（强制对齐以及自然对齐） malloc函数字节对齐很经典的问题 C语言字节对齐 网络编程(9)内存对齐对跨平台通讯的影响 Usage Issue of std::align std::align and std::aligned_storage for aligned allocation of memory blocks http://www.cnblogs.com/fangkm/p/4370492.html]]></content>
      <categories>
        <category>Memory</category>
      </categories>
      <tags>
        <tag>tips</tag>
        <tag>Memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebRTC的模块处理机制]]></title>
    <url>%2F2017%2F07%2F06%2Fwebrtc-modules%2F</url>
    <content type="text"><![CDATA[对于实时音视频应用来讲，媒体数据从采集到渲染，在数据流水线上依次完成一系列处理。流水线由不同的功能模块组成，彼此分工协作：数据采集模块负责从摄像头/麦克风采集音视频数据，编解码模块负责对数据进行编解码，RTP模块负责数据打包和解包。数据流水线上的数据处理速度是影响应用实时性的最重要因素。与此同时，从服务质量保证角度讲，应用需要知道数据流水线的运行状态，如视频采集模块的实时帧率、当前网络的实时速率、接收端的数据丢包率，等等。各个功能模块可以基于这些运行状态信息作相应调整，从而在质量、速度等方面优化数据流水线的运行，实现更快、更好的用户体验。 WebRTC采用模块机制，把数据流水线上功能相对独立的处理点定义为模块，每个模块专注于自己的任务，模块之间基于数据流进行通信。与此同时，专有线程收集和处理模块内部的运行状态信息，并把这些信息反馈到目标模块，实现模块运行状态监控和服务质量保证。本文在深入分析WebRTC源代码基础上，学习研究其模块处理机制的实现细节，从另一个角度理解WebRTC的技术原理。 1 WebRTC数据流水线我们可以把WebRTC看作是一个专注于实时音视频通信的SDK。其对外的API主要负责PeerConnection建立、MediaStream创建、NAT穿透、SDP协商等工作，对内则主要集中于音视频数据的处理，从数据采集到渲染的整个处理过程可以用一个数据流水线来描述，如图1所示。 音视频数据首先从采集端进行采集，一般来说音频数据来自麦克风，视频数据来自摄像头。在某些应用场景下，音频数据来自扬声器，视频数据来自桌面共享。采集端的输出是音视频Raw Data。然后Raw Data到达编码模块，数据被编码器编码成符合语法规则的NAL单元，到达发送端缓冲区PacedSender处。接下来PacedSender把NAL单元发送到RTP模块打包为RTP数据包，最后经过网络模块发送到网络。 在接收端，RTP数据包经过网络模块接收后进行解包得到NAL单元，接下来NAL单元到达接收端缓冲区(JitterBuffer或者NetEQ)进行乱序重排和组帧。一帧完整的数据接收并组帧之后，调用解码模块进行解码，得到该帧数据的Raw Data。最后Raw Data交给渲染模块进行播放/显示。 在数据流水线上，还有一系列模块负责服务质量监控，如丢帧策略，丢包策略，编码器过度使用保护，码率估计，前向纠错，丢包重传，等等。 WebRTC数据流水线上的功能单元被定义为模块，每个模块从上游模块获取输入数据，在本模块进行加工后得到输出数据，交给下游模块进行下一步处理。WebRTC的模块处理机制包括模块和模块处理线程，前者把WebRTC数据流水线上的功能部件封装为模块，后者驱动模块内部状态更新和模块之间状态传递。模块一般挂载到模块处理线程上，由处理线程驱动模块的处理函数。下面分别描述之。 WebRTC模块WebRTC模块虚基类Module定义在webrtc/modules/include/modue.h中，如图2所示。 Module虚基类对外提供三个函数作为API：TimeUntilNextProcess()用来计算距下次调用处理函数Process()的时间间隔；Process()是模块的处理函数，负责模块内部运行监控、状态更新和模块间通信；ProcessThreadAttached()用来把模块挂载到模块处理线程，或者从模块处理线程分离出来，实际实现中这个函数暂时没有被用到。 Module的派生类分布在WebRTC数据流水线上的不同部分，各自承担自己的数据处理和服务质量保证任务。 3 WebRTC模块处理线程WebRTC模块处理线程是模块处理机制的驱动器，它的核心作用是对所有挂载在本线程下的模块，周期性调用其Process()处理函数处理模块内部事务，并处理异步任务。其虚基类ProcessThread和派生类ProcessThreadImpl如图3所示。 ProcessThread基类提供一系列API完成线程功能：Start()/Stop()函数用来启动和结束线程；WakeUp()函数用来唤醒挂载在本线程下的某个模块，使得该模块有机会马上执行其Process()处理函数；PostTask()函数用来邮递一个任务给本线程；RegisterModule()和DeRegisterModule()用来向线程注册/注销模块。 WebRTC基于ProcessThread线程实现派生类ProcessThreadImpl，如图3所示。在成员变量方面，wakeup用来唤醒处于等待状态的线程；thread是平台相关的线程实现如POSIX线程；modules是注册在本线程下的模块集合；queue_是邮递给本线程的任务集合；threadname是线程名字。在成员函数方面，Process()完成ProcessThread的核心任务，其伪代码如下所示。 1234567891011121314151617181920212223bool ProcessThreadImpl::Process() &#123; for (ModuleCallback&amp; m : modules_) &#123; if (m.next_callback == 0) m.next_callback = GetNextCallbackTime(m.module, now); if (m.next_callback &lt;= now || m.next_callback == kCallProcessImmediately) &#123; m.module-&gt;Process(); m.next_callback = GetNextCallbackTime(m.module, rtc::TimeMillis();); &#125; if (m.next_callback &lt; next_checkpoint) next_checkpoint = m.next_callback; &#125; while (!queue_.empty()) &#123; ProcessTask* task = queue_.front(); queue_.pop(); task-&gt;Run(); delete task; &#125; &#125; int64_t time_to_wait = next_checkpoint - rtc::TimeMillis(); if (time_to_wait &gt; 0) wake_up_-&gt;Wait(static_cast&lt;unsigned long&gt;(time_to_wait)); return true;&#125; Process()函数首先处理挂载在本线程下的模块，这也是模块处理线程的核心任务：针对每个模块，计算其下次调用模块Process()处理函数的时刻(调用该模块的TimeUntilNextProcess()函数)。如果时刻是当前时刻，则调用模块的Process()处理函数，并更新下次调用时刻。需要注意，不同模块的执行频率不一样，线程在本轮调用末尾的等待时间和本线程下所有模块的最近下次调用时刻相关。 接下来线程Process()函数处理ProcessTask队列中的异步任务，针对每个任务调用Run()函数，然后任务出队列并销毁。等模块调用和任务都处理完后，则把本次时间片的剩余时间等待完毕，然后返回。如果在等待期间其他线程向本线程Wakeup模块或者邮递一个任务，则线程被立即唤醒并返回，进行下一轮时间片的执行。 至此，关于WebRTC的模块和模块处理线程的基本实现分析完毕，下一节将对WebRTC SDK内模块实例和模块处理线程实例进行详细分析。 4 WebRTC模块处理线程实例WebRTC关于模块和处理线程的实现在webrtc/modules目录下，该目录汇集了所有派生类模块和模块处理线程的实现及实例分布。本节对这些内容进行总结。 WebRTC目前创建三个ProcessThreadImpl线程实例，分别是负责处理音频的VoiceProcessTread，负责处理视频和音视频同步的ModuleProcessThread，以及负责数据平滑发送的PacerThread。这三个线程和挂载在线程下的模块如图4所示。 VoiceProcessThread线程由Worker线程在创建VoiceEngine时创建，负责音频端模块的处理。挂载在该线程下的模块如图4所示，其中MonitorModule负责对音频数据混音处理过程中产生的警告和错误进行处理，AudioDeviceModuleImpl负责对音频设备采集和播放音频数据时产生的警告和错误进行处理，ModuleRtpRtcpImpl负责音频RTP数据包发送过程中的码率计算、RTT更新、RTCP报文发送等内容。 ModuleProcessThread线程由Worker线程在创建VideoChannel时创建，负责视频端模块的处理。挂载在该线程下的模块如图4所示，其中CallStats负责Call对象统计数据(如RTT)的更新，CongestionController负责拥塞控制[1][2]，VideoSender负责视频发送端统计数据的更新，VideoReceiver负责视频接收端统计数据更新和处理状态反馈(如请求关键帧)，ModuleRtpRtcpImpl负责视频RTP数据包发送过程中的码率计算、RTT更新、RTCP报文发送等内容，OveruseFrameDetector负责视频帧采集端过载监控，ReceiveStatisticsImpl负责由接收端统计数据触发的码率更新过程，ViESyncModule负责音视频同步。 PacerThread线程由Worker线程在创建VideoChannel时创建，负责数据平滑发送。挂载在该线程下的PacedSender负责发送端数据平滑发送；RemoteEstimatorProxy派生自RemoteBitrateEstimator，负责在启用发送端码率估计的情况下把接收端收集到的反馈信息发送回发送端。 由以上分析可知，WebRTC创建的模块处理线程实例基本上涵盖了音视频数据从采集到渲染过程中的大部分数据操作。但还有一些模块不依赖于模块线程工作，这部分模块是少数，本文不展开具体的描述。 参考文献[1] WebRTC基于GCC的拥塞控制(上) – 算法分析 [2] WebRTC基于GCC的拥塞控制(下) - 实现分析 转至]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android应用性能优化]]></title>
    <url>%2F2017%2F07%2F03%2Ftips-android-performance%2F</url>
    <content type="text"><![CDATA[Android手机由于其本身的后台机制和硬件特点，性能上一直被诟病，所以软件开发者对软件本身的性能优化就显得尤为重要；本文将对Android开发过程中性能优化的各个方面做一个回顾与总结。 Cache优化 ListView缓存： ListView中有一个回收器，Item滑出界面的时候View会回收到这里，需要显示新的Item的时候，就尽量重用回收器里面的View；每次在getView函数中inflate新的item之前会先判断回收器中是否有缓存的view，即判断convertView是否为null，是则inflate一个新的item View，否则重用回收器中的item。 此外，ListView还使用静态的ViewHolder减少findViewById的次数 ListView中有getViewTypeCount()函数用于获取列表有几种布局类型，getItemViewType(int position)用于获取在position位置上的布局类型; 我们可以利用ViewType来给不同类型的item创建不同的View，这样可以利于ListView的回收 对Item中图片进行适当压缩, 并进行异步加载；如果快速滑动，不加载图片；实现数据的分页加载 IO缓存：在文件和网络IO中使用具有缓存策略的输入流，BufferedInputStream替代InputStream，BufferedReader替代Reader，BufferedReader替代BufferedInputStream data缓存(空间换时间)：①缓存数据库的查询结果，比如缓存数据库表中的数据条数，这样就可以避免多次的select count查询 ②缓存磁盘文件中需要频繁访问的数据到内存中 ③缓存耗时计算的结果 Battery优化 cpu的使用率和使用频率将直接或间接的影响电量的分配和使用，cpu降频可以节约电量 service优化 service作为一个运行在主线程中的后台服务，应该尽量避免耗时动作，而应该尽量新开线程去处理耗时动作 监听系统广播看service是否存活，否则kill掉；降低service优先级使得系统内存吃紧时会被自动kill掉 使用Android提供的IntentService代替service，因为IntentService会在运行完成之后自动停止，而service需要手动调用stopService()才能停止运行 定时执行任务的Alarm机制：Android的定时任务有两种实现方式，Timer类和Alarm机制；Timer不适合长期后台运行的定时任务。因为每种手机都会有自己的休眠策略，Android手机就会在长时间不操作的情况下自动让CPU进入到睡眠状态，这就有可能导致Timer中的定时任务无法正常运行。而Alarm机制则不存在这种情况，它具有唤醒CPU的功能，即可以保证每次需要执行定时任务的时候CPU能正常工作。然而从Android4.4之后，Alarm任务的触发时间将会变得不准确，有可能会延迟一段时间后任务才能得到执行。这不是bug，而是系统在耗电性方面进行的优化。系统会自动检测目前有多少Alarm任务存在，然后将触发时间将近的几个任务放在一起执行，这就可以大幅度的减少CPU被唤醒的次数，从而有效延长电池的使用时间 渲染层优化 Android 界面卡顿的原因？ UI线程中做耗时操作，比如进行网络请求,磁盘读取，位图修改，更新UI等耗时操作，从而导致UI线程卡顿 布局Layout过于复杂，无法在16ms内完成渲染，或者嵌套层次过深 View过度绘制或者频繁的触发measure、layout，同一时间动画执行的次数过多，导致CPU或GPU负载过重 冗余资源及逻辑等导致加载和执行缓慢 Android 界面卡顿怎么处理？ xml布局优化：尽量使用include、merge、ViewStub标签，尽量不存在冗余嵌套及过于复杂布局（譬如10层就会直接异常），例如使用RelativeLayout代替LinearLayout可以减少布局层次和复杂性，View的嵌套层次不能过深，尽量使用GONE替换INVISIBLE，使用weight后尽量将width和heigh设置为0dp，减少运算，Item存在非常复杂的嵌套时考虑使用自定义Item View来取代，减少measure与layout次数等。 ListView及Adapter优化；尽量复用getView方法中的相关View，不重复获取实例导致卡顿，列表尽量在滑动过程中不进行UI元素刷新等。 背景和图片等内存分配优化；尽量减少不必要的背景设置，图片尽量压缩处理显示，尽量避免频繁内存抖动等问题出现；尽可能为不同分辨率创建资源，以减少不必要的硬件缩放 自定义View等绘图与布局优化；尽量避免在draw、measure、layout中做过于耗时及耗内存操作，尤其是draw方法中，尽量减少draw、measure、layout等执行次数，避免过度渲染和绘制；减少不必要的inflate，尽量使用全局变量缓存View 避免ANR，不要在UI线程中做耗时操作，譬如多次数据库操作等 Layout常用的标签 include标签：该标签可以用于将布局文件中的公共部分提取出来给其它布局文件复用，从而使得布局模块化，代码轻量化; 注意点: ①如果标签已经定义了id，而嵌入布局文件的root布局文件也定义了id，标签的id会覆盖掉嵌入布局文件root的id，如果include标签没有定义id则会使用嵌入文件root的id ②如果想使用标签覆盖嵌入布局root布局属性，必须同时覆盖layout_height和layout_width属性，否则会直接报编译时语法错误 viewstub标签：该标签与include一样用于引入布局模块，只是引入的布局默认不会扩张，既不会占用显示也不会占用位置，从而在解析layout时节省cpu和内存，只有通过调用setVisibility函数或者Inflate函数才会将其要装载的目标布局给加载出来，从而达到延迟加载的效果；例如条目详情、进度条标识或者未读消息等，这些情况如果在一开始初始化，虽然设置可见性View.GONE,但是在Inflate的时候View仍然会被Inflate，仍然会创建对象。 merge标签：该标签在layout中会被自动忽略，从而减少一层布局嵌套，其主要用处是当一个布局作为子布局被其他布局include时，使用merge当作该布局的顶节点来代替layout顶节点就可以减少一层嵌套 hierarchy viewer：该工具可以方便的查看Activity的布局，各个View的属性、measure、layout、draw的时间，如果耗时较多会用红色标记，否则显示绿色 网络优化 异步请求网络数据，避免频繁请求数据（例如如果某个页面内请求过多，可以考虑做一定的请求合并），尽可能的减少网络请求次数和减小网络请求时间间隔 网络应用传输中使用高效率的数据格式，譬如使用JSON代替XML，使用WebP代替其他图片格式,并对数据进行Gzip压缩数据，比如post请求的body和header内容 及时缓存数据到内存/文件/数据库 执行某些操作前尽量先进行网络状态判断，比如wifi传输数据比蜂窝数据更省电，所以尽量在wifi下进行数据的预加载 httpClient和httpUrlConnection对比： httpClient是apache的开源实现，API数量多，非常稳定 httpUrlConnection是java自带的模块: ①可以直接支持GZIP压缩,而HttpClient虽然也支持GZIP，但要自己写代码处理 ②httpUrlConnection直接在系统层面做了缓存策略处理，加快重复请求的速度 ③API简单，体积较小,而且直接支持系统级连接池，即打开的连接不会直接关闭，在一段时间内所有程序可共用 HttpURLConnection在Android2.2之前有个重大Bug，调用close()函数会影响连接池，导致连接复用失效，需要关闭keepAlive;因此在2.2之前http请求都是用httpClient，2.2之后则是使用HttpURLConnection 但是!!!现在!!!Android不再推荐这两种方式！二是直接使用OKHttp这种成熟方案！支持Android 2.3及其以上版本 数据结构优化 ArrayList和LinkedList的选择：ArrayList根据index取值更快，LinkedList更占内存、随机插入删除更快速、扩容效率更高 ArrayList、HashMap、LinkedHashMap、HashSet的选择：hash系列数据结构查询速度更优，ArrayList存储有序元素，HashMap为键值对数据结构，LinkedHashMap可以记住加入次序的hashMap，HashSet不允许重复元素 HashMap、WeakHashMap选择：WeakHashMap中元素可在适当时候被系统垃圾回收器自动回收，所以适合在内存紧张时使用 Collections.synchronizedMap和ConcurrentHashMap的选择：ConcurrentHashMap为细分锁，锁粒度更小，并发性能更优；Collections.synchronizedMap为对象锁，自己添加函数进行锁控制更方便 Android中性能更优的数据类型：如SparseArray、SparseBooleanArray、SparseIntArray、Pair；Sparse系列的数据结构是为key为int情况的特殊处理，采用二分查找及简单的数组存储，加上不需要泛型转换的开销，相对Map来说性能更优 内存优化 Android应用内存溢出OOM 内存溢出的主要导致原因有如下几类：①应用代码存在内存泄露，长时间积累无法释放导致OOM；②应用的某些逻辑操作疯狂的消耗掉大量内存（譬如加载一张不经过处理的超大超高清图片等）导致超过阈值OOM 解决思路①在内存引用上做些处理，常用的有软引用、弱引用 ②在内存中加载图片时直接在内存中做处理，如：边界压缩 ③动态回收内存，手动recyle bitmap，回收对象 ④优化Dalvik虚拟机的堆内存分配 ⑤自定义堆内存大小]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>tips</tag>
        <tag>性能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多媒体之术语]]></title>
    <url>%2F2017%2F06%2F27%2Fmedia-terminology%2F</url>
    <content type="text"><![CDATA[一.编解码术语1.GOP/码流/比特率/帧速率/分辨率1.1GOP（Group of picture）关键帧的周期，也就是两个IDR帧之间的距离，一个帧组的最大帧数，一般而言，每一秒视频至少需要使用 1 个关键帧。增加关键帧个数可改善质量，但是同时增加带宽和网络负载。 需要说明的是，通过提高GOP值来提高图像质量是有限度的，在遇到场景切换的情况时，H.264编码器会自动强制插入一个I帧，此时实际的GOP值被缩短了。另一方面，在一个GOP中，P、B帧是由I帧预测得到的，当I帧的图像质量比较差时，会影响到一个GOP中后续P、B帧的图像质量，直到下一个GOP开始才有可能得以恢复，所以GOP值也不宜设置过大。 同时，由于P、B帧的复杂度大于I帧，所以过多的P、B帧会影响编码效率，使编码效率降低。另外，过长的GOP还会影响Seek操作的响应速度，由于P、B帧是由前面的I或P帧预测得到的，所以Seek操作需要直接定位，解码某一个P或B帧时，需要先解码得到本GOP内的I帧及之前的N个预测帧才可以，GOP值越长，需要解码的预测帧就越多，seek响应的时间也越长。 1.12CABAC/CAVLCH.264/AVC标准中两种熵编码方法，CABAC叫自适应二进制算数编码，CAVLC叫前后自适应可变长度编码， CABAC：是一种无损编码方式，画质好，X264就会舍弃一些较小的DCT系数，码率降低，可以将码率再降低10-15%（特别是在高码率情况下），会降低编码和解码的速速。 CAVLC将占用更少的CPU资源，但会影响压缩性能。 其他: 帧：当采样视频信号时，如果是通过逐行扫描，那么得到的信号就是一帧图像，通常帧频为25帧每秒（PAL制）、30帧每秒（NTSC制）； 场：当采样视频信号时，如果是通过隔行扫描（奇、偶数行），那么一帧图像就被分成了两场，通常场频为50Hz（PAL制）、60Hz（NTSC制）； 帧频、场频的由来：最早由于抗干扰和滤波技术的限制，电视图像的场频通常与电网频率（交流电）相一致，于是根据各地交流电频率不同就有了欧洲和中国等PAL制的50Hz和北美等NTSC制的60Hz，但是现在并没有这样的限制了，帧频可以和场频一样，或者场频可以更高。 帧编码、场编码方式：逐行视频帧内邻近行空间相关性较强，因此当活动量非常小或者静止的图像比较适宜采用帧编码方式；而场内相邻行之间的时间相关性较强，对运动量较大的运动图像则适宜采用场编码方式。 1.1.3Deblocking开启会减少块效应. 1.1.4FORCE_IDR是否让每个I帧变成IDR帧，如果是IDR帧，支持随机访问。 1.1.5frame,tff,bff–frame 将两场合并作为一帧进行编码,–tff Enable interlaced mode (开启隔行编码并设置上半场在前),–bff Enable interlaced mode。 PAFF 和MBAFF：当对隔行扫描图像进行编码时，每帧包括两个场，由于两个场之间存在较大的扫描间隔，这样，对运动图像来说，帧中相邻两行之间的空间相关性相对于逐行扫描时就会减小，因此这时对两个场分别进行编码会更节省码流。 对帧来说，存在三种可选的编码方式：将两场合并作为一帧进行编码(frame 方式)或将两场分别编码(field 方式)或将两场合并起来作为一帧，但不同的是将帧中垂直相邻的两个宏块合并为宏块对进行编码；前两种称为PAFF 编码，对运动区域进行编码时field 方式有效，对非运区域编码时，由于相邻两行有较大的相关性，因而frame 方式会更有效。当图像同时存在运动区域和非运动区域时，在MB 层次上，对运动区域采取field 方式，对非运动区域采取frame 方式会更加有效，这种方式就称为MBAFF，预测的单位是宏块对。 1.2码流/码率码流(Data Rate)是指视频文件在单位时间内使用的数据流量，也叫码率或码流率，通俗一点的理解就是取样率,是视频编码中画面质量控制中最重要的部分，一般我们用的单位是kb/s或者Mb/s。一般来说同样分辨率下，视频文件的码流越大，压缩比就越小，画面质量就越高。码流越大，说明单位时间内取样率越大，数据流，精度就越高，处理出来的文件就越接近原始文件，图像质量越好，画质越清晰，要求播放设备的解码能力也越高。 当然，码流越大，文件体积也越大，其计算公式是文件体积=时间X码率/8。例如，网络上常见的一部90分钟1Mbps码流的720P RMVB文件，其体积就=5400秒×1Mb/8=675MB。 通常来说，一个视频文件包括了画面及声音，例如一个RMVB的视频文件，里面包含了视频信息和音频信息，音频及视频都有各自不同的采样方式和比特率，也就是说，同一个视频文件音频和视频的比特率并不是一样的。而我们所说的一个视频文件码流率大小，一般是指视频文件中音频及视频信息码流率的总和。 以以国内最流行，大家最熟悉的RMVB视频文件为例，RMVB中的VB，指的是VBR，即Variable Bit Rate的缩写，中文含义是可变比特率，它表示RMVB采用的是动态编码的方式，把较高的采样率用于复杂的动态画面(歌舞、飞车、战争、动作等)，而把较低的采样率用于静态画面，合理利用资源，达到画质与体积可兼得的效果。 码率和取样率最根本的差别就是码率是针对源文件来讲的。 1.3采样率采样率（也称为采样速度或者采样频率）定义了每秒从连续信号中提取并组成离散信号的采样个数，它用赫兹（Hz）来表示。采样率是指将模拟信号转换成数字信号时的采样频率，也就是单位时间内采样多少点。一个采样点数据有多少个比特。比特率是指每秒传送的比特(bit)数。单位为 bps(Bit Per Second)，比特率越高，传送的数据越大，音质越好.比特率 =采样率 x 采用位数 x声道数. 采样率类似于动态影像的帧数，比如电影的采样率是24赫兹，PAL制式的采样率是25赫兹，NTSC制式的采样率是30赫兹。当我们把采样到的一个个静止画面再以采样率同样的速度回放时，看到的就是连续的画面。同样的道理，把以44.1kHZ采样率记录的CD以同样的速率播放时，就能听到连续的声音。显然，这个采样率越高，听到的声音和看到的图像就越连贯。当然，人的听觉和视觉器官能分辨的采样率是有限的，基本上高于44.1kHZ采样的声音，绝大部分人已经觉察不到其中的分别了。 而声音的位数就相当于画面的颜色数，表示每个取样的数据量，当然数据量越大，回放的声音越准确，不至于把开水壶的叫声和火车的鸣笛混淆。同样的道理，对于画面来说就是更清晰和准确，不至于把血和西红柿酱混淆。不过受人的器官的机能限制，16位的声音和24位的画面基本已经是普通人类的极限了，更高位数就只能靠仪器才能分辨出来了。比如电话就是3kHZ取样的7位声音，而CD是44.1kHZ取样的16位声音，所以CD就比电话更清楚。 当你理解了以上这两个概念，比特率就很容易理解了。以电话为例，每秒3000次取样，每个取样是7比特，那么电话的比特率是21000。 而CD是每秒 44100次取样，两个声道，每个取样是13位PCM编码，所以CD的比特率是44100213=1146600，也就是说CD每秒的数据量大约是 144KB，而一张CD的容量是74分等于4440秒，就是639360KB＝640MB。 码率和取样率最根本的差别就是码率是针对源文件来讲的 1.4比特率比特率是指每秒传送的比特(bit)数。单位为bps(Bit Per Second)，比特率越高，传送的数据越大。在视频领域,比特率常翻译为码率 !!! 比特率表示经过编码（压缩）后的音、视频数据每秒钟需要用多少个比特来表示，而比特就是二进制里面最小的单位，要么是0，要么是1。比特率与音、视频压缩的关系，简单的说就是比特率越高，音、视频的质量就越好，但编码后的文件就越大；如果比特率越少则情况刚好相反。 比特率是指将数字声音、视频由模拟格式转化成数字格式的采样率，采样率越高，还原后的音质、画质就越好。 常见编码模式： VBR（Variable Bitrate）动态比特率 也就是没有固定的比特率，压缩软件在压缩时根据音频数据即时确定使用什么比特率，这是以质量为前提兼顾文件大小的方式，推荐编码模式； ABR（Average Bitrate）平均比特率 是VBR的一种插值参数。LAME针对CBR不佳的文件体积比和VBR生成文件大小不定的特点独创了这种编码模式。ABR在指定的文件大小内，以每50帧（30帧约1秒）为一段，低频和不敏感频率使用相对低的流量，高频和大动态表现时使用高流量，可以做为VBR和CBR的一种折衷选择。 CBR（Constant Bitrate），常数比特率 指文件从头到尾都是一种位速率。相对于VBR和ABR来讲，它压缩出来的文件体积很大，而且音质相对于VBR和ABR不会有明显的提高。 1.5帧速率帧速率也称为FPS(Frames PerSecond)的缩写——帧/秒。是指每秒钟刷新的图片的帧数，也可以理解为图形处理器每秒钟能够刷新几次。越高的帧速率可以得到更流畅、更逼真的动画。每秒钟帧数(FPS)越多，所显示的动作就会越流畅。 1.6分辨率就是帧大小每一帧就是一副图像。 640*480分辨率的视频，建议视频的码速率设置在700以上，音频采样率44100就行了 一个音频编码率为128Kbps，视频编码率为800Kbps的文件，其总编码率为928Kbps，意思是经过编码后的数据每秒钟需要用928K比特来表示。 视频分辨率是指视频成像产品所成图像的大小或尺寸。常见的视像分辨率有352×288，176×144，640×480，1024×768。在成像的两组数字中，前者为图片长度，后者为图片的宽度，两者相乘得出的是图片的像素，长宽比一般为4：3. 目前监控行业中主要使用Qcif(176×144）、CIF(352×288）、HALF D1(704×288）、D1(704×576）等几种分辨率。 D1是数字电视系统显示格式的标准，共分为以下5种规格： D1：480i格式（525i）：720×480（水平480线，隔行扫描），和NTSC模拟电视清晰度相同，行频为15.25kHz，相当于我们所说的4CIF(720×576) D2：480P格式（525p）：720×480（水平480线，逐行扫描），较D1隔行扫描要清晰不少，和逐行扫描DVD规格相同，行频为31.5kHz D3：1080i格式（1125i）：1920×1080（水平1080线，隔行扫描），高清方式采用最多的一种分辨率，分辨率为1920×1080i/60Hz，行频为33.75kHz D4：720p格式（750p）：1280×720（水平720线，逐行扫描），虽然分辨率较D3要低，但是因为逐行扫描，市面上更多人感觉相对于1080I（实际逐次540线）视觉效果更加清晰。不过个人感觉来说，在最大分辨率达到1920×1080的情况下，D3要比D4感觉更加清晰，尤其是文字表现力上，分辨率为1280×720p/60Hz，行频为45kHz D5：1080p格式（1125p）：1920×1080（水平1080线，逐行扫描），目前民用高清视频的最高标准，分辨率为1920×1080P/60Hz，行频为67.5KHZ。 其中D1 和D2标准是我们一般模拟电视的最高标准，并不能称的上高清晰，D3的1080i标准是高清晰电视的基本标准，它可以兼容720p格式，而D5的1080P只是专业上的标准。 计算输出文件大小公式： （音频编码率（KBit为单位）/8 +视频编码率（KBit为单位）/8）×影片总长度（秒为单位）=文件大小（MB为单位） 2.高清视频目前的720P以及1080P采用了很多种编码，例如主流的MPEG2，VC-1以及H.264，还有Divx以及Xvid，至于封装格式更多到令人发指，ts、mkv、wmv以及蓝光专用等等。 720和1080代表视频流的分辨率，前者1280720，后者19201080，不同的编码需要不同的系统资源，大概可以认为是H.264&gt;VC-1&gt;MPEG2。 VC-1是最后被认可的高清编码格式，不过因为有微软的后台，所以这种编码格式不能小窥。相对于MPEG2，VC-1的压缩比更高，但相对于H.264而言，编码解码的计算则要稍小一些，目前来看，VC-1可能是一个比较好的平衡，辅以微软的支持，应该是一只不可忽视的力量。一般来说，VC-1多为 “.wmv”后缀，但这都不是绝对的，具体的编码格式还是要通过软件来查询。 总的来说，从压缩比上来看，H.264的压缩比率更高一些，也就是同样的视频，通过H.264编码算法压出来的视频容量要比VC-1的更小，但是VC-1 格式的视频在解码计算方面则更小一些，一般通过高性能的CPU就可以很流畅的观看高清视频。相信这也是目前NVIDIA Geforce 8系列显卡不能完全解码VC-1视频的主要原因。 PS&amp;TS是两种视频或影片封装格式，常用于高清片。扩展名分别为VOB/EVO和TS等；其文件编码一般用MPEG2/VC-1/H.264 高清，英文为“High Definition”，即指“高分辨率”。 高清电视(HDTV)，是由美国电影电视工程师协会确定的高清晰度电视标准格式。现在的大屏幕液晶电视机，一般都支持1080i和720P，而一些俗称的“全高清”(Full HD)，则是指支持1080P输出的电视机。 目前的高清视频编码格式主要有H.264、VC-1、MPEG-2、MPEG-4、DivX、XviD、WMA-HD以及X264。事实上，现在网络上流传的高清视频主要以两类文件的方式存在：一类是经过MPEG-2标准压缩，以tp和ts为后缀的视频流文件;一类是经过WMV-HD(Windows Media Video HighDefinition)标准压缩过的wmv文件，还有少数文件后缀为avi或mpg，其性质与wmv是一样的。真正效果好的高清视频更多地以H.264与VC-1这两种主流的编码格式流传。 一般来说，H.264格式以“.avi”、“.mkv”以及“.ts”封装比较常见。 3.位率（定码率，变码率）位率又称为“码率”。指单位时间内，单个录像通道所产生的数据量，其单位通常是bps、Kbps或Mbps。可以根据录像的时间与位率估算出一定时间内的录像文件大小。 位率是一个可调参数，不同的分辨率模式下和监控场景下，合适的位率大小是不同的。在设置时，要综合考虑三个因素： 分辨率:分辨率是决定位率（码率）的主要因素，不同的分辨率要采用不同的位率。总体而言，录像的分辨率越高，所要求的位率（码率）也越大，但并不总是如此，图1说明了不同分辨率的合理的码率选择范围。所谓“合理的范围”指的是，如果低于这个范围，图像质量看起来会变得不可接受；如果高于这个范围，则显得没有必要，对于网络资源以及存储资源来说是一种浪费。 场景:监控的场景是设置码率时要考虑的第二个因素。在视频监控中，图像的运动剧烈程度还与位率有一定的关系，运动越剧烈，编码所要求的码率就越高。反之则越低。因此在同样的图像分辨率条件下，监控人多的场景和人少的场景，所要求的位率也是不同的。 存储空间:最后需要考量的因素是存储空间，这个因素主要是决定了录像系统的成本。位率设置得越高，画质相对会越好，但所要求的存储空间就越大。所以在工程实施中，设置合适的位率即可以保证良好的回放图像质量，又可以避免不必要的资源浪费。 QP(quantizer parameter) 介于0~31之间，值越小，量化越精细，图像质量就越高，而产生的码流也越长。 PSNR 允许计算峰值信噪比(PSNR,Peak signal-to-noise ratio),编码结束后在屏幕上显示PSNR计算结果。开启与否与输出的视频质量无关，关闭后会带来微小的速度提升。 profile level 分别是BP、EP、MP、HP： BP-Baseline Profile：基本画质。支持I/P 帧，只支持无交错（Progressive）和CAVLC； EP-Extended profile：进阶画质。支持I/P/B/SP/SI 帧，只支持无交错（Progressive）和CAVLC； MP-Main profile：主流画质。提供I/P/B 帧，支持无交错（Progressive）和交错（Interlaced），也支持CAVLC 和CABAC 的支持； HP-High profile：高级画质。在main Profile 的基础上增加了8x8内部预测、自定义量化、无损视频编码和更多的YUV 格式； H.264规定了三种档次，每个档次支持一组特定的编码功能，并支持一类特定的应用。 基本档次：利用I片和P片支持帧内和帧间编码，支持利用基于上下文的自适应的变长编码进行的熵编码（CAVLC）。主要用于可视电话、会议电视、无线通信等实时视频通信； 主要档次：支持隔行视频，采用B片的帧间编码和采用加权预测的帧内编码；支持利用基于上下文的自适应的算术编码（CABAC）。主要用于数字广播电视与数字视频存储； 扩展档次：支持码流之间有效的切换（SP和SI片）、改进误码性能（数据分割），但不支持隔行视频和CABAC。主要用于网络的视频流，如视频点播。]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Image Stride(内存图像行跨度)]]></title>
    <url>%2F2017%2F06%2F27%2Fmedia-graphic-image-stride%2F</url>
    <content type="text"><![CDATA[When a video image is stored in memory, the memory buffer might contain extra padding bytes after each row of pixels. The padding bytes affect how the image is store in memory, but do not affect how the image is displayed. 当视频图像存储在内存时，图像的每一行末尾也许包含一些扩展的内容，这些扩展的内容只影响图像如何存储在内存中，但是不影响图像如何显示出来； The stride is the number of bytes from one row of pixels in memory to the next row of pixels in memory. Stride is also called pitch. If padding bytes are present, the stride is wider than the width of the image, as shown in the following illustration. Stride 就是这些扩展内容的名称，Stride 也被称作 Pitch，如果图像的每一行像素末尾拥有扩展内容，Stride 的值一定大于图像的宽度值，就像下图所示： Two buffers that contain video frames with equal dimensions can have two different strides. If you process a video image, you must take into the stride into account. 两个缓冲区包含同样大小（宽度和高度）的视频帧，却不一定拥有同样的 Stride 值，如果你处理一个视频帧，你必须在计算的时候把 Stride 考虑进去； In addition, there are two ways that an image can be arranged in memory. In a top-down image, the top row of pixels in the image appears first in memory. In a bottom-up image, the last row of pixels appears first in memory. The following illustration shows the difference between a top-down image and a bottom-up image. 另外，一张图像在内存中有两种不同的存储序列（arranged），对于一个从上而下存储（Top-Down） 的图像，最顶行的像素保存在内存中最开头的部分，对于一张从下而上存储（Bottom-Up）的图像，最后一行的像素保存在内存中最开头的部分，下面图示展示了这两种情况： A bottom-up image has a negative stride, because stride is defined as the number of bytes need to move down a row of pixels, relative to the displayed image. YUV images should always be top-down, and any image that is contained in a Direct3D surface must be top-down. RGB images in system memory are usually bottom-up. 一张从下而上的图像拥有一个负的 Stride 值，因为 Stride 被定义为[从一行像素移动到下一行像素时需要跨过多少个像素]，仅相对于被显示出来的图像而言；而 YUV 图像永远都是从上而下表示的，以及任何包含在 Direct3D Surface 中的图像必须是从上而下，RGB 图像保存在系统内存时通常是从下而上； Video transforms in particular need to handle buffers with mismatched strides, because the input buffer might not match the output buffer. For example, suppose that you want to convert a source image and write the result to a destination image. Assume that both images have the same width and height, but might not have the same pixel format or the same image stride. 尤其是视频变换，特别需要处理不同 Stride 值的图像，因为输入缓冲也许与输出缓冲不匹配，举个例子，假设你想要将源图像转换并且将结果写入到目标图像，假设两个图像拥有相同的宽度和高度，但是其像素格式与 Stride 值也许不同； The following example code shows a generalized approach for writing this kind of function. This is not a complete working example, because it abstracts many of the specific details. 下面代码演示了一种通用方法来编写这种功能，这段代码并不完整，因为这只是一个抽象的算法，没有完全考虑到真实需求中的所有细节； 12345678910111213141516171819202122void ProcessVideoImage( BYTE* pDestScanLine0, LONG lDestStride, const BYTE* pSrcScanLine0, LONG lSrcStride, DWORD dwWidthInPixels, DWORD dwHeightInPixels )&#123; for (DWORD y = 0; y &lt; dwHeightInPixels; y++) &#123; SOURCE_PIXEL_TYPE *pSrcPixel = (SOURCE_PIXEL_TYPE*)pDestScanLine0; DEST_PIXEL_TYPE *pDestPixel = (DEST_PIXEL_TYPE*)pSrcScanLine0; for (DWORD x = 0; x &lt; dwWidthInPixels; x +=2) &#123; pDestPixel[x] = TransformPixelValue(pSrcPixel[x]); &#125; pDestScanLine0 += lDestStride; pSrcScanLine0 += lSrcStride; &#125;&#125; This function takes six parameters: A pointer to the start of scan line 0 in the destination image. The stride of the destination image. A pointer to the start of scan line 0 in the source image. The stride of the source image. The width of the image in pixels. The height of the image in pixels. 这个函数需要六个参数： 目标图像的起始扫描行的内存指针 目标图像的 Stride 值 源图像的起始扫描行的内存指针 源图像的 Stride 值 图像的宽度值（以像素为单位） 图像的高度值（以像素为单位） The general idea is to process one row at a time, iterating over each pixel in the row. Assume that SOURCE_PIXEL_TYPE and DEST_PIXEL_TYPE are structures representing the pixel layout for the source and destination images, respectively. (For example, 32-bit RGB uses the RGBQUAD structure. Not every pixel format has a pre-defined structure.) Casting the array pointer to the structure type enables you to access the RGB or YUV components of each pixel. At the start of each row, the function stores a pointer to the row. At the end of the row, it increments the pointer by the width of the image stride, which advances the pointer to the next row. 这里的要点是如何一次处理一行像素，遍历一行里面的每一个像素，假设源像素类型与目标像素类型各自在像素的层面上已经结构化来表示一个源图像与目标图像的像素，（举个例子，32 位 RGB 像素使用 RGBQUAD 结构体，并不是每一种像素类型都有预定义结构体的）强制转换数组指针到这样的结构体指针，可以方便你直接读写每一个像素的 RGB 或者 YUV 值，在每一行的开头，这个函数保存了一个指向这行像素的指针，函数的最后一行，通过图像的 Stride 值直接将指针跳转到图像的下一行像素的起始点； This example calls a hypothetical function named TransformPixelValue for each pixel. This could be any function that calculates a target pixel from a source pixel. Of course, the exact details will depend on the particular task. For example, if you have a planar YUV format, you must access the chroma planes independently from the luma plane; with interlaced video, you might need to process the fields separately; and so forth. To give a more concrete example, the following code converts a 32-bit RGB image into an AYUV image. The RGB pixels are accessed using an RGBQUAD structure, and the AYUV pixels are accessed using aDXVA2_AYUVSample8 Structure structure. 引用: 如果你用的是 MSDN Library For Visual Studio 2008 SP1，那么你应该能够在下面地址中找到这篇文章的原文： ms-help://MS.MSDNQTR.v90.chs/medfound/html/13cd1106-48b3-4522-ac09-8efbaab5c31d.htm http://blog.csdn.net/g0ose/article/details/52116453]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenGL混色]]></title>
    <url>%2F2017%2F06%2F27%2Fgl-glBlendFunc%2F</url>
    <content type="text"><![CDATA[混合就是把两种颜色混在一起。具体一点，就是把某一像素位置原来的颜色和将要画上去的颜色，通过某种方式混在一起，从而实现特殊的效果。 假设我们需要绘制这样一个场景：透过红色的玻璃去看绿色的物体，那么可以先绘制绿色的物体，再绘制红色玻璃。在绘制红色玻璃的时候，利用“混合”功能，把将要绘制上去的红色和原来的绿色进行混合，于是得到一种新的颜色，看上去就好像玻璃是半透明的。 要使用OpenGL的混合功能，只需要调用：glEnable(GL_BLEND);即可。要关闭OpenGL的混合功能，只需要调用：glDisable(GL_BLEND);即可。 注意： 只有在RGBA模式下，才可以使用混合功能，颜色索引模式下是无法使用混合功能的。 1.源因子和目标因子混合需要把原来的颜色和将要画上去的颜色找出来，经过某种方式处理后得到一种新的颜色。这里把将要画上去的颜色称为“源颜色”，把原来的颜色称为“目标颜色”。 OpenGL 会把源颜色和目标颜色各自取出，并乘以一个系数（源颜色乘以的系数称为“源因子”，目标颜色乘以的系数称为“目标因子”），然后相加，这样就得到了新的颜 色。（也可以不是相加，新版本的OpenGL可以设置运算方式，包括加、减、取两者中较大的、取两者中较小的、逻辑运算等） 下面用数学公式来表达一下这个运算方式。假设源颜色的四个分量（指红色，绿色，蓝色，alpha值）是(Rs, Gs, Bs, As)，目标颜色的四个分量是(Rd, Gd, Bd, Ad)，又设源因子为(Sr, Sg, Sb, Sa)，目标因子为(Dr, Dg, Db, Da)。则混合产生的新颜色可以表示为： (Rs*Sr+Rd*Dr, Gs*Sg+Gd*Dg, Bs*Sb+Bd*Db, As*Sa+Ad*Da) 如果颜色的某一分量超过了1.0，则它会被自动截取为1.0，不需要考虑越界的问题。 源因子和目标因子是可以通过glBlendFunc函数来进行设置的。glBlendFunc有两个参数，前者表示源因子，后者表示目标因子。这两个参数可以是多种值，下面介绍比较常用的几种。 GL_ZERO：表示使用0.0作为因子，实际上相当于不使用这种颜色参与混合运算。 GL_ONE：表示使用1.0作为因子，实际上相当于完全的使用了这种颜色参与混合运算。 GL_SRC_ALPHA：表示使用源颜色的alpha值来作为因子。 GL_DST_ALPHA：表示使用目标颜色的alpha值来作为因子。 GL_ONE_MINUS_SRC_ALPHA：表示用1.0减去源颜色的alpha值来作为因子。 GL_ONE_MINUS_DST_ALPHA：表示用1.0减去目标颜色的alpha值来作为因子。 GL_SRC_COLOR: 把源颜色的四个分量分别作为因子的四个分量 GL_ONE_MINUS_SRC_COLOR GL_DST_COLOR GL_ONE_MINUS_DST_COLOR GL_SRC_COLOR与GL_ONE_MINUS_SRC_COLOR在OpenGL旧版本中只能用于设置目标因子，GL_DST_COLOR与GL_ONE_MINUS_DST_COLOR在OpenGL 旧版本中只能用于设置源因子。新版本的OpenGL则没有这个限制，并且支持新的GL_CONST_COLOR（设定一种常数颜色，将其四个分量分别作为 因子的四个分量）、GL_ONE_MINUS_CONST_COLOR、GL_CONST_ALPHA、 GL_ONE_MINUS_CONST_ALPHA。另外还有GL_SRC_ALPHA_SATURATE。新版本的OpenGL还允许颜色的alpha 值和RGB值采用不同的混合因子。 2.模式示例 如果设置了glBlendFunc(GL_ONE, GL_ZERO);，则表示完全使用源颜色，完全不使用目标颜色，因此画面效果和不使用混合的时候一致（当然效率可能会低一点点）。如果没有设置源因子和目标因子，则默认情况就是这样的设置。 如果设置了glBlendFunc(GL_ZERO, GL_ONE);，则表示完全不使用源颜色，因此无论你想画什么，最后都不会被画上去了。（但这并不是说这样设置就没有用，有些时候可能有特殊用途） 如果设置了glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);，则表示源颜色乘以自身的alpha 值，目标颜色乘以1.0减去源颜色的alpha值，这样一来，源颜色的alpha值越大，则产生的新颜色中源颜色所占比例就越大，而目标颜色所占比例则减 小。这种情况下，我们可以简单的将源颜色的alpha值理解为“不透明度”。这也是混合时最常用的方式。 如果设置了glBlendFunc(GL_ONE, GL_ONE);，则表示完全使用源颜色和目标颜色，最终的颜色实际上就是两种颜色的简单相加。例如红色(1, 0, 0)和绿色(0, 1, 0)相加得到(1, 1, 0)，结果为黄色。 注意： 所谓源颜色和目标颜色，是跟绘制的顺序有关的。假如先绘制了一个红色的物体，再在其上绘制绿色的物体。则绿色是源颜色，红色是目标颜色。如果顺序反过来，则 红色就是源颜色，绿色才是目标颜色。在绘制时，应该注意顺序，使得绘制的源颜色与设置的源因子对应，目标颜色与设置的目标因子对应。 3.对两种示例模式的具体解释:模式一: 12GLES20.glEnable(GLES20.GL_BLEND); GLES20.glBlendFunc(GLES20.GL_SRC_ALPHA, GLES20.GL_ONE_MINUS_SRC_ALPHA); 模式二: 12GLES20.glEnable(GLES20.GL_BLEND); GLES20.glBlendFunc(GLES20.GL_ONE, GLES20.GL_ONE_MINUS_SRC_ALPHA); 模式一是传统的alpha通道混合，这种模式下颜色和alpha值是分立的，rgb决定颜色，alpha决定..（英文是决定how solid it is 水平有限找不到准确的中文来表达） 在数学上表达式是：blend(source, dest) = (source.rgb * source.a) + (dest.rgb * (1 – source.a)).要注意的是这种模式下，透明只跟alpha有关，跟rgb值无关，一个透明的颜色，不透明的颜色有相同的rgb值，只要alpha=0即可。 模式二是alpha预乘的混合（Premultiplied Alpha Blending），这种模式下rgb与alpha是联系在一起的，数学上的表达式是 blend(source, dest) = source.rgb + (dest.rgb * (1 – source.a)),在这种模式下，透明的表示是rgb值都为0. 4.EGLSurface背景透明设置在OpenGL绘制中,除了设置混色外,还要设置EGLSurface配置支持Alpha,如果不设置EGL相关的EGLSurface支持透明,就算OpenGL函数中开启混色,绘制完成后仍是有黑色背景. 4.1GLSurfaceView在onSurfaceCreated里，调用GLES20.glClearColor(0f, 0f, 0f, 0f);alpha为0，即透明。 然后，对surfaceview要作一定处理： 12345mGLSurfaceView.setEGLConfigChooser(8, 8, 8, 8, 16, 0);TestRenderer renderer = new TestRenderer();mGLSurfaceView.setRender(renderer);mGLSurfaceView.getHolder().setFormat(PixelFormat.TRANSLUCENT);mGLSurfaceView.setZOrderOnTop(true); 4.2SurfaceTexture或Surface构造的SurfaceEGL14.eglChooseConfig中config数组增加EGL10.EGL_ALPHA_SIZE配置: 12345678int[] CONFIG_RGBA = &#123; EGL10.EGL_RED_SIZE, 8, EGL10.EGL_GREEN_SIZE, 8, EGL10.EGL_BLUE_SIZE, 8, EGL10.EGL_ALPHA_SIZE, 8, EGL10.EGL_RENDERABLE_TYPE, EGL_OPENGL_ES2_BIT, EGL10.EGL_NONE &#125;; 5.实现三维混合在进行三维场景的混合时必须注意的是深度缓冲。 深度缓冲是这样一段数据，它记录了每一个像素距离观察者有多近。在启用深度缓冲测试的情况下，如果将要绘制的像素比原来的像素更近，则像素将被绘制。否则,像素就会被忽略掉，不进行绘制。这在绘制不透明的物体时非常有用——不管是先绘制近的物体再绘制远的物体，还是先绘制远的物体再绘制近的物体，或者干脆以 混乱的顺序进行绘制，最后的显示结果总是近的物体遮住远的物体。 然而在你需要实现半透明效果时，发现一切都不是那么美好了。如果你绘制了一个近距离的半透明物体，则它在深度缓冲区内保留了一些信息，使得远处的物体将无法再被绘制出来。虽然半透明的物体仍然半透明，但透过它看到的却不是正确的内容了。 要 解决以上问题，需要在绘制半透明物体时将深度缓冲区设置为只读，这样一来，虽然半透明物体被绘制上去了，深度缓冲区还保持在原来的状态。如果再有一个物体 出现在半透明物体之后，在不透明物体之前，则它也可以被绘制（因为此时深度缓冲区中记录的是那个不透明物体的深度）。以后再要绘制不透明物体时，只需要再 将深度缓冲区设置为可读可写的形式即可。怎么绘制一个一部分半透明一部分不透明的物体？这个好办，只需要把物体分为两个部分，一部分全是半透明 的，一部分全是不透明的，分别绘制就可以了。 即使使用了以上技巧，我们仍然不能随心所欲的按照混乱顺序来进行绘制。必须是先绘制不透明的物体，然 后绘制透明的物体。否则，假设背景为蓝色，近处一块红色玻璃，中间一个绿色物体。如果先绘制红色半透明玻璃的话，它先和蓝色背景进行混合，则以后绘制中间 的绿色物体时，想单独与红色玻璃混合已经不能实现了。 总结起来，绘制顺序就是：首先绘制所有不透明的物体。如果两个物体都是不透明的，则谁先谁后 都没有关系。然后，将深度缓冲区设置为只读。接下来，绘制所有半透明的物体。如果两个物体都是半透明的，则谁先谁后只需要根据自己的意愿（注意了，先绘制 的将成为“目标颜色”，后绘制的将成为“源颜色”，所以绘制的顺序将会对结果造成一些影响）。最后，将深度缓冲区设置为可读可写形式。 调用glDepthMask(GL_FALSE);可将深度缓冲区设置为只读形式。调用glDepthMask(GL_TRUE);可将深度缓冲区设置为可读可写形式。 glBlendFunc函数官网文档]]></content>
      <categories>
        <category>OpenGL</category>
      </categories>
      <tags>
        <tag>OpenGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android系统编码MediaCodec]]></title>
    <url>%2F2017%2F06%2F25%2Ftips-android-mediacodec%2F</url>
    <content type="text"><![CDATA[android编码器支持参数Supported Media FormatsVideo encoding recommendations The table below lists the Android media framework video encoding profiles and parameters recommended for playback using the H.264 Baseline Profile codec. The same recommendations apply to the Main Profile codec, which is only available in Android 6.0 and later. SD (Low quality) SD (High quality) HD 720p (N/A on all devices) Video resolution 176 x 144 px 480 x 360 px 1280 x 720 px Video frame rate 12 fps 30 fps 30 fps Video bitrate 56 Kbps 500 Kbps 2 Mbps Audio codec AAC-LC AAC-LC AAC-LC Audio channels 1 (mono) 2 (stereo) 2 (stereo) Audio bitrate 24 Kbps 128 Kbps 192 Kbps The table below lists the Android media framework video encoding profiles and parameters recommended for playback using the VP8 media codec. SD (Low quality) SD (High quality) HD 720p (N/A on all devices) HD 1080p (N/A on all devices) Video resolution 320 x 180 px 640 x 360 px 1280 x 720 px 1920 x 1080 px Video frame rate 30 fps 30 fps 30 fps 30 fps Video bitrate 800 Kbps 2 Mbps 4 Mbps 10 Mbps CamcorderProfileRetrieves the predefined camcorder profile settings for camcorder applications. These settings are read-only. The compressed output from a recording session with a given CamcorderProfile contains two tracks: one for audio and one for video. Each profile specifies the following set of parameters: The file output format Video codec format Video bit rate in bits per second Video frame rate in frames per second Video frame width and height, Audio codec format Audio bit rate in bits per second, Audio sample rate Number of audio channels for recording. Android编码器常见问题MediaCodec KEY_FRAME_RATE seems to be ignored总结起来就是和输入编码器的帧率有关系 I am trying to modify the source for screenrecord in android 4.4 and lower the captured frame rate, but no matter what value I put in: 1format-&gt;setFloat(&quot;frame-rate&quot;, 5); the result is always the same ( a very high frame rate ) Is the encoder ignoring this property ? how can I control the frame rate ? The frame-rate value is not ignored, but it doesn’t do what you want. The combination of frame-rate and i-frame-interval determines how often I-frames (also called “sync frames”) appear in the encoded output. The frame rate value might also play a role in meeting the bitrate target on some devices, but I’m not sure about that (see e.g. this post). The MediaCodec encoder does not drop frames. If you want to reduce the frame rate, you have to do so by sending fewer frames to it. The screenrecord command doesn’t “sample” the screen at a fixed frame rate. Instead, every frame it receives from the surface compositor (SurfaceFlinger) is sent to the encoder, with an appropriate time stamp. If screenrecord receives 60 frames per seconds, you’ll have 60fps output. If it receives 10 frames in quick succession, followed by nothing for 5 seconds, followed by a couple more, you’ll have exactly that in the output file. You can modify screenrecord to drop frames, but you have to be a bit careful. If you try to reduce the maximum frame rate from 60fps to 30fps by dropping every-other frame, you run the risk that in a “frame0 - frame1 - long_pause - frame2” sequence you’ll drop frame1, and the video will hold on frame0 instead, showing a not-quite-complete animation. So you need to buffer up a frame, and then encode or drop frame N-1 if the difference in presentation times between that and frame N is ~17ms. The tricky part is that screenrecord, in its default operating mode, directs the frames to the encoder without touching them, so all you see is the encoded output. You can’t arbitrarily drop individual frames of encoded data, so you really want to prevent the encoder from seeing them in the first place. If you use the screenrecord v1.1 sources you can tap into “overlay” mode, used for –bugreport, to have the frames pass through screenrecord on their way to the encoder. In some respects it might be simpler to write a post-processor that reduces the frame rate. I don’t know how much quality would be lost by decoding and re-encoding the video.]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>Android</tag>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[media-storage-and-transfer]]></title>
    <url>%2F2017%2F06%2F20%2Fmedia-storage-and-transfer%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[media-compress]]></title>
    <url>%2F2017%2F06%2F20%2Fmedia-compress%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[media-video]]></title>
    <url>%2F2017%2F06%2F20%2Fmedia-video%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[media-audio]]></title>
    <url>%2F2017%2F06%2F20%2Fmedia-audio%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[多媒体技术(一)之图形图像]]></title>
    <url>%2F2017%2F06%2F20%2Fmedia-graphic%2F</url>
    <content type="text"><![CDATA[1.图形与图像的基本概念1.1图形与图像的颜色模型1.1.1颜色的基本概念1.1.1.1 物体的颜色物体的颜色不同是因为他们对光的吸收和反射的属性不同.物体的颜色是由该物体所反射的光的波长来决定的. 人眼看到的物体的颜色不仅取决于该物体所反射的光的波长,还与照射它的光源有关.如果用单一蓝色去照射绿色的树叶, 则此时的树叶只能是黑色的.因为蓝色光源中没有绿色成分,树叶吸收了全部蓝色而呈现黑色. 在彩色显示器中,为了使颜色具有较好的还原度和真实感,通常采用类似自然光作为照明光源. 1.1.1.2 彩色三要素颜色信息对人的视觉反应,可通过色调,色饱和度和亮度这三个参量来表示: 色调:用来描述颜色的不同类别的物理量称为色调,如红,橙,黄,绿,青,蓝,紫.色调取决于该种颜色的主要波长. 色饱和度:色饱和度则是描述颜色的深浅程度的物理量,它按该种颜色混入白光的比例来表示,当某色光的饱和度为100%时,就是表示该色光是完全没有混入白色光的单色光饱和度越高则颜色也越浓.如果大量混入白色光使饱和度降低,人视觉会感到颜色变淡.例如,在浓的的红色光中混入大量的白光,由于饱和度降低就变成了粉红色,但是因为红色是基本色,所以色调并不改变.在某颜色中混入白光与增强白光对某颜色物体的照射是不同的.前者是在摄入人眼的眸色光中混入白光,而后者的结果则是加强了某色物体的反射光的强度,在摄入人眼的反射光中并没有混入白光,因此它并没有改变该色的饱和度. 亮度:用来描述色光的明暗变化的强度的物理量成为亮度.亮度是色光能量的一种描述,是指色调和色饱和度已经固定的光,当它的全部能量增强时感觉明亮,否则感觉暗淡. 色调和色饱和度统称为色度. 1.1.1.3 三基色原理三基色原理认为自然界中景物的绝大多数的彩色光,能分解为互相独立的红(R),绿(G),蓝(B)三种基色光;反之,用互相独立的红,绿,蓝3种基色光以不同的比例混合,可模拟出自然界中绝大多数景色的光.RGB三基色相互独立的含义是指,任一种基色都不能由另外两种基色混合而产生.三基色的选择并不是唯一的. 1.1.1.4 像素(pixel)像素是计算机图形与图像中能被单独处理的最小基本单元. 从像素的视觉属性看,它是一个最小可是单位.一幅彩色图像可以看成是由许多很小的可是点组成的,这个点就是像素.每个像素点都有确定的颜色和亮度,这个颜色就是有互相独立的红,绿,蓝三种基色光以不同的比例混合而成的. 从像素的量值属性看,它的数据结构应同时显示地址,色彩,亮度等数据信息,这些数据就称为像素值. 1.1.2 颜色模型颜色模型(或称色彩模型)就是定量颜色方法.在不同的领域应用于图像时,为了尽可能多地,有效地描述各种颜色,往往采用不同的颜色模型,例如,用显示器这类发光物体显示时采用的是RGB模型,用打印机这类吸光物体输出图像时用CMY模型,进行彩色电视信号的显示与传输时采用YUV模型,从事艺术绘画时习惯采用HSL模型. 1.1.2.1 RGB模型RGB模型也称为加色法混色模型.其混色规律是:以等量的红,绿,蓝基色光混合时: 红 + 绿 = 黄色 红 + 蓝 = 品红色 绿 + 蓝 = 青色 红 + 绿 + 蓝 = 白色 3种基色光全无 = 黑色 加色法的混色规律可以使用下图表示,3个圆分别红绿蓝3种基色,圆与圆的叠加区域表示以等量的基色相混合时所合成的颜色,其色调如该区域的文字表示.当3种基色等量相加时,就会得到白色.其中,又尝尝把品红色成为绿色的补色,青色称为红色的补色,黄色称为蓝色的补色. 物体的颜色是丰富多彩的,任何一种颜色和这3种基色之间的关系可以用下面的配色方程式来描述: F(物体颜色)=R(红色的百分比) + G(绿色的百分比) + B(蓝色的百分比) 由于人类的视觉特性,两种或3种基色产生混色效果,不一定要同时和同一空间位置混合.例如,在心理学实验中有一个色轮实验,它是在可旋转的圆盘上按扇形面积均等的分成3部分,并涂上3种基色,当圆盘慢慢旋转时能够分辨出3中基色,但当圆盘旋转的频率提高到闪光融合频率以上时,人眼不再能分辨出3种基色,而产生白颜色的感觉,以达到混色的效果.这就是 “时间混色法” .最初的顺序制彩色电视就应用了人的这一视觉效果.例如,很细小的红点和旅店均匀间置互相靠的很近,只有在近距离仔细观看才能区分出来,当观看距离很远时就只感觉到黄色的一篇了,这就是 “空间混色法” .目前广泛使用的彩色显像管以及大型LED真彩色广告屏就是利用了这一混色视觉效应. 在多媒体技术中,RGB颜色模型是最基本的模型,因为彩色显示器只有按RGB分量式输入,才能在显示屏幕上合成任意颜色. 1.1.2.2 CMY模型CMY(Cyan Magenta Yellow)模型是采用青,品红,黄色3种基本颜色按一定比例合成颜色的方法.CMY模型又称为减色法混色模型,因为色彩的显示不是直接来自于光线的色彩,而是光线被物体吸收掉一部分之后发射回来的剩余光线所产生的.光线都被吸收时称为黑色,当光线都被反射时成为白色.这种模式适合于彩色打印机等用彩色墨水或颜料进行混合显示的情况. 在相减混色中,当3种基本颜色等量相减时得到黑色;等量黄色(Y)和品红(M)相减而青色为0时,等到红色(R);等量青色(C)和品红(M)相减而黄色为0时,得到蓝色(B);等量黄色(Y)和青色(C)相减而品红(M)为0时,得到绿色(G). 由于颜料的化学特性,实际上等量的CMY混合后并不能产生真正的黑色,因此在印刷时通常再加上黑色(Black),这样又称为CMYK模式,四色印刷便是由此而来. 显然,RGB与CMY模型是颜色互补的模型,它们之间可以互相转换.如果按每个像素每种颜色用一位二进制数表示的话,RGB与CMY模型之间的颜色关系如下表所示.利用它们之间的关系,可以把显示器显示的颜色转换成打印的颜色.但实际上因为发射光与反射光的性质完全不同,显示器上的颜色不可能精确地在打印机上复制出来,因此实际的转换过程会利用一定的算法进行一定程度上的失真补偿. |RGB|CMY|颜色| |—|—|—| |000|111|黑| |001|110|蓝| |010|101|绿| |011|100|青| |100|011|红| |101|010|品红| |110|001|黄| |111|000|白| 1.1.2.3 YUV与YIQ模型在彩色电视系统中不采用RGB颜色模型,而采用YUV或YIQ模型表示彩色图像.YUV适用于PAL(Phase Altermation Line,同行倒相制式)和SECAM(法文)(Sequential C欧了让Memo,顺序传送彩色与存储制式)彩色电视制式,而YIQ适用于美国国家电视标准委员会(NTSC,National Television System Committee)彩色电视制式. Y是亮度信号,U和V则是两个色差信号,分别传送红基色分量和蓝基色分量与亮度分量的差值信号. 采用YUV颜色模型的好处是:其一,亮度信号Y解决了彩色电视与黑白电视的兼容性问题;其二,由于人眼对颜色细节的分辨率低于对亮度细节的分辨率,所以可以用一个通道来传送,Y,U,V这3个信号,给亮度信号较大的带宽(6MHz)来传送图像的细节,而给色差信号较小的带宽(1.3MHz)来进行大面积涂色.这样,总的传输数据量和RGB模型相比,要明显小一些,起到了一种数据压缩节省存储空间的作用,而对于这种数据压缩带来的画面变化人眼一般是感觉不到的. 电视系统通常采用摄像机把摄得的彩色图像信号经分色,放大和校正分成RGB这3个分量的信号,再经过矩阵变换电路将彩色信号分解成亮度信号Y和色差信号,U,V,而后对其进行编码,用同一信道发送出去.接收端再通过编码及矩阵逆变换还原成3个基色显示. 在NTSC彩色电视制式中使用YIQ模型,其特性与YUV模型相近.其中的Y表示亮度,I,Q也是两个色差分量,但它们在色度矢量图中与U,V的位置不同.Q,I正交坐标轴与U,V正交坐标轴之间有33度夹角,如图所示: I,Q与U,V之间的关系如下: I = Vcos33 - Usin33 Q = Vsin33 + Ucos33 人眼的彩色视觉特性表明,人眼分辨红与黄之间的颜色变化的能力最强,而分辨蓝,紫之间颜色变化的能力最弱.因此YIQ模型在色度矢量图中,选择I轴正好处于夹角123度处,即人眼具有最大彩色分辨率的红与黄之间的橙色和青色(相角为303度)处,选择与I轴正交的色度信号轴为Q轴(相角为33度),正是人眼最不敏感的色轴位置.因而YIQ模型传送分辨力较强的I信号时,用较宽的频带(1.3MHz~1.5MHz),传送分辨力弱的Q信号时,可用较窄的频带(0.5MHz),这就可以在保证较好的颜色传输特性情况下,最大限度地节省存储空间. 1.1.2.4 HSI颜色模型HSI或HSL是Hue Saturation Intensity(Lightness)的英文缩写,颜色模型用H,S,I这三个参数描述颜色特性,其中H定义颜色的波长,称为色调;S表示颜色的深浅程度,称为饱和度;I表示强度或亮度,这正是颜色的三要素. HSI模型更接近人对颜色更接近人对彩色的认识,符合人眼对颜色的感知方式,是一种从事艺术绘画的画家们习惯使用的描述色彩的方法.它比RGB模型使用更方便,从而能减少彩色图像处理的复杂性,增加快速性,因此一般的图像处理软件中,都提供了这种定量色彩的方式. 1.1.3 颜色模型的转换无论采用什么颜色模型来表示彩色图形与图像,由于所有的显示器都需要RGB值来驱动,所以在显示每个像素之前,必须要把彩色分量值转换成RGB值. 1.1.3.1 YUV与RGB颜色模型变换RGB与YUV的对应关系可以近似地用下面的方程表示: Y = 0.299R + 0.587G + 0.114B U = -0.147R - 0.289G + 0.436B V = 0.615R - 0.515G - 0.096B1.1.3.2 YIQ与RGB颜色模型变换YIQ与RGB的对应关系可以近似地用下面的方程表示: Y = 0.229R + 0.587G + 0.114B I = 0.596R - 0.275G - 0.321B Q = 0.212R - 0.523G + 0.311B HSI 与 RGB颜色变换HSI与RGB空间的转换关系可以用下面的方程表示: H = [90 - arctan(F/sqr(3)) + [0, G&gt;B;180,G&lt;B]]/360 S = 1 - min(R,G,B)/I I = (R+G+B)/3 其中,F=(2R-G-B)/(G-B),sqr为求平方根 1.2图形与图像的基本属性一幅彩色图像可以看成二维连续函数f(x,y),其彩色幅度是位置(x,y)的函数.计算机多媒体技术从其图像的生成,显示,处理和存储的机制出发,需要对彩色图像数字化.数字化一幅彩色图像就是要把连续函数f(x,y)在空间的坐标和彩色幅度进行离散和量化.空间坐标x,y的离散化通常以分辨率来表征,而彩色幅度的离散化则由像素的颜色深度来表征. 1.2.1分辨率分辨率是一个统称,分为显示分辨率,图像分辨率,扫描分辨率和打印分辨率等. 1.2.1.1显示分辨率是指某一种显示方式下,显示屏上能够显示出的像素数目,以水平和垂直的像素表示.例如,显示分辨率为640*480表示显示屏分成480行,每行显示640个像素,整个显示屏就含有307200个显像点.屏幕上的像素越多,分辨率就越高,显示出来的图像也就越细腻,显示的图像质量也就约高.屏幕能够显示的最大像素数目越多,也说明显示设备的最大分辨率越高.显示屏上的每个彩色像素由代表R,G,B这3种模拟信号的相对强度决定,这些彩色像素点就构成一幅彩色图像. 1.2.1.2图像分辨率图像分辨率指数字化图像的大小,以水平和垂直的像素数表示.如果组成图像的像素数目越多,则说明图像的分辨率越高,看起来就越逼真,图像分辨率实际上决定了图像的显示质量,也就是说,即使提高了显示分辨率,也无法真正改善图像的质量.图像分辨率与显示分辨率是两个不同的概念.图像分辨率的确定组成一幅图像的像素数目,而显示分辨率是确定显示图像的区域大小.当图像分辨率与屏幕分辨率一致时,图像正好占据满屏;当图像分辨率小于屏幕分辨率时,图像占据屏幕的一部分;当图像分辨率大于屏幕分辨率时,则屏幕仅能显示图像的一部分. 1.2.1.3扫描分辨率和打印分辨率在用于扫描仪扫描图像时,通常要指定扫描的分辨率,用每英寸包含的点(d/i,dots per inch)表示.如果用300d/i来扫描一幅86的彩色图像,就得到一幅24001800个像素的图像.分辨率越高,像素就越多. 打印分辨率是指图像打印时每英寸可识别的点数,也使用d/i(dots per inch)为衡量单位.两种分辨率之间是有区别的,扫描分辨率反映了扫描后的图像与原始图像之间的差异程度,分辨率越高,差异越小.打印分辨率反映了打印的图像与原数字图像之间的差异程度,分辨率越接近原图像的分辨率,打印质量越高.两种分辨率的最高值都受到设备的限制. 1.2.2颜色深度颜色深度是指图像中每个像素的颜色(或亮度)信息所占的二进制数位数,记做位/像素(b/p,bits per pixel).屏幕上的每一个像素都占有一个或多个位,用于存放与它相关的颜色信息.颜色深度决定了构成图像的每个像素可能出现的最大颜色数,因而颜色深度值越高,显示的图像色彩越丰富.反之,颜色深度太浅,会影响图像的质量,图像看起来让人觉得很粗糙和很不自然.常见颜色深度有一下5种: 4bit:这是VGA标准支持的颜色深度,共2的四次方16种颜色; 8bit:这是多媒体应用中的最低颜色深度,共2的8次方256种颜色,称为索引彩色图(由颜色查找决定); 16bit:在16bit中,用其中的15bit表示RGB这3种颜色,每种颜色5bit,用剩余的以为表示图像的其他属性,如透明度.所以16bit的颜色深度实际可以表示为2的15次方323232共32768种颜色.称为HI-Color(高彩色)图像. 24bit:用3个8bit分别表示RGB,可生成的颜色数2的24次16777216种,约16M种颜色,这已经成为真彩色; 32bit:同24bit颜色深度一样,也是用3个bit分别表示RGB这三种颜色,剩余的8bit用来表示图像的其他属性,如透明度等. 虽然像素的颜色颜色深度值越大图像色彩越丰富,但由于设备的限制,人眼分辨率的限制,不一定要追求特别深的颜色深度,一般来说,32bit的颜色深度已经足够.此外,像素颜色深度越深,所占用的存储空间越大. 一个像素的颜色深度位数除R,G,B分量占用固定bit数表示颜色外,一般要腾出1bit或几bit作为属性(Attribute)位.属性位用来指定该像素应具有的性质.例如,像素的颜色深度为32bit时,R,G,B分别用8bit表示,那么余下的8bit常称为a通道(Alpha Channel)位,或称为覆盖(Overlay)位,中断位,属性位,它用来控制该像素点的透明度.假如定义一个像素值(A,R,G,B)的4个分量(其中A为Alpha属性位数值)都用归一化的数值表示,那么像素91,1,0,0)时显示红色.当像素为(0.5,1,0,0)时,预乘的结果就变成了(0.5.0.5,0,0),这表示现在显示的红色的强度减低一半.用这种定义像素属性的办法可以实现两幅彩色图像之间的透明叠加效果.当Alpha数值很小时,渲染出来的效果是几乎透明的,如玻璃;当Alpha数值处于中间的位置时,则可以得到一种半透明的效果;当Alpha数值接近255时,是几乎不透明的效果.这种属性位的加入为实现透明和半透明的显示掉过带来了方便. 1.2.3文件的大小图形和图像文件的大小(也称为数据量)是指在磁盘上存储整幅图像所有点的字节数(Bytes),反映了图像所需数据存储空间的大小,可按下面的公式计算: 文件字节数 = 图像分辨率 * 图像深度/8 从公式看,图像文件的大小与图像的颜色和内容无关.其实在实际应用中,为了节省存储空间,总要对图像应用某种压缩文件格式.这样,不同的图像会因为内容的不同而使文件的大小有所不同.相对于颜色层次多,图形复杂的图像文件较大.但各种图像文件的最大值(文件字节数)都不会超越由上式决定的字节数. 1.2.4 真彩色,伪彩色与直接色 真彩色(True Color):真彩色是指在组成一幅彩色图像的每个像素值中,有R,G,B这3个基色分量,每个基色分量直接决定显示设备的基色强度,这样产生的彩色称为真彩色.例如,用RGB的8:8:8方式表示一幅彩色图像,就是R,G,B都用8bit来表示,每个基色分量占一个字节,共3个字节,每个像素的颜色就是由这3字节中的数值直接决定,可生成的颜色数就是2的24次中,共计16777216中. 伪彩色(Pseudo Color) 伪彩色图像是每个像素的颜色不是由每个基色分量的数值直接决定,而是把像素值当做彩色查找表(CLUT,Color-Look-Up Table)的表项入口地址,去查找一个显示图像时使用的RGB强度值,用查找出的R,G,B强度值产生的彩色称为伪彩色.彩色查找表是一个事先制作好的表,表项入口地址也称为索引号.彩色图像本身的像素值和彩色查找表中的索引号有一个变换关系,也可以是用自己定义的变换关系.使用查找得到的数值在显示器上显示的颜色是真的,但不是图像本身真正的颜色,因此称其为伪彩色; 直接色(Direct Color):把像素值的R,G,B分量作为单独的索引值,通过相应的彩色变换表找出R,G,B各自的基色程度,用这个强度值产生的彩色称其为直接色.真彩色系统虽然也是采用R,G,B分量来决定基色强度,但这是由R,G,B经变换后决定的,这与直接用R,G,B决定基色基色强度产生的颜色就有差别.比较而言,直接色在显示器上显示彩色图像看起来更真实,更自然. 伪彩色系统是把整个像素当做查找表的索引值进行彩色变换,而直接色系统是对R,G,B分量分别采用查找表进行变换,因此伪彩色系统色彩还原度自然就查多了. 1.3图形与图像的基本类型1.3.1位图与矢量图 位图(Bit-mapped Graphics) 矢量图(Vector Graphic) 位图和矢量图的关系 1.3.2 图形与图像的区别与联系2.图形与图像的处理2.1图形与图像的获取2.1.1 图形与图像的数字化2.1.2 图形的获取2.1.3 图像的获取2.2图形与图像的存储2.2.1静态图形与图像常见文件存储格式2.2.2 动态图形与图像常见文件存储格式2.2.3 文件存储格式的数据结构2.3图形与图像的显示2.3.1映射显示原理2.3.2硬复制设备2.4图形与图像的处理2.4.1 图形与图像处理的基本内容2.4.2 图像处理实例分析-图像识别2.4.3 图形与图像处理软件3.计算机动画3.1计算机动画的原理3.2计算机动画的类型3.3计算机动画的制作3.4虚拟现实动画技术]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac下打包qt程序成dmg]]></title>
    <url>%2F2017%2F06%2F18%2Ftips-qt-pack%2F</url>
    <content type="text"><![CDATA[参考http://doc.qt.digia.com/4.7-snapshot/appicon.html 图片格式在线转换http://iconverticons.com/，可以生成icns格式图片。 Test.pro中添加macx{ICON = Test.icns} （记得把Test.icns添加到工程中） 发布dmb包 QT在mac下有个发布命令：macdeployqt 我的mac上macdeployqt目录如下：/Users/duobianxing/QtSDK/Desktop/Qt/4.8.1/gcc/bin 将macdeployqt的路径添加到环境变量里面 终端里 vim /etc/profile（如果在保存时有问题，可以用 sudo vim /etc/profile）1234567891011121314#在最后添加一行，如下所示：# System-wide .profile for sh(1)if [ -x /usr/libexec/path_helper ]; then eval `/usr/libexec/path_helper -s`fiif [ &quot;$&#123;BASH-no&#125;&quot; != &quot;no&quot; ]; then [ -r /etc/bashrc ] &amp;&amp; . /etc/bashrcfi export PATH=/Users/duobianxing/QtSDK/Desktop/Qt/4.8.1/gcc/bin:$PATH cd进入到Test.app所在目录，然后执行macdeployqt extractor.app -verbose=1 -dmg，即可生成Test.dmg包。]]></content>
      <categories>
        <category>QT</category>
      </categories>
      <tags>
        <tag>QT</tag>
        <tag>MAC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mpeg编码seekto方法精确定位到指定帧]]></title>
    <url>%2F2017%2F06%2F18%2Ftips-seekto%2F</url>
    <content type="text"><![CDATA[MediaExtractor有一个方法如下: 12//All selected tracks seek near the requested time according to the specified mode.public void seekTo (long timeUs, int mode) timeUs是要seek的时间戳，mode是seek的模式，可以是SEEK_TO_PREVIOUS_SYNC, SEEK_TO_CLOSEST_SYNC, SEEK_TO_NEXT_SYNC，分别是seek指定帧的上一帧，最近帧和下一帧。 此方法可用于视频播放时动态定位播放帧，用于动态改变视频播放进度，比如使用seekBar来跟踪视频播放进度，同时可拖动来动态改变播放进度。 mpeg编码决定seekTo方法无法精确定位到指定帧。即使使用的是某一帧精确的时间戳作为seekTo方法的输入参数也无法实现精确定位。 在google, stackoverlfow查询得出的结论是：在每次seekTo方法调用后，MediaCodec必须从关键帧开始解码。因此seekTo方法只会seek到最近的／上一个／下一个关键帧，也就是I-Frame(key frame = I frame = sync frame)。之所以要从关键帧开始解码，是因为每一帧不一定是单独编码的，只有I frame才是帧内编码，而P, B frame都是要参考别的帧来进行编码，因此单独拿出来是不完整的一帧。 stackoverflow上有人对此的做法是：seekTo的输入参数mode设置为SEEK_TO_PREVIOUS_SYNC,即seek的是指定帧的上一个关键帧。然后判断当前的时间戳是否小于定位关键帧的时间戳，如果是就调用MediaExtractor的advance方法，“快进”到指定帧。 1234extractor.seekTo(expectedPts, MediaExtractor.SEEK_TO_PREVIOUS_SYNC); while (currentPts &lt; expectedPts) &#123; extractor.advance(); &#125; 但是这个方法仍不理想，如果seek的位置和当前位置比较远的话，会有一定延迟。而且视频内容偶尔会出现不完整的帧的闪现。 经过一段时间的研究，终于解决了这个问题，现在播放时可以根据seekBar随时拖动到视频任何一帧，不会有任何延迟，甚至可以实现倒播了。 因为之前播放视频的是自己用MediaCodec, MediaMuxer等编码合成的视频文件，在编码参数设置的时候，将关键帧间隔KEY_I_FRAME_INTERVAL设置为了1（因为要求参数为整数）。注意这个参数的单位是秒，而不是帧数！网上看到很多例子包括fadden的bigflake和Grafika上都将这里设置为了20几。搞得我一开始还以为是每隔二十几帧就有一个关键帧。如果设置为20几，那么就是说你用MediaCodec编码录制一段20多秒的视频，只有开头的一个关键帧！剩下的都是P或者B帧。 这显然是不合理的。一般来说是每隔1秒有一个关键帧，这样就可以seek到对应秒的关键帧。或者说1秒内如果有30帧，那么这30帧至少有一个关键帧。因此我将KEY_I_FRAME_INTERVAL设置为了1。 1mediaFormat.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 1); 这也是为什么我在播放用这种参数编码的视频的时候，使用seekTo方法不能准确定位帧了。之前有讲，seekTo是定位到关键帧的，如果不是关键帧，那么它会去找上一个／最近一个／下一个关键帧，这取决于你输入参数mode的设置。 因此如果想使用seekBar准确拖动定位到任何一帧播放，必须保证每一帧都是关键帧。 于是，我将KEY_I_FRAME_INTERVAL设置为了0： 1mediaFormat.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 0); 事实也证明这样可以保证录制的每一帧都是关键帧，因此在使用seekTo方法的时候终于可以准确定位任何一帧了。拖动seekBar的时候视频内容也会立刻改变，无论是往前还是往后，都不会有任何延迟和画面不完整的情况. 但是，把视频每一帧都设置为关键帧是否合理呢？是否会占太大空间呢？ 带着这个疑问，我使用ffmpeg查看了我测试使用的手机（Lenovo X2)内置相机录制的视频。 只需一行代码： 1ffprobe -show_frames video.mp4 &gt; frames.txt 打开frames.txt可以看到每一帧的key_frame=1，表示是关键帧 这说明了手机本来录像就是把每一帧都作为关键帧的。 当然，不能以偏概全，于是我使用iPhone 6s录制的一段普通视频和慢动作视频。使用ffmpeg查看，发现每一帧也都是关键帧（慢动作视频1秒有240帧也都全部作为关键帧也是蛮拼的）。 目前我测试的两部手机都是如此，具体为什么手机录的视频每一帧都是关键帧我也不明白。而视频文件体积大小和是否将每一帧设为关键帧似乎不成线性关系，所以将KEY_I_FRAME_INTERVAL设置为0的方案是可行的。 因此只要保证视频每帧都是关键帧，那么seekTo方法就可以精确定位指定帧了。]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>Android</tag>
        <tag>Mpeg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多媒体技术简介]]></title>
    <url>%2F2017%2F06%2F07%2Fmedia-intro%2F</url>
    <content type="text"><![CDATA[关键帧间隔关键帧包含了显示帧需要的所有信息 所有的视频都至少包含一个关键帧，作为文件的第一个帧。其它的关键帧可用来改善视频的质量，不过它们也会增加文件大小。一般而言，每一秒视频至少需要使用 1 个关键帧。若使用此公式，在每秒播放 25个帧的视频中，每 25 个帧就会有 1 个关键帧。增加关键帧个数可改善质量，但是同时增加带宽和网络负载。 两种彩电视频制式： NTSC (525 lines @ 59.94 Hz) 29.97 fps PAL (625 lines @ 50 Hz) 25 fps NTSC和PAL属于全球两大主要的电视广播制式，但是由于系统投射颜色影像的频率而有所不同。NTSC是National Television System Committee的缩写，其标准主要应用于日本、美国，加拿大、墨西哥等等，PAL 则是Phase Alternating Line的缩写，主要应用于中国，香港、中东地区和欧洲一带。 GOP最大可含帧数目：18 (NTSC) / 15 (PAL) GOP是由固定模式的一系列I帧、P帧、B帧组成。 I帧编码是为了减少空间域冗余，P帧和B帧是为了减少时间域冗余。 常用的结构由15个帧组成，具有以下形式IBBPBBPBBPBBPBB。简称GOP(4,2)，指的是该图像组除了一个I帧外，包含了4个P帧，并且任何两个P帧或者I、P之间都有两个B帧。 GOP（Group of Pictures）策略影响编码质量：所谓GOP，意思是画面组，一个GOP就是一组连续的画面。MPEG编码将画面（即帧）分为I、P、B三种，I是内部编码帧，P是前向预测帧，B是双向内插帧。简单地讲，I帧是一个完整的画面，而P帧和B帧记录的是相对于I帧的变化。没有I帧，P帧和B帧就无法解码，这就是MPEG格式难以精确剪辑的原因，也是我们之所以要微调头和尾的原因。 MPEG-2 帧结构 MPEG-2压缩的帧结构有两个参数，一个是GOP（Group Of Picture）图像组的长度，一般可按编码方式从1－15；另一个是I帧和P帧之间B帧的数量，一般是1－2个。前者在理论上记录为N，即多少帧里面出现一次I帧；后者描述为多少帧里出现一次P帧，记录为M。]]></content>
      <categories>
        <category>media</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>音视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jni介绍]]></title>
    <url>%2F2017%2F06%2F07%2Fjni-intro%2F</url>
    <content type="text"><![CDATA[1.C回调JAVA c中返回一个字符串 1（*env）-&gt;NewStringUTF(env,&quot;Huazi 华仔&quot;); c中返回一个数组 12345678910..................... int i = 0; jintArray array; array =(*env)-&gt;NewIntArray(env,8); for(;i&lt;8;i++) // 赋值成 0 ~ 7 (*env)-&gt;SetObjectArrayElement(env,array,i,i); &#125; return array; c中使用调用传入的参数是数组array 是传入的数组 123456789......... int sum =0, i; int len = (*env)-&gt;GetArrayLength(env,array); jint *element =(*env)-&gt;GetIntArrayElement(env,array,0); for(i=0;i&lt;len;i++) &#123; sum+= *(element+i); &#125; return sum; c中调用java中类的方法 没有参数 只有返回值String 1234567//()Ljava/lang/String;&quot; 表示参数为空 返回值是String类型 JNIEXPORT jstring JNICALLJava_com_huazi_Demo_getCallBack(JNIENV env,jobject object)&#123; jmethodID mid; jclass cls =(*env)-&gt;FindClass(env,&quot;com/huazi/Demo&quot;); //后面是包名+类名 mid =(*env)-&gt;GetMethodID(env,cls,&quot;TestMethod&quot;,&quot;()Ljava/lang/String;&quot;);//TestMethod java中的方法名 jstring msg =(*env)-&gt;CallObjectMethod(env,object,mid); //object 注意下是jni传过来的jobject return msg; c中调用java中类的静态方法 没有参数 只有返回值String 12345678//@&quot;()Ljava/lang/String;&quot; 表示参数为空 返回值是String类型JNIEXPORT jstring JNICALLJava_com_huazi_Demo_getCallBack(JNIENV env,jobject object)&#123; jmethodID mid; jclass cls =(*env)-&gt;FindClass(env,&quot;com/huazi/Demo&quot;); //后面是包名+类名 mid =(*env)-&gt;GeStatictMethodID(env,cls,&quot;TestMethod&quot;,&quot;()Ljava/lang/String;&quot;);// TestMethod java中的方法名 jstring msg =(*env)-&gt;CallStaticObjectMethod(env,cls,mid); //object 注意下是jni传过来的jobject return msg; &#125;]]></content>
      <tags>
        <tag>JNI</tag>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FFMPEG编译之Mac]]></title>
    <url>%2F2017%2F06%2F07%2Fffmpeg-compile-mac%2F</url>
    <content type="text"><![CDATA[Mac下FFMPEG使用There are a few ways to get FFmpeg on OS X. One is to build it yourself. Compiling on Mac OS X is as easy as any other *nix machine, there are just a few caveats(警告). The general procedure is get the source, then ./configure ; make &amp;&amp; sudo make install, though specific configure flags are possible. Another is to use some “build helper” tool, to install it for you. For example, homebrew or macports, see the homebrew section in this document. Alternatively, if you are unable to compile, or do not want to install homebrew, you can simply download a static build for OS X, but it may not contain the features you want. Typically this involves unzipping an FFmpeg distribution file [like .zip file], then running it from within the newly extracted files/directories. 手动编译FFMPEG1.下载FFMPEG源码使用git clone https://github.com/FFmpeg/FFmpeg从github下载ffmpeg源码,切换到要使用的目标分支(这里使用release/3.3):git checkout -b r3.3 origin/release/3.3,或者直接从github下载分支release/3.3的压缩包,解压. 2.准备XcodeStarting with Lion 10.7, Xcode is available for free from the Mac App Store and is required to compile anything on your Mac. Make sure you install the Command Line Tools from Preferences &gt; Downloads &gt; Components. Older versions are still available with an AppleID and free Developer account at ​developer.apple.com. 3.准备HomeBrew工具To get ffmpeg for OS X, you first have to install ​Homebrew. If you don’t want to use Homebrew, see the section below. 1ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; Then: 12brew install automake fdk-aac git lame libass libtool libvorbis libvpx \opus sdl shtool texi2html theora wget x264 x265 xvid yasm Mac OS X Lion comes with Freetype already installed (older versions may need ‘X11’ selected during installation), but in an atypical location: /usr/X11. Running freetype-config in Terminal can give the locations of the individual folders, like headers, and libraries, so be prepared to add lines like CFLAGS=freetype-config --cflags LDFLAGS=freetype-config --libs PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig:/usr/lib/pkgconfig:/usr/X11/lib/pkgconfig before ./configure or add them to your $HOME/.profile file. 4.编译Once you have compiled all of the codecs/libraries you want, you can now download the FFmpeg source either with Git or the from release tarball links on the website. Study the output of ./configure –help and make sure you’ve enabled all the features you want, remembering that –enable-nonfree and –enable-gpl will be necessary for some of the dependencies above. A sample command is: 123456git clone http://source.ffmpeg.org/git/ffmpeg.git ffmpegcd ffmpeg./configure --prefix=/usr/local/ffmpeg --enable-gpl --enable-nonfree --enable-libass \--enable-libfdk-aac --enable-libfreetype --enable-libmp3lame \--enable-libtheora --enable-libvorbis --enable-libvpx --enable-libx264 --enable-libx265 --enable-libopus --enable-libxvidmake &amp;&amp; sudo make install --prefix指定编译完成后安装路径,这里指定到/usr/local/ffmpeg,安装完成会在/usr/local/ffmpeg下生成:bin,include,lib,share四个目录 安装环境介绍A package consists of several related files which are installed in several directories. The configure step usually allows the user to specify the so-called install prefix, and is usually specified through the configure option configure –prefix=PREFIX, where PREFIX usually is by default /usr/local. The prefix specifies the common directory where all the components are installed. The following directories are usually involved in the installation: PREFIX/bin: contains the generated binaries (e.g. ffmpeg, ffplay, ffprobe etc. in the case of FFmpeg) PREFIX/include: contains the library headers (e.g. libavutil/avstring.h, libavcodec/avcodec.h, libavformat/avformat.h etc. in case of FFmpeg) required to compile applications linked against the package libraries PREFIX/lib: contains the generated libraries (e.g. libavutil, libavcodec, libavformat etc. in the case of FFmpeg) PREFIX/share: contains various system-independent components; especially documentation files and examples By specifying the prefix it is possible to define the installation layout. By using a shared prefix like /usr/local/, different packages will be installed in the same directory, so in general it will be more difficult to revert the installation. Using a prefix like /opt/PROJECT/, the project will be installed in a dedicated directory, and to remove from the system you can simply remove the /opt/PREFIX path. On the other hand, such installation will require to edit all the environment variables to point to the custom path. Environment variablesSeveral variables defined in the environment affect your package install. In particular, depending on your installation prefix, you may need to update some of these variables in order to make sure that the installed components can be found by the system tools. The list of environment variables can be shown through the command env. A list of the affected variables follows: PATH: defines the list of :-separated paths where the system looks for binaries. For example if you install your package in /usr/local/, you should update the PATH so that it will contain /usr/local/bin. This can be done for example through the command export PATH=/usr/local/bin:$PATH. LD_LIBRARY_PATH: contains the :-separated paths where the system looks for libraries. For example if you install your package in /usr/local/, you should update the LD_LIBRARY_PATH so that it will contain /usr/local/lib. This can be done for example through the command export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH. This variable is sometimes deprecated in favor of the use of ldconfig. CFLAGS: contains flags used by the C compiler, and usually includes preprocessing directives like -IPREFIX/include or compilation flags. Custom CFLAGS are usually prefixed to the source package compiler flags by the source package build system. Alternatively many build systems allow to specify the configure option -extra-cflags. LDFLAGS: these are directives used by the linker, and usually include linking directives like -LPREFIX/lib needed to find libraries installed in custom paths. Custom LDFLAGS are usually prefixed to the source package linker flags by the source package build system. Alternatively, many build systems allow to specify the configure option -extra-ldflags. PKG_CONFIG_PATH: contains the :-separated paths used by pkg-config to detect the pkg-config files used by many build systems to detect the custom CFLAGS/LDFLAGS used by a specific library. In case you installed a package in a non standard path, you need to update these environment libraries so that system tools will be able to detect the package components. This is especially required when running a configure script for a package relying on other installed libraries/headers/tools. Environment variables are usually defined in the profile file, for example .profile defined in the user directory for sh/bash users, and in /etc/profile. This file can be edited to permanently set the custom environment. Alternatively, the variables can be set in a script or in a particular shell session. Remember to export the variables to the child process, e.g. using the export command. Read the fine documentation of your shell for more detailed information. MAC下的动态链接库扩展名Windows下.DLL,Linux下.so,Mac OS X下的扩展名是.dylib。 .dylib是Mach-O格式，也就是Mac OS X下的二进制文件格式。Mac OS X提供了一系列 工具，用于创建和访问动态链接库。 编译器/usr/bin/cc，也就是gcc了，Apple改过的。这个主要还是一个壳，去调用其他 的一些部件。当然同时还有/usr/bin/c++，等等。 汇编器/usr/bin/as 链接器/usr/bin/ld MAC下创建动态链接库步骤: 首先是生成module文件，也就是.o文件。这跟一般的unix没什么区别。例如cc -c a.c b.c,就得到a.o和b.o 可以用ld来合并.o文件，比如ld -r -o c.o a.o b.o 然后可以用libtool来创建动态链接库:libtool -dynamic -o c.dylib a.o b.o.（ 这里也可以用libtool -static -o c.a a.o b.o就创建静态库） 如果用gcc直接编译，我记得linux下一般是可以: gcc -shared -o c.so a.c b.c 而在Mac OS X下需要: gcc -dynamiclib -o c.dylib a.c b.c 动态链接库的工具nm是最常用的，这个用法跟linux下差不多:nm c.dylib,可以看到导出符号表，等等。 另一个常用的工具是otool，这个是Mac OS X独有的。比如想看看c.dylib的依赖关系otool -L c.dylib 官网方法 CompilationGuide-Generic CompilationGuide-MacOSX 编译ffmpeg3.3结果没有ffplay因为系统没有sdl环境或sdl版本不匹配,ffmpeg3.3需要sdl2 http://www.libsdl.org/download-2.0.php 下载Source Code SDL2-2.0.5.zip - GPG signed,解压缩,执行命令: 123./configure make sudo make install 进行编译]]></content>
      <categories>
        <category>FFMPEG</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>FFMPEG</tag>
        <tag>Mac</tag>
        <tag>音视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenGL Frame Buffer Object(FBO)]]></title>
    <url>%2F2017%2F06%2F03%2Fgl-fbo%2F</url>
    <content type="text"><![CDATA[Update: Framebuffer object extension is promoted as a core feature of OpenGL version 3.0, and is approved by ARB combining the following extensions; EXT_framebuffer_object EXT_framebuffer_blit EXT_framebuffer_multisample EXT_packed_depth_stencil OverviewIn OpenGL rendering pipeline, the geometry data and textures are transformed and passed several tests, and then finally rendered onto a screen as 2D pixels. The final rendering destination of the OpenGL pipeline is called framebuffer. Framebuffer is a collection of 2D arrays or storages utilized by OpenGL; colour buffers, depth buffer, stencil buffer and accumulation buffer. By default, OpenGL uses the framebuffer as a rendering destination that is created and managed entirely by the window system. This default framebuffer is called window-system-provided framebuffer. 在OpenGL渲染管线中，几何数据和纹理经过多次转化和多次测试，最后以二维像素的形式显示在屏幕上。OpenGL管线的最终渲染目的地被称作帧缓存（framebuffer）。帧缓冲是一些二维数组和OpenG所使用的存储区的集合：颜色缓存、深度缓存、模板缓存和累计缓存。一般情况下，帧缓存完全由window系统生成和管理，由OpenGL使用。这个默认的帧缓存被称作“window系统生成”（window-system-provided）的帧缓存。 The OpenGL extension, GL_ARB_framebuffer_object provides an interface to create additional non-displayable framebuffer objects (FBO). This framebuffer is called application-created framebuffer in order to distinguish from the default window-system-provided framebuffer. By using framebuffer object (FBO), an OpenGL application can redirect the rendering output to the application-created framebuffer object (FBO) other than the traditional window-system-provided framebuffer. And, it is fully controlled by OpenGL. 在OpenGL扩展中，GL_EXT_framebuffer_object提供了一种创建额外的不能显示的帧缓存对象的接口。为了和默认的“window系统生成”的帧缓存区别，这种帧缓冲成为应用程序帧缓存（application-createdframebuffer）。通过使用帧缓存对象（FBO），OpenGL可以将显示输出到引用程序帧缓存对象，而不是传统的“window系统生成”帧缓存。而且，它完全受OpenGL控制。 Similar to window-system-provided framebuffer, a FBO contains a collection of rendering destinations; color, depth and stencil buffer. (Note that accumulation buffer is not defined in FBO.) These logical buffers in a FBO are called framebuffer-attachable images, which are 2D arrays of pixels that can be attached to a framebuffer object. 相似于window系统提供的帧缓存，一个FBO也包含一些存储颜色、深度和模板数据的区域。（注意：没有累积缓存）我们把FBO中这些逻辑缓存称之为“帧缓存关联图像”，它们是一些能够和一个帧缓存对象关联起来的二维数组像素。 There are two types of framebuffer-attachable images; texture images and renderbuffer images. If an image of a texture object is attached to a framebuffer, OpenGL performs “render to texture”. And if an image of a renderbuffer object is attached to a framebuffer, then OpenGL performs “offscreen rendering”. 有两种类型的“帧缓存关联图像”：纹理图像（texture images）和渲染缓存图像（renderbuffer images）。如果纹理对象的图像数据关联到帧缓存，OpenGL执行的是“渲染到纹理”（render to texture）操作。如果渲染缓存的图像数据关联到帧缓存，OpenGL执行的是离线渲染（offscreen rendering）。 By the way, renderbuffer object is a new type of storage object defined in GL_ARB_framebuffer_object extension. It is used as a rendering destination for a single 2D image during rendering process. 这里要提到的是，渲染缓存对象是在GL_EXT_framebuffer_object扩展中定义的一种新的存储类型。在渲染过程中它被用作存储单幅二维图像。 The following diagram shows the connectivity among the framebuffer object, texture object and renderbuffer object. Multiple texture objects or renderbuffer objects can be attached to a framebuffer object through the attachment points. 下面这幅图显示了帧缓存对象、纹理对象和渲染缓存对象之间的联系。多多个纹理对象或者渲染缓存对象能够通过关联点关联到一个帧缓存对象上。 There are multiple color attachment points (GL_COLOR_ATTACHMENT0,…, GL_COLOR_ATTACHMENTn), one depth attachment point (GL_DEPTH_ATTACHMENT), and one stencil attachment point (GL_STENCIL_ATTACHMENT) in a framebuffer object. The number of color attachment points is implementation dependent, but each FBO must have at least one color attachement point. You can query the maximum number of color attachement points with GL_MAX_COLOR_ATTACHMENTS, which are supported by a graphics card. The reason that a FBO has multiple color attachement points is to allow to render the color buffer to multiple destinations at the same time. This “multiple render targets” (MRT) can be accomplished by GL_ARB_draw_buffers extension. Notice that the framebuffer object itself does not have any image storage(array) in it, but, it has only multiple attachment points. 在一个帧缓存对象中有多个颜色关联点（GL_COLOR_ATTACHMENT0_EXT,…,GL_COLOR_ATTACHMENTn_EXT），一个深度关联点（GL_DEPTH_ATTACHMENT_EXT），和一个模板关联点（GL_STENCIL_ATTACHMENT_EXT）。每个FBO中至少有一个颜色关联点，其数目与实体显卡相关。可以通过GL_MAX_COLOR_ATTACHMENTS_EXT来查询颜色关联点的最大数目。FBO有多个颜色关联点的原因是这样可以同时将颜色而换成渲染到多个FBO关联区。这种“多渲染目标”（multiple rendertargets,MRT）可以通过GL_ARB_draw_buffers扩展实现。需要注意的是：FBO本身并没有任何图像存储区，只有多个关联点。 Framebuffer object (FBO) provides an efficient switching mechanism; detach the previous framebuffer-attachable image from a FBO, and attach a new framebuffer-attachable image to the FBO. Switching framebuffer-attachable images is much faster than switching between FBOs. FBO provides glFramebufferTexture2D() to switch 2D texture objects, and glFramebufferRenderbuffer() to switch renderbuffer objects. FBO提供了一种高效的切换机制；将前面的帧缓存关联图像从FBO分离，然后把新的帧缓存关联图像关联到FBO。在帧缓存关联图像之间切换比在FBO之间切换要快得多。FBO提供了glFramebufferTexture2DEXT()来切换2D纹理对象和glFramebufferRenderbufferEXT()来切换渲染缓存对象。 Creating Frame Buffer Object (FBO)Creating framebuffer objects is similar to generating vertex buffer objects (VBO). 创建FBO和产生VBO类似。 12void glGenFramebuffers(GLsizei n, GLuint* ids)void glDeleteFramebuffers(GLsizei n, const GLuint* ids) glGenFramebuffers() requires 2 parameters; the first one is the number of framebuffers to create, and the second parameter is the pointer to a GLuint variable or an array to store a single ID or multiple IDs. It returns the IDs of unused framebuffer objects. ID 0 means the default framebuffer, which is the window-system-provided framebuffer. glGenFramebuffersEXT()需要两个参数：第一个是要创建的帧缓存的数目，第二个是指向存储一个或者多个ID的变量或数组的指针。它返回未使用的FBO的ID。ID为0表示默认帧缓存，即window系统提供的帧缓存。 And, FBO may be deleted by calling glDeleteFramebuffers() when it is not used anymore. 当FBO不再被使用时，FBO可以通过调用glDeleteFrameBuffersEXT()来删除。 1glBindFramebuffer() Once a FBO is created, it has to be bound before using it. 一旦一个FBO被创建，在使用它之前必须绑定。 1void glBindFramebuffer(GLenum target, GLuint id) The first parameter, target, should be GL_FRAMEBUFFER, and the second parameter is the ID of a framebuffer object. Once a FBO is bound, all OpenGL operations affect onto the current bound framebuffer object. The object ID 0 is reserved for the default window-system provided framebuffer. Therefore, in order to unbind the current framebuffer (FBO), use ID 0 in glBindFramebuffer(). 第一个参数target应该是GL_FRAMEBUFFER_EXT，第二个参数是FBO的ID号。一旦FBO被绑定，之后的所有的OpenGL操作都会对当前所绑定的FBO造成影响。ID号为0表示缺省帧缓存，即默认的window提供的帧缓存。因此，在glBindFramebufferEXT()中将ID号设置为0可以解绑定当前FBO。 Renderbuffer ObjectIn addition, renderbuffer object is newly introduced for offscreen rendering. It allows to render a scene directly to a renderbuffer object, instead of rendering to a texture object. Renderbuffer is simply a data storage object containing a single image of a renderable internal format. It is used to store OpenGL logical buffers that do not have corresponding texture format, such as stencil or depth buffer. 另外，渲染缓存是为离线渲染而新引进的。它允许将一个场景直接渲染到一个渲染缓存对象中，而不是渲染到纹理对象中。渲染缓存对象是用于存储单幅图像的数据存储区域。该图像按照一种可渲染的内部格式存储。它用于存储没有相关纹理格式的OpenGL逻辑缓存，比如模板缓存或者深度缓存。 glGenRenderbuffers()12void glGenRenderbuffers(GLsizei n, GLuint* ids)void glDeleteRenderbuffers(GLsizei n, const Gluint* ids) Once a renderbuffer is created, it returns non-zero positive integer. ID 0 is reserved for OpenGL. 一旦一个渲染缓存被创建，它返回一个非零的正整数。ID为0是OpenGL保留值。 glBindRenderbuffer()1void glBindRenderbuffer(GLenum target, GLuint id) Same as other OpenGL objects, you have to bind the current renderbuffer object before referencing it. The target parameter should be GL_RENDERBUFFER for renderbuffer object. 和OpenGL中其他对象一样，在引用渲染缓存之前必须绑定当前渲染缓存对象。他target参数应该是GL_RENDERBUFFER_EXT。 glRenderbufferStorage()1234void glRenderbufferStorage(GLenum target, GLenum internalFormat, GLsizei width, GLsizei height) When a renderbuffer object is created, it does not have any data storage, so we have to allocate a memory space for it. This can be done by using glRenderbufferStorage(). The first parameter must be GL_RENDERBUFFER. The second parameter would be color-renderable (GL_RGB, GL_RGBA, etc.), depth-renderable (GL_DEPTH_COMPONENT), or stencil-renderable formats (GL_STENCIL_INDEX). The width and height are the dimension of the renderbuffer image in pixels. 当一个渲染缓存被创建，它没有任何数据存储区域，所以我们还要为他分配空间。这可以通过用glRenderbufferStorageEXT()实现。第一个参数必须是GL_RENDERBUFFER_EXT。第二个参数可以是用于颜色的（GL_RGB，GL_RGBA，etc.），用于深度的（GL_DEPTH_COMPONENT），或者是用于模板的格式（GL_STENCIL_INDEX）。Width和height是渲染缓存图像的像素维度。 The width and height should be less than GL_MAX_RENDERBUFFER_SIZE, otherwise, it generates GL_INVALID_VALUE error. width和height必须比GL_MAX_RENDERBUFFER_SIZE_EXT小，否则将会产生GL_UNVALID_VALUE错误。 glGetRenderbufferParameteriv()123void glGetRenderbufferParameteriv(GLenum target, GLenum param, GLint* value) You also get various parameters of the currently bound renderbuffer object. target should be GL_RENDERBUFFER, and the second parameter is the name of parameter. The last is the pointer to an integer variable to store the returned value. The available names of the renderbuffer parameters are; 我们也可以得到当前绑定的渲染缓存对象的一些参数。Target应该是GL_RENDERBUFFER_EXT，第二个参数是所要得到的参数名字。最后一个是指向存储返回值的整型量的指针。渲染缓存的变量名有如下: 12345678910GL_RENDERBUFFER_WIDTHGL_RENDERBUFFER_HEIGHTGL_RENDERBUFFER_INTERNAL_FORMATGL_RENDERBUFFER_RED_SIZEGL_RENDERBUFFER_GREEN_SIZEGL_RENDERBUFFER_BLUE_SIZEGL_RENDERBUFFER_ALPHA_SIZEGL_RENDERBUFFER_DEPTH_SIZEGL_RENDERBUFFER_STENCIL_SIZE Attaching images to FBOFBO itself does not have any image storage(buffer) in it. Instead, we must attach framebuffer-attachable images (texture or renderbuffer objects) to the FBO. This mechanism allows that FBO quickly switch (detach and attach) the framebuffer-attachable images in a FBO. It is much faster to switch framebuffer-attachable images than to switch between FBOs. And, it saves unnecessary data copies and memory consumption. For example, a texture can be attached to multiple FBOs, and its image storage can be shared by multiple FBOs. FBO本身没有图像存储区。我们必须帧缓存关联图像（纹理或渲染对象）关联到FBO。这种机制允许FBO快速地切换（分离和关联）帧缓存关联图像。切换帧缓存关联图像比在FBO之间切换要快得多。而且，它节省了不必要的数据拷贝和内存消耗。比如，一个纹理可以被关联到多个FBO上，图像存储区可以被多个FBO共享。 Attaching a 2D texture image to FBO12345glFramebufferTexture2D(GLenum target, GLenum attachmentPoint, GLenum textureTarget, GLuint textureId, GLint level) glFramebufferTexture2D() is to attach a 2D texture image to a FBO. The first parameter must be GL_FRAMEBUFFER, and the second parameter is the attachment point where to connect the texture image. A FBO has multiple color attachment points (GL_COLOR_ATTACHMENT0, …, GL_COLOR_ATTACHMENTn), GL_DEPTH_ATTACHMENT, and GL_STENCIL_ATTACHMENT. The third parameter, “textureTarget” is GL_TEXTURE_2D in most cases. The fourth parameter is the identifier of the texture object. The last parameter is the mipmap level of the texture to be attached. glFramebufferTexture2DEXT()把一幅纹理图像关联到一个FBO。第一个参数一定是GL_FRAMEBUFFER_EXT，第二个参数是关联纹理图像的关联点。第三个参数textureTarget在多数情况下是GL_TEXTURE_2D。第四个参数是纹理对象的ID号。最后一个参数是要被关联的纹理的mipmap等级 If the textureId parameter is set to 0, then, the texture image will be detached from the FBO. If a texture object is deleted while it is still attached to a FBO, then, the texture image will be automatically detached from the currently bound FBO. However, if it is attached to multiple FBOs and deleted, then it will be detached from only the bound FBO, but will not be detached from any other un-bound FBOs. 如果参数textureId被设置为0，那么纹理图像将会被从FBO分离。如果纹理对象在依然关联在FBO上时被删除，那么纹理对象将会自动从当前帮的FBO上分离。然而，如果它被关联到多个FBO上然后被删除，那么它将只被从绑定的FBO上分离，而不会被从其他非绑定的FBO上分离。 Attaching a Renderbuffer image to FBO1234void glFramebufferRenderbuffer(GLenum target, GLenum attachmentPoint, GLenum renderbufferTarget, GLuint renderbufferId) A renderbuffer image can be attached by calling glFramebufferRenderbuffer(). The first and second parameters are same as glFramebufferTexture2D(). The third parameter must be GL_RENDERBUFFER, and the last parameter is the ID of the renderbuffer object. 通过调用glFramebufferRenderbufferEXT()可以关联渲染缓存图像。前两个参数和glFramebufferTexture2DEXT()一样。第三个参数只能是GL_RENDERBUFFER_EXT，最后一个参数是渲染缓存对象的ID号。 If renderbufferId parameter is set to 0, the renderbuffer image will be detached from the attachment point in the FBO. If a renderbuffer object is deleted while it is still attached in a FBO, then it will be automatically detached from the bound FBO. However, it will not be detached from any other non-bound FBOs. 如果参数renderbufferId被设置为0，渲染缓存图像将会从FBO的关联点分离。如果渲染缓存图像在依然关联在FBO上时被删除，那么纹理对象将会自动从当前绑定的FBO上分离，而不会从其他非绑定的FBO上分离。 FBO with MSAA (Multi Sample Anti Aliasing)When you render to a FBO, anti-aliasing is not automatically enabled even if you properly create a OpenGL rendering context with the multisampling attribute (SAMPLEBUFFERS_ARB) for window-system-provided framebuffer. In order to activate multisample anti-aliasing mode for rendering to a FBO, you need to prepare and attach multisample images to a FBO’s color and/or depth attachement points. FBO extension provides glRenderbufferStorageMultisample() to create a renderbuffer image for multisample anti-aliasing rendering mode. 12345void glRenderbufferStorageMultisample(GLenum target, GLsizei samples, GLenum internalFormat, GLsizei width, GLsizei height) It adds new parameter, samples on top of glRenderbufferStorage(), which is the number of multisamples for anti-aliased rendering mode. If it is 0, then no MSAA mode is enabled and glRenderbufferStorage() is called instead. You can query the maximum number of samples with GL_MAX_SAMPLES token in glGetIntegerv(). The following code is to create a FBO with multisample colorbuffer and depthbuffer images. Note that if multiple images are attached to a FBO, then all images must have the same number of multisamples. Otherwise, the FBO status is incomplete. 12345678910111213141516171819202122232425262728293031323334// create a 4x MSAA renderbuffer object for colorbufferint msaa = 4;GLuint rboColorId;glGenRenderbuffers(1, &amp;rboColorId);glBindRenderbuffer(GL_RENDERBUFFER, rboColorId);glRenderbufferStorageMultisample(GL_RENDERBUFFER, msaa, GL_RGB8, width, height);// create a 4x MSAA renderbuffer object for depthbufferGLuint rboDepthId;glGenRenderbuffers(1, &amp;rboDepthId);glBindRenderbuffer(GL_RENDERBUFFER, rboDepthId);glRenderbufferStorageMultisample(GL_RENDERBUFFER, msaa, GL_DEPTH_COMPONENT, width, height);// create a 4x MSAA framebuffer objectGLuint fboId;glGenFramebuffers(1, &amp;fboMsaaId);glBindFramebuffer(GL_FRAMEBUFFER, fboMsaaId);// attach colorbuffer image to FBOglFramebufferRenderbuffer(GL_FRAMEBUFFER, // 1. fbo target: GL_FRAMEBUFFER GL_COLOR_ATTACHMENT0, // 2. color attachment point GL_RENDERBUFFER, // 3. rbo target: GL_RENDERBUFFER rboColorId); // 4. rbo ID// attach depthbuffer image to FBOglFramebufferRenderbuffer(GL_FRAMEBUFFER, // 1. fbo target: GL_FRAMEBUFFER GL_DEPTH_ATTACHMENT, // 2. depth attachment point GL_RENDERBUFFER, // 3. rbo target: GL_RENDERBUFFER rboDepthId); // 4. rbo ID// check FBO statusGLenum status = glCheckFramebufferStatus(GL_FRAMEBUFFER);if(status != GL_FRAMEBUFFER_COMPLETE) fboUsed = false; It is important to know that glRenderbufferStorageMultisample() only enables MSAA rendering to FBO. However, you cannot directly use the result from MSAA FBO. If you need to transfer the result to a texture or other non-multisampled framebuffer, you have to convert (downsample) the result to single-sample image using glBlitFramebuffer(). 1234void glBlitFramebuffer(GLint srcX0, GLint srcY0, GLint srcX1, GLint srcY1, // source rectangle GLint dstX0, GLint dstY0, GLint dstX1, GLint dstY1, // destination rect GLbitfield mask, GLenum filter) glBlitFramebuffer() copies a rectangle of images from the source (GL_READ_BUFFER) to the destination framebuffer (GL_DRAW_BUFFER). The “mask” parameter is to specify which buffers are copied, GL_COLOR_BUFFER_BIT, GL_DEPTH_BUFFER_BIT and/or GL_STENCIL_BUFFER_BIT. The last parameter, “filter” is to specify the interpolation mode if the source and destination rectangles are not same. It is either GL_NEAREST or GL_LINEAR. The following code is to transfer a multisampled image from a FBO to another non-multisampled FBO. Notice it requires an additional FBO to get the result of MSAA rendering. Please see fboMsaa.zip for details to perform render-to-texture with MSAA. 12345678910// copy rendered image from MSAA (multi-sample) to normal (single-sample)// NOTE: The multi samples at a pixel in read buffer will be converted// to a single sample at the target pixel in draw buffer.glBindFramebuffer(GL_READ_FRAMEBUFFER, fboMsaaId); // src FBO (multi-sample)glBindFramebuffer(GL_DRAW_FRAMEBUFFER, fboId); // dst FBO (single-sample)glBlitFramebuffer(0, 0, width, height, // src rect 0, 0, width, height, // dst rect GL_COLOR_BUFFER_BIT, // buffer mask GL_LINEAR); // scale filter Checking FBO StatusOnce attachable images (textures and renderbuffers) are attached to a FBO and before performing FBO operation, you must validate if the FBO status is complete or incomplete by using glCheckFramebufferStatus(). If the FBO is not complete, then any drawing and reading command (glBegin(), glCopyTexImage2D(), etc) will be failed. 一旦关联图像（纹理和渲染缓存）被关联到FBO上，在执行FBO的操作之前，你必须检查FBO的状态，这可以通过调用glCheckFramebufferStatusEXT()实现。如果这个FBObuilding完整，那么任何绘制和读取命令（glBegin(),glCopyTexImage2D(), etc）都会失败。 1GLenum glCheckFramebufferStatus(GLenum target) glCheckFramebufferStatus() validates all its attached images and framebuffer parameters on the currently bound FBO. And, this function cannot be called within glBegin()/glEnd() pair. The target parameter should be GL_FRAMEBUFFER. It returns non-zero value after checking the FBO. If all requirements and rules are satisfied, then it returns GL_FRAMEBUFFER_COMPLETE. Otherwise, it returns a relevant error value, which tells what rule is violated. glCheckFramebufferStatusEXT()检查当前帧缓存的关联图像和帧缓存参数。这个函数不能在glBegin()/glEnd()之间调用。Target参数必须为GL_FRAMEBUFFER_EXT。它返回一个非零值。如果所有要求和准则都满足，它返回GL_FRAMEBUFFER_COMPLETE_EXT。否则，返回一个相关错误代码告诉我们哪条准则没有满足。 The rules of FBO completeness are: The width and height of framebuffer-attachable image must be not zero. If an image is attached to a color attachment point, then the image must have a color-renderable internal format. (GL_RGBA, GL_DEPTH_COMPONENT, GL_LUMINANCE, etc) If an image is attached to GL_DEPTH_ATTACHMENT, then the image must have a depth-renderable internal format. (GL_DEPTH_COMPONENT, GL_DEPTH_COMPONENT24, etc) If an image is attached to GL_STENCIL_ATTACHMENT, then the image must have a stencil-renderable internal format. (GL_STENCIL_INDEX, GL_STENCIL_INDEX8, etc) FBO must have at least one image attached. All images attached a FBO must have the same width and height. All images attached the color attachment points must have the same internal format. FBO完整性准则有： 帧缓存关联图像的宽度和高度必须非零。 如果一幅图像被关联到一个颜色关联点，那么这幅图像必须有颜色可渲染的内部格式（GL_RGBA, GL_DEPTH_COMPONENT, GL_LUMINANCE, etc)。 如果一幅被图像关联到GL_DEPTH_ATTACHMENT_EXT，那么这幅图像必须有深度可渲染的内部格式(GL_DEPTH_COMPONENT,GL_DEPTH_COMPONENT24_EXT, etc)。 如果一幅被图像关联到GL_STENCIL_ATTACHMENT_EXT，那么这幅图像必须有模板可渲染的内部格式(GL_STENCIL_INDEX,GL_STENCIL_INDEX8_EXT, etc)。 FBO至少有一幅图像关联。 被关联到FBO的缩影图像必须有相同的宽度和高度。 被关联到颜色关联点上的所有图像必须有相同的内部格式。 Note that even though all of the above conditions are satisfied, your OpenGL driver may not support some combinations of internal formats and parameters. If a particular implementation is not supported by OpenGL driver, then glCheckFramebufferStatus() returns GL_FRAMEBUFFER_UNSUPPORTED. 注意：即使以上所有条件都满足，你的OpenGL驱动也可能不支持某些格式和参数的组合。如果一种特别的实现不被OpenGL驱动支持，那么glCheckFramebufferStatusEXT()返回GL_FRAMEBUFFER_UNSUPPORTED_EXT。 The sample code provides some utility functions to report the information of the current FBO; printFramebufferInfo() and checkFramebufferStatus(). Java Code Examples for javax.media.opengl.GL.GL_FRAMEBUFFER_COMPLETE_EXT GL_EXT_discard_framebufferOverviewThis extension provides a new command, DiscardFramebufferEXT, which causes the contents of the named framebuffer attachable images to become undefined. The contents of the specified buffers are undefined until a subsequent operation modifies the content, and only the modified region is guaranteed to hold valid content. Effective usage of this command may provide an implementation with new optimization opportunities. Some OpenGL ES implementations cache framebuffer images in a small pool of fast memory. Before rendering, these implementations must load the existing contents of one or more of the logical buffers (color, depth, stencil, etc.) into this memory. After rendering, some or all of these buffers are likewise stored back to external memory so their contents can be used again in the future. In many applications, some or all of the logical buffers are cleared at the start of rendering. If so, the effort to load or store those buffers is wasted. Even without this extension, if a frame of rendering begins with a full-screen Clear, an OpenGL ES implementation may optimize away the loading of framebuffer contents prior to rendering the frame. With this extension, an application can use DiscardFramebufferEXT to signal that framebuffer contents will no longer be needed. In this case an OpenGL ES implementation may also optimize away the storing back of framebuffer contents after rendering the frame. Issues1) Should DiscardFramebufferEXT’s argument be a list of COLOR_ATTACHMENTx enums, or should it use the same bitfield from Clear and BlitFramebuffer? RESOLVED: We’ll use a sized list of framebuffer attachments. This will give us some future-proofing for when MRTs and multisampled FBOs are supported. 2) What happens if the app discards only one of the depth and stencil attachments, but those are backed by the same packed_depth_stencil buffer? a) Generate an error b) Both images become undefined c) Neither image becomes undefined d) Only one of the images becomes undefined RESOLVED: (b) which sort of falls out of Issue 4. 3) How should DiscardFramebufferEXT interact with the default framebuffer? a) Generate an error b) Ignore the hint silently c) The contents of the specified attachments become undefined RESOLVED: (c), with appropriate wording to map FBO attachments to the corresponding default framebuffer’s logical buffers 4) What happens when you discard an attachment that doesn’t exist? This is the case where a framebuffer is complete but doesn’t have, for example, a stencil attachment, yet the app tries to discard the stencil attachment. a) Generate an error b) Ignore the hint silently RESOLVED: (b) for two reasons. First, this is just a hint anyway, and if we required error detection, then suddenly an implementation can’t trivially ignore it. Second, this is consistent with Clear, which ignores specified buffers that aren’t present. Example: Render To TextureSometimes, you need to generate dynamic textures on the fly. The most common examples are generating mirroring/reflection effects, dynamic cube/environment maps and shadow maps. Dynamic texturing can be accomplished by rendering the scene to a texture. A traditional way of render-to-texture is to draw a scene to the framebuffer as normal, and then copy the framebuffer image to a texture by using glCopyTexSubImage2D(). 有时候，你需要产生动态纹理。比较常见的例子是产生镜面反射效果、动态环境贴图和阴影等效果。动态纹理可以通过把场景渲染到纹理来实现。渲染到纹理的一种传统方式是将场景绘制到普通的帧缓存上，然后调用glCopyTexSubImage2D()拷贝帧缓存图像至纹理。 Using FBO, we can render a scene directly onto a texture, so we don’t have to use the window-system-provided framebuffer at all. Further more, we can eliminate an additional data copy (from framebuffer to texture). 使用FBO，我们能够将场景直接渲染到纹理，所以我们不必使用window系统提供的帧缓存。并且，我们能够去除额外的数据拷贝（从帧缓存到纹理）；。 This demo program performs render to texture operation with/without FBO, and compares the performance difference. Other than performance gain, there is another advantage of using FBO. If the texture resolution is larger than the size of the rendering window in traditional render-to-texture mode (without FBO), then the area out of the window region will be clipped. However, FBO does not suffer from this clipping problem. You can create a framebuffer-renderable image larger than the display window. 这个demo实现了使用FBO和不使用FBO两种情况下渲染到纹理的操作，并且比较了性能差异。除了能够获得性能上的提升，使用FBO的还有另外一个优点。在传统的渲染到纹理的模式中（不使用FBO），如果纹理分辨率比渲染窗口的尺寸大，超出窗口区域的部分将被剪切掉。然后，使用FBO就不会有这个问题。你可以产生比显示窗口大的帧缓存渲染图像。 The following codes is to setup a FBO and framebuffer-attachable images before the rendering loop is started. Note that not only a texture image is attached to the FBO, but also, a renderbuffer image is attached to the depth attachment point of the FBO. We do not actually use this depth buffer, however, the FBO itself needs it for depth test. If we don’t attach this depth renderable image to the FBO, then the rendering output will be corrupted because of missing depth test. If stencil test is also required during FBO rendering, then additional renderbuffer image should be attached to GL_STENCIL_ATTACHMENT. 以下代码在渲染循环开始之前，对FBO和帧缓存关联图像进行了初始化。注意只有一幅纹理图像被关联到FBO，但是，一个深度渲染图像被关联到FBO的深度关联点。实际上我们并没有使用这个深度缓存，但是FBO本身需要它进行深度测试。如果我们不把这个深度可渲染的图像关联到FBO，那么由于缺少深度测试渲染输出结果是不正确的。如果在FBO渲染期间模板测试也是必要的，那么也需要把额外的渲染图像和GL_STENCIL_ATTACHMENT_EXT关联起来。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748...// create a texture objectGLuint textureId;glGenTextures(1, &amp;textureId);glBindTexture(GL_TEXTURE_2D, textureId);glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR);glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);glTexParameteri(GL_TEXTURE_2D, GL_GENERATE_MIPMAP, GL_TRUE); // automatic mipmapglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA8, TEXTURE_WIDTH, TEXTURE_HEIGHT, 0, GL_RGBA, GL_UNSIGNED_BYTE, 0);glBindTexture(GL_TEXTURE_2D, 0);// create a renderbuffer object to store depth infoGLuint rboId;glGenRenderbuffers(1, &amp;rboId);glBindRenderbuffer(GL_RENDERBUFFER, rboId);glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, TEXTURE_WIDTH, TEXTURE_HEIGHT);glBindRenderbuffer(GL_RENDERBUFFER, 0);// create a framebuffer objectGLuint fboId;glGenFramebuffers(1, &amp;fboId);glBindFramebuffer(GL_FRAMEBUFFER, fboId);// attach the texture to FBO color attachment pointglFramebufferTexture2D(GL_FRAMEBUFFER, // 1. fbo target: GL_FRAMEBUFFER GL_COLOR_ATTACHMENT0, // 2. attachment point GL_TEXTURE_2D, // 3. tex target: GL_TEXTURE_2D textureId, // 4. tex ID 0); // 5. mipmap level: 0(base)// attach the renderbuffer to depth attachment pointglFramebufferRenderbuffer(GL_FRAMEBUFFER, // 1. fbo target: GL_FRAMEBUFFER GL_DEPTH_ATTACHMENT, // 2. attachment point GL_RENDERBUFFER, // 3. rbo target: GL_RENDERBUFFER rboId); // 4. rbo ID// check FBO statusGLenum status = glCheckFramebufferStatus(GL_FRAMEBUFFER);if(status != GL_FRAMEBUFFER_COMPLETE) fboUsed = false;// switch back to window-system-provided framebufferglBindFramebuffer(GL_FRAMEBUFFER, 0);... The rendering procedure of render-to-texture is almost same as normal drawing. We only need to switch the rendering destination from the window-system-provided to the non-displayable, application-created framebuffer (FBO). 渲染到纹理的过程和普通的绘制过程基本一样。我们只需要把渲染的目的地由window系统提供的帧缓存改成不可显示的应用程序创建的帧缓存（FBO）就可以了。 123456789101112131415161718192021...// set rendering destination to FBOglBindFramebuffer(GL_FRAMEBUFFER, fboId);// clear buffersglClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);// draw a scene to a texture directlydraw();// unbind FBOglBindFramebuffer(GL_FRAMEBUFFER, 0);// trigger mipmaps generation explicitly// NOTE: If GL_GENERATE_MIPMAP is set to GL_TRUE, then glCopyTexSubImage2D()// triggers mipmap generation automatically. However, the texture attached// onto a FBO should generate mipmaps manually via glGenerateMipmap().glBindTexture(GL_TEXTURE_2D, textureId);glGenerateMipmap(GL_TEXTURE_2D);glBindTexture(GL_TEXTURE_2D, 0);... Note that glGenerateMipmap() is also included as part of FBO extension in order to generate mipmaps explicitly after modifying the base level texture image. If GL_GENERATE_MIPMAP is set to GL_TRUE, then glTex{Sub}Image2D() and glCopyTex{Sub}Image2D() trigger automatic mipmap generation (in OpenGL version 1.4 or greater). However, FBO operation does not generate its mipmaps automatically when the base level texture is modified because FBO does not call glCopyTex{Sub}Image2D() to modify the texture. Therefore, glGenerateMipmap() must be explicitly called for mipmap generation. 注意到，glGenerateMipmapEXT()也是作为FBO扩展的一部分，用来在改变了纹理图像的基级之后显式生成mipmap的。如果GL_GENERATE_MIPMAP被设置为GL_TRUE，那么glTex{Sub}Image2D()和 glCopyTex{Sub}Image2D()将会启用自动mipmap生成（在OpenGL版本1.4或者更高版本中）。然后，当纹理基级被改变时，FBO操作不会自动产生mipmaps。因为FBO不会调用glCopyTex{Sub}Image2D()来修改纹理。因此，要产生mipmap，glGenerateMipmapEXT()必须被显示调用。 If you need to a post processing of the texture, it is possible to combine with Pixel Buffer Object (PBO) to modify the texture efficiently.PBuffer vs FBOopengles2.0渲染到纹理的方法有三种： 使用glCopyTexImage2D或者glCopyTexSubImage2D，这两个函数，复制framebuffer中的像素到纹理缓存里面，但这两个函数性能比较低下，并且要求纹理的尺寸必须小于等于framebuffer的尺寸。 使用一个附加到纹理的pbuffer，来执行渲染到纹理的操作。我们知道，窗口系统为我们提供的surface必须添加到一个渲染环境里面，但是，在某些平台上要求每个pbuffer和窗口系统提供的surface都需要一个单独的context，所以如果要渲染到pbuffer里面的话，就会发生context的切换，这种切换操作时很耗时的。 使用fbo，rbo等，这种是最高效的。 pbuffer跟framebuffer功能是一样的，都是用来做渲染到一个off-screen surface上的，但是如果要做的是渲染到一个纹理上，还是使用framebuffer，效率高些。pbuffer的用途是：渲染到纹理上，随后这个纹理可以给其他API用的，比如openVG。创建pbuffer的过程跟创建窗口surface差不多的： 1EGLSurface eglCreatePbufferSurface(EGLDisplay display,EGLConfig config,const EGLint *attribList); 需要在attribList指定一些pbuffer的属性。选择config的时候需要指定：EGL_SURFACE_TYPE：EGL_PBUFFER_BIT 频繁的在自己创建的fbo和窗口系统创建的vbo之间切换，比较影响性能。不要在每一帧都去创建，销毁fbo，vbo对象。要一次创建多次使用。如果一个纹理attach到一个fbo的attachment point，就要尽量避免调用glTexImage2D或glTexSubImage2D,glCopyTexImage2D等去修改纹理的值。 Presumably eglSwapBuffers has no effect on PBufferSurface (since it is not double-buffer surface) but if it is you would try to read pixels from undefined buffer, with undefined result..引用 原文 译文 GL_EXT_discard_framebuffer]]></content>
      <categories>
        <category>OpenGL</category>
      </categories>
      <tags>
        <tag>OpenGL</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android textureview处理预览摄像头变形问题]]></title>
    <url>%2F2017%2F05%2F04%2Fandroid-view-textureview%2F</url>
    <content type="text"><![CDATA[当TextureView的大小和匹配到的摄像头PreviewSize宽高比例不完全一致时,TextureView可通过setTransform函数对预览画面进行处理后再显示到TextureView,如下对图形居中裁剪: 1234567891011121314151617181920212223242526272829303132333435363738public void sizeNotify(Camera.Size size) &#123; float viewWidth = getWidth(); float viewHeight = getHeight(); float scaleX = 1.0f; float scaleY = 1.0f; int mPreviewWidth = size.width; int mPreviewHeight = size.height; if(viewWidth &lt; viewHeight) &#123; mPreviewWidth = size.height; mPreviewHeight = size.width; &#125; if (mPreviewWidth &gt; viewWidth &amp;&amp; mPreviewHeight &gt; viewHeight) &#123; scaleX = mPreviewWidth / viewWidth; scaleY = mPreviewHeight / viewHeight; &#125; else if (mPreviewWidth &lt; viewWidth &amp;&amp; mPreviewHeight &lt; viewHeight) &#123; scaleY = viewWidth / mPreviewWidth; scaleX = viewHeight / mPreviewHeight; &#125; else if (viewWidth &gt; mPreviewWidth) &#123; scaleY = (viewWidth / mPreviewWidth) / (viewHeight / mPreviewHeight); &#125; else if (viewHeight &gt; mPreviewHeight) &#123; scaleX = (viewHeight / mPreviewHeight) / (viewWidth / mPreviewWidth); &#125; // Calculate pivot points, in our case crop from center int pivotPointX = (int) (viewWidth / 2); int pivotPointY = (int) (viewHeight / 2); Matrix matrix = new Matrix(); matrix.setScale(scaleX, scaleY, pivotPointX, pivotPointY); /*Log.e(TAG, &quot;viewsize:&quot; + viewWidth + &quot; * &quot; + viewHeight + &quot;;prviewSize:&quot; + mPreviewWidth + &quot; * &quot; + mPreviewHeight + &quot;;scale:&quot; + scaleX + &quot; * &quot; + scaleY + &quot;;pivot:&quot; + pivotPointX + &quot; * &quot; + pivotPointY);*/ setTransform(matrix); &#125; TextureView中setTransform函数说明: Sets the transform to associate with this texture view. The specified transform applies to the underlying surface texture and does not affect the size or position of the view itself, only of its content. Some transforms might prevent the content from drawing all the pixels contained within this view’s bounds. In such situations, make sure this texture view is not marked opaque.]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>android</tag>
        <tag>view</tag>
        <tag>TextureView</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc之Native APIs]]></title>
    <url>%2F2017%2F05%2F03%2Fwebrtc-nativeapis%2F</url>
    <content type="text"><![CDATA[Block diagram Calling sequencesSet up a call Receive a call Close down a call Threading modelWebRTC native APIs use two globally available threads: the signaling thread and the worker thread. Depending on how the PeerConnection factory is created, the application can either provide those 2 threads or just let them be created internally. The calls to the Stream APIs and the PeerConnection APIs will be proxied to the signaling thread which means that the application can call those APIs from whatever thread. All callbacks will be made on the signaling thread. The application should return the callback as quickly as possible to avoid blocking the signaling thread. Resource intensive processes should be posted to a different thread. The worker thread is used to handle more resource intensive processes such as data streaming. https://sites.google.com/site/webrtc/native-code/native-apis]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc源码走读之api]]></title>
    <url>%2F2017%2F05%2F02%2Fwebrtc-source-api%2F</url>
    <content type="text"><![CDATA[api目录下封装了webrtc相关的供外部调用接口. datachannel.h123// Including this file is deprecated. It is no longer part of the public API.// This only includes the file in its new location for backwards compatibility.#include &quot;webrtc/pc/datachannel.h&quot; datachannelinterface.h DataChannelObserver:Used to implement RTCDataChannel events.The code responding to these callbacks should unwind the stack before using any other webrtc APIs; re-entrancy is not supported. DataChannelInterface: dtmfsenderinterface.h DtmfSenderObserverInterface:DtmfSender callback interface, used to implement RTCDtmfSender events.Applications should implement this interface to get notifications from the DtmfSender. DtmfSenderInterface:The interface of native implementation of the RTCDTMFSender defined by the WebRTC W3C Editor’s Draft. fakemetricsobserver.h/cc FakeMetricsObserver jsep.h IceCandidateInterface:Class representation of an ICE candidate.An instance of this interface is supposed to be owned by one class at a time and is therefore not expected to be thread safe.An instance can be created by CreateIceCandidate. IceCandidateCollection:This class represents a collection of candidates for a specific m= section.Used in SessionDescriptionInterface. SessionDescriptionInterface:Class representation of an SDP session description.An instance of this interface is supposed to be owned by one class at a time and is therefore not expected to be thread safe.An instance can be created by CreateSessionDescription. CreateSessionDescriptionObserver:CreateOffer and CreateAnswer callback interface. SetSessionDescriptionObserver:SetLocalDescription and SetRemoteDescription callback interface. jsepicecandidate.h JsepIceCandidate:继承自IceCandidateInterface JsepCandidateCollection:继承自IceCandidateCollection jsepsessiondescription.h JsepSessionDescription:Implementation of SessionDescriptionInterface. mediaconstraintsinterface.h/cc MediaConstraintsInterface:Interface used for passing arguments about media constraints to the MediaStream and PeerConnection implementation.Constraints may be either “mandatory”, which means that unless satisfied,the method taking the constraints should fail, or “optional”, which means they may not be satisfied.. mediastream.h123// Including this file is deprecated. It is no longer part of the public API.// This only includes the file in its new location for backwards compatibility.#include &quot;webrtc/pc/mediastream.h&quot; mediastreaminterface.h/cc OberverInterface NotifierInterface MediaSourceInterface:Base class for sources. A MediaStreamTrack has an underlying source that provides media. A source can be shared by multiple tracks.继承自notifierInterface MediaStreamTrackInterface:继承自notifierInterface VideoTrackSourceInterface:VideoTrackSourceInterface is a reference counted source used for VideoTracks. The same source can be used by multiple VideoTracks.继承自MediaSourceinterface与VideoSourceInterface VideoTrackInterface: 继承自MediaStreamTrackInterface与VideoSourceInterface AudioTrackSinkinterface: AudioSourceInterface:AudioSourceInterface is a reference counted source used for AudioTracks.The same source can be used by multiple AudioTracks.继承自MediaSourceInterface. AudioProcessorInterface:Interface of the audio processor used by the audio track to collect statistics. AudioTrackInterface:继承自MediaStreamTrackInterface MediaStreamInterface: A major difference is that remote audio/video tracks (received by a PeerConnection/RtpReceiver) are not synchronized simply by adding them to the same stream; a session description with the correct “a=msid” attributes must be pushed down.Thus, this interface acts as simply a container for tracks. mediastreamproxy.hMove this to .cc file and out of api/. What threads methods // are called on is an implementation detail. mediastreamtrack.h123// Including this file is deprecated. It is no longer part of the public API.// This only includes the file in its new location for backwards compatibility.#include &quot;webrtc/pc/mediastreamtrack.h&quot; mediatypes.h/ccmediatype到string转换 notifier.h Notifier: peerconnectionfactoryproxy.hpeerconnectioninterface.h12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// This file contains the PeerConnection interface as defined in// http://dev.w3.org/2011/webrtc/editor/webrtc.html#peer-to-peer-connections.//// The PeerConnectionFactory class provides factory methods to create// PeerConnection, MediaStream and MediaStreamTrack objects.//// The following steps are needed to setup a typical call using WebRTC://// 1. Create a PeerConnectionFactoryInterface. Check constructors for more// information about input parameters.//// 2. Create a PeerConnection object. Provide a configuration struct which// points to STUN and/or TURN servers used to generate ICE candidates, and// provide an object that implements the PeerConnectionObserver interface,// which is used to receive callbacks from the PeerConnection.//// 3. Create local MediaStreamTracks using the PeerConnectionFactory and add// them to PeerConnection by calling AddTrack (or legacy method, AddStream).//// 4. Create an offer, call SetLocalDescription with it, serialize it, and send// it to the remote peer//// 5. Once an ICE candidate has been gathered, the PeerConnection will call the// observer function OnIceCandidate. The candidates must also be serialized and// sent to the remote peer.//// 6. Once an answer is received from the remote peer, call// SetRemoteDescription with the remote answer.//// 7. Once a remote candidate is received from the remote peer, provide it to// the PeerConnection by calling AddIceCandidate.//// The receiver of a call (assuming the application is &quot;call&quot;-based) can decide// to accept or reject the call; this decision will be taken by the application,// not the PeerConnection.//// If the application decides to accept the call, it should://// 1. Create PeerConnectionFactoryInterface if it doesn&apos;t exist.//// 2. Create a new PeerConnection.//// 3. Provide the remote offer to the new PeerConnection object by calling// SetRemoteDescription.//// 4. Generate an answer to the remote offer by calling CreateAnswer and send it// back to the remote peer.//// 5. Provide the local answer to the new PeerConnection by calling// SetLocalDescription with the answer.//// 6. Provide the remote ICE candidates by calling AddIceCandidate.//// 7. Once a candidate has been gathered, the PeerConnection will call the// observer function OnIceCandidate. Send these candidates to the remote peer. StreamCollectionInterface StatsObserver PeerConnectionInterface PeerConnectionObserver:PeerConnection callback interface, used for RTCPeerConnection events. Application should implement these methods. PeerConnectionFactoryInterface:PeerConnectionFactoryInterface is the factory interface used for creating PeerConnection, MediaStream and MediaStreamTrack objects.The simplest method for obtaiing one, CreatePeerConnectionFactory will create the required libjingle threads, socket and network manager factory classes for networking if none are provided, though it requires that the application runs a message loop on the thread that called the method (see explanation below) If an application decides to provide its own threads and/or implementation of networking classes, it should use the alternate CreatePeerConnectionFactory method which accepts threads as input, and use the CreatePeerConnection version that takes a PortAllocator as an argument. peerconnectionproxy.hproxy.h12345678910111213141516171819202122232425262728293031323334353637383940// This file contains Macros for creating proxies for webrtc MediaStream and// PeerConnection classes.// TODO(deadbeef): Move this to pc/; this is part of the implementation.//// Example usage://// class TestInterface : public rtc::RefCountInterface &#123;// public:// std::string FooA() = 0;// std::string FooB(bool arg1) const = 0;// std::string FooC(bool arg1) = 0;// &#125;;//// Note that return types can not be a const reference.//// class Test : public TestInterface &#123;// ... implementation of the interface.// &#125;;//// BEGIN_PROXY_MAP(Test)// PROXY_SIGNALING_THREAD_DESTRUCTOR()// PROXY_METHOD0(std::string, FooA)// PROXY_CONSTMETHOD1(std::string, FooB, arg1)// PROXY_WORKER_METHOD1(std::string, FooC, arg1)// END_PROXY_MAP()//// Where the destructor and first two methods are invoked on the signaling// thread, and the third is invoked on the worker thread.//// The proxy can be created using//// TestProxy::Create(Thread* signaling_thread, Thread* worker_thread,// TestInterface*).//// The variant defined with BEGIN_SIGNALING_PROXY_MAP is unaware of// the worker thread, and invokes all methods on the signaling thread.//// The variant defined with BEGIN_OWNED_PROXY_MAP does not use// refcounting, and instead just takes ownership of the object being proxied. rtcerror.h/ccrtcerror_unittest.ccrtpparameters.hrtpreceiverinterface.hrtpsender.hrtpsenderinterface.hstatstypes.h/ccstreamcollection.humametrics.hvideosourceproxy.hvideotracksource.hwebrtcsdp.haudio/audio_mixer.h AudioMixer:This class is under development and is not yet intended for for use outside of WebRtc/Libjingle. audio_codecs/audio_decoder.h/cc AudioDecoder audio_codecs/audio_decoder_factory.h AudioDecoderFactory audio_codecs/audio_encoder.h/cc-AudioEncoder: his is the interface class for encoders in AudioCoding module. Each codec type must have an implementation of this class. audio_codecs/audio_encoder_factory.h AudioEncoderFactory audio_codecs/audio_format.h/ccaudio_codecs/builtin_audio_encoder_factory.h/ccaudio_codecs/builtin_audio_decoder_factory.h/cccall/audio_sink.hcall/transport.hortc/mediadescription.h/ccortc/mediadescription_unittest.ccortc/ortcfactoryinterface.hortc/ortcrtpreceiverinterface.hortc/ortcrtpsenderinterface.hortc/packettransportinterface.hortc/rtptransportcontrollerinterface.hortc/rtptransportinterface.hortc/sessiondecription.h/ccortc/sessiondescription_unittest.ccortc/srtptransportinerface.hortc/udptransportinterface.h]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc源码走读之base]]></title>
    <url>%2F2017%2F05%2F02%2Fwebrtc-source-base%2F</url>
    <content type="text"><![CDATA[src/webrtc/base是webrtc基础平台库，包括线程、锁、socket,智能指针等. 智能指针refcount.h定义了rtc::RefCountInterface: 123456789101112131415#include &quot;webrtc/base/refcountedobject.h&quot;namespace rtc &#123;// Reference count interface.class RefCountInterface &#123; public: virtual int AddRef() const = 0; virtual int Release() const = 0; protected: virtual ~RefCountInterface() &#123;&#125;&#125;;&#125; // namespace rtc refcountedobject.h下定义了RefCountedObject: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;utility&gt;#include &quot;webrtc/base/atomicops.h&quot;namespace rtc &#123;template &lt;class T&gt;class RefCountedObject : public T &#123; public: RefCountedObject() &#123;&#125; template &lt;class P0&gt; explicit RefCountedObject(P0&amp;&amp; p0) : T(std::forward&lt;P0&gt;(p0)) &#123;&#125; template &lt;class P0, class P1, class... Args&gt; RefCountedObject(P0&amp;&amp; p0, P1&amp;&amp; p1, Args&amp;&amp;... args) : T(std::forward&lt;P0&gt;(p0), std::forward&lt;P1&gt;(p1), std::forward&lt;Args&gt;(args)...) &#123;&#125; virtual int AddRef() const &#123; return AtomicOps::Increment(&amp;ref_count_); &#125; virtual int Release() const &#123; int count = AtomicOps::Decrement(&amp;ref_count_); if (!count) &#123; delete this; &#125; return count; &#125; // Return whether the reference count is one. If the reference count is used // in the conventional way, a reference count of 1 implies that the current // thread owns the reference and no other thread shares it. This call // performs the test for a reference count of one, and performs the memory // barrier needed for the owning thread to act on the object, knowing that it // has exclusive access to the object. virtual bool HasOneRef() const &#123; return AtomicOps::AcquireLoad(&amp;ref_count_) == 1; &#125; protected: virtual ~RefCountedObject() &#123;&#125; mutable volatile int ref_count_ = 0;&#125;;&#125; // namespace rtc 线程Thread网络Socket]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc之sdp协议]]></title>
    <url>%2F2017%2F04%2F27%2Fwebrtc-sdp%2F</url>
    <content type="text"><![CDATA[Session Description Protocol(会话描述协议)RFC定义SDP的协议有两个: RFC3264: An Offer/Answer Model with the session Description Protocol(SDP),用来概述一个请求/响应模型 RFC2327: SDP:Session Description Protocol,描述数据格式. 1.RFC23271.1.概述SDP 完全是一种会话描述格式 ― 它不属于传输协议 ― 它只使用不同的适当的传输协议，包括会话通知协议（SAP）、会话初始协议（SIP）、实时流协议（RTSP）、MIME 扩展协议的电子邮件以及超文本传输协议（HTTP）。SDP协议是也是基于文本的协议，这样就能保证协议的可扩展性比较强，这样就使其具有广泛的应用范围。SDP 不支持会话内容或媒体编码的协商，所以在流媒体中只用来描述媒体信息。媒体协商这一块要用RTSP来实现． SDP包括以下一些方面： 会话的名称和目的 会话存活时间 包含在会话中的媒体信息，包括： 媒体类型(video, audio, etc) 传输协议(RTP/UDP/IP, H.320, etc) 媒体格式(H.261 video, MPEG video, etc) 多播或远端（单播）地址和端口 为接收媒体而需的信息(addresses, ports, formats and so on) 使用的带宽信息 可信赖的接洽信息（Contact information） 1.2.SDP协议格式SDP描述由许多文本行组成，文本行的格式为&lt;类型&gt;=&lt;值&gt;，&lt;类型&gt;是一个字母，&lt;值&gt;是结构化的文本串，其格式依&lt;类型&gt;而定。 ＜type＞=[CRLF] 1.2.1.fields分类 Seeesion Description v(Protocol Version),mnd,The current protocol version.Always “0” using RFC4566 o(Origin),Mnd,The session originator’s name and session identifiers. s(Session Name), Mnd,The textural session Name i(Session Information), opt,Textural information about the session u(Uri),opt, A pointer to supplemental session Information e(Email Address), opt, Email contract information for the person responsible. P(phone Address),opt,Phone contract information for the person responsible c(Connection Data),C,The connection type and Address b(Bandwidth),opt,Proposed bandwidth limits. z(Time Zones), opt, Accounts for daylight saving information k(Encryption Keys),opt,A simple mechanism for exchanging keys, Rarely used. Timing Description t(Timing),mnd, start and end times. r(Repeat Times),opt, Specified the duration and intervals for any session repeats. Media Description m(Media Description),mnd, Media definitions including media type(e.g.”audio”),transport details and formats. i(Session Information),opt c(Connection Data),c b(Bandwidth):opt k( Encryption keys),opt a(Attributes),opt 1.2.2.典型格式1234567891011121314151617181920212223242526Session description v= (protocol version) o= (owner/creator and session identifier) s= (session name) i=* (session information) u=* (URI of description) e=* (email address) p=* (phone number) c=* (connection information - not required if included in all media) b=* (zero or more bandwidth information lines) One or more time descriptions (&quot;t=&quot; and &quot;r=&quot; lines, see below) z=* (time zone adjustments) k=* (encryption key) a=* (zero or more session attribute lines) Zero or more media descriptionsTime description t= (time the session is active) r=* (zero or more repeat times)Media description, if present m= (media name and transport address) i=* (media title) c=* (connection information - optional if included at session-level) b=* (zero or more bandwidth information lines) k=* (encryption key) a=* (zero or more media attribute lines) 带&quot;*&quot;号的是可选的,其余的是必须的。一般顺序也按照上面的顺序来排列。 1.2.3.各type对应值的结构化文本串 v= 其中：nettype是IN,代表internet,addrtype是IP4或IP6。unicast-address任务创建计算机的地址。 整个这个属性，是唯一表示一个任务。 e=123@126.com 或 p=+1 616 555-6011 对于一个任务只能两者之中的一个，表示会议控制者的联系方式。邮件地址可以是[email]j.doe@example.com[/email] (Jane Doe)形式，括号里面的是描述联系人的名称，或者Jane Doe &lt;[email]j.doe@example.com[/email]&gt;，前面的是联系人的名称。 c= 这个连接数据，可以是传话级别的连接数据，或者是单独一个媒体数据的连接数据。在是多播时，connection-address就该是一个多播组地址，当是单播时，connection-address就该是一个单播地址。对于addrtype是IP4的情况下，connection-address不仅包含IP地址，并且还要包含a time to live value(TTL 0-255)，如：c=IN IP4 224.2.36.42/128，IP6没有这个TTL值。还允许象这样的[/]/格式的connection-address。如：c=IN IP4 224.2.1.1/127/3等同于包含c=IN IP4 224.2.1.1/127, c=IN IP4 224.2.1.2/127, c=IN IP4 224.2.1.3/127三行内容。 b=: bwtype可以是CT或AS，CT方式是设置整个会议的带宽，AS是设置单个会话的带宽。缺省带宽是千比特每秒。 t= ，这个可以有行，指定多个不规则时间段，如果是规则的时间段，则r=属性可以使用。start-time和stop- time都遵从NTP(Network Time Protocol),是以秒为单位，自从1900以来的时间。要转换为UNIX时间，减去2208988800。如果stop-time设置为0,则会话没有时间限制。如果start-time也设置为0，则会话被认为是永久的。 b=: bwtype可以是CT或AS，CT方式是设置整个会议的带宽，AS是设置单个会话的带宽。缺省带宽是千比特每秒。 t= ，这个可以有行，指定多个不规则时间段，如果是规则的时间段，则r=属性可以使用。start-time和stop- time都遵从NTP(Network Time Protocol),是以秒为单位，自从1900以来的时间。要转换为UNIX时间，减去2208988800。如果stop-time设置为0,则会话没有时间限制。如果start-time也设置为0，则会话被认为是永久的。 r= 重复次数在时间表示里面可以如下表示： d - days (86400 seconds) h - hours (3600 seconds) m - minutes (60 seconds) s - seconds (allowed for completeness) z=&lt;adjustment time&gt; &lt;offset&gt; &lt;adjustment time&gt; &lt;offset&gt; .... k=&lt;method&gt; k=&lt;method&gt;:&lt;encryption key&gt; a=&lt;attribute&gt; a=&lt;attribute&gt;:&lt;value&gt; m=&lt;media&gt; &lt;port&gt; &lt;proto&gt; &lt;fmt&gt; ... m=&lt;media&gt; &lt;port&gt;/&lt;number of ports&gt; &lt;proto&gt; &lt;fmt&gt; ... a=cat:分类，根据分类接收者隔离相应的会话 a=keywds:关键字，根据关键字隔离相应的会话 a=tool:创建任务描述的工具的名称及版本号 a=ptime:在一个包里面的以毫秒为单位的媒体长度 a=maxptime:以毫秒为单位，能够压缩进一个包的媒体量。 a=rtpmap: / [/] a=recvonly a=sendrecv a=sendonly a=inactive， a=orient:其可能的值，”portrait”, “landscape” and “seascape” 。 a=type:,建议值是，”broadcast”, “meeting”, “moderated”, “test” and “H332”。 a=charset: a=sdplang:指定会话或者是媒体级别使用的语言 a=framerate:设置最大视频帧速率 a=quality:值是0-10 a=fmtp: 在SIP协议的包含的内容是SDP时，应该把Content-Type设置成application/sdp。1.3.SDP协议例子1.3.1.helix流媒体服务器的RTSP协议中的SDP协议:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152v=0 //SDP version// o field定义的源的一些信息。其格式为：o=&lt;username&gt; &lt;sess-id&gt; &lt;sess-version&gt; &lt;nettype&gt; &lt;addrtype&gt; &lt;unicast-address&gt;o=- 1271659412 1271659412 IN IP4 10.56.136.37 s=&lt;No title&gt;i=&lt;No author&gt; &lt;No copyright&gt; //session的信息c=IN IP4 0.0.0.0 //connect 的信息，分别描述了：网络协议，地址的类型，连接地址。c=IN IP4 0.0.0.0t=0 0 //时间信息，分别表示开始的时间和结束的时间，一般在流媒体的直播的时移中见的比较多。a=SdpplinVersion:1610641560 //描述性的信息a=StreamCount:integer;2 //用来描述媒体流的信息，表示有两个媒体流。integer表示信息的格式为整数。a=control:*a=DefaultLicenseValue:integer;0 //License信息a=FileType:string;&quot;MPEG4&quot; ////用来描述媒体流的信息说明当前协商的文件是mpeg4格式的文件a=LicenseKey:string;&quot;license.Summary.Datatypes.RealMPEG4.Enabled&quot;a=range:npt=0-72.080000 //用来表示媒体流的长度m=audio 0 RTP/AVP 96 //做为媒体描述信息的重要组成部分描述了媒体信息的详细内容：表示session的audio是通过RTP来格式传送的，其payload值为96传送的端口还没有定。b=as:24 //audio 的bitrateb=RR:1800b=RS:600a=control:streamid=1 //通过媒体流1来发送音频a=range:npt=0-72.080000 //说明媒体流的长度。a=length:npt=72.080000a=rtpmap:96 MPEG4-GENERIC/32000/2 //rtpmap的信息，表示音频为AAC的其sample为32000a=fmtp:96 profile-level-id=15;mode=AAC-hbr;sizelength=13;indexlength=3;indexdeltalength=3;config=1210 //config为AAC的详细格式信息a=mimetype:string;&quot;audio/MPEG4-GENERIC&quot;a=Helix-Adaptation-Support:1a=AvgBitRate:integer;48000a=HasOutOfOrderTS:integer;1a=MaxBitRate:integer;48000a=Preroll:integer;1000a=OpaqueData:buffer;&quot;A4CAgCIAAAAEgICAFEAVABgAAAC7gAAAu4AFgICAAhKIBoCAgAEC&quot;a=StreamName:string;&quot;Audio Track&quot;下面是video的信息基本和audio的信息相对称，这里就不再说了。m=video 0 RTP/AVP 97b=as:150b=RR:11250b=RS:3750a=control:streamid=2a=range:npt=0-72.080000a=length:npt=72.080000a=rtpmap:97 MP4V-ES/2500a=fmtp:97 profile-level-id=1;a=mimetype:string;&quot;video/MP4V-ES&quot;a=Helix-Adaptation-Support:1a=AvgBitRate:integer;300000a=HasOutOfOrderTS:integer;1a=Height:integer;240 //影片的长度a=MaxBitRate:integer;300000a=MaxPacketSize:integer;1400a=Preroll:integer;1000a=Width:integer;320 //影片的宽度a=OpaqueData:buffer;&quot;AzcAAB8ELyARAbd0AAST4AAEk+AFIAAAAbDzAAABtQ7gQMDPAAABAAAAASAAhED6KFAg8KIfBgEC&quot;a=StreamName:string;&quot;Video Track&quot; 1.3.2.Webrtc SDP示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152v=0o=- 0 0 IN IP4 127.0.0.1s=WX-RTC-SERVERt=0 0a=group:BUNDLE audio videoa=msid-semantic: WMS ryODEhTpFzm=audio 1 UDP/TLS/RTP/SAVPF 0 126c=IN IP4 0.0.0.0a=rtcp:1 IN IP4 0.0.0.0a=candidate:1 1 udp 2013266431 192.168.0.68 42739 typ host generation 0a=ice-ufrag:T+0ca=ice-pwd:FzV1T/5PiBI78s630cwSb6a=fingerprint:sha-256 2D:38:ED:09:73:36:F9:18:A6:CB:BC:ED:FB:C5:60:B3:F1:6C:FC:BD:97:57:AD:A6:38:11:9D:D4:8F:77:D6:C3a=setup:activea=recvonlya=extmap:1 urn:ietf:params:rtp-hdrext:ssrc-audio-levela=mid:audioa=rtcp-muxa=rtpmap:0 PCMU/8000a=rtpmap:126 telephone-event/8000m=video 1 UDP/TLS/RTP/SAVPF 124 125 96c=IN IP4 0.0.0.0a=rtcp:1 IN IP4 0.0.0.0a=candidate:1 1 udp 2013266431 192.168.0.68 42739 typ host generation 0a=ice-ufrag:T+0ca=ice-pwd:FzV1T/5PiBI78s630cwSb6a=extmap:2 urn:ietf:params:rtp-hdrext:toffseta=extmap:3 http://www.webrtc.org/experiments/rtp-hdrext/abs-send-timea=extmap:4 urn:3gpp:video-orientationa=extmap:6 http://www.webrtc.org/experiments/rtp-hdrext/playout-delaya=fingerprint:sha-256 2D:38:ED:09:73:36:F9:18:A6:CB:BC:ED:FB:C5:60:B3:F1:6C:FC:BD:97:57:AD:A6:38:11:9D:D4:8F:77:D6:C3a=setup:activea=recvonlya=mid:videoa=rtcp-muxa=rtpmap:124 H264/90000a=rtcp-fb:124 ccm fira=rtcp-fb:124 nacka=rtcp-fb:124 nack plia=rtcp-fb:124 goog-remba=fmtp:124 x-google-max-bitrate=800;x-google-min-bitrate=150;x-google-start-bitrate=300a=rtpmap:125 H264/90000a=rtcp-fb:125 ccm fira=rtcp-fb:125 nacka=rtcp-fb:125 nack plia=rtcp-fb:125 goog-remba=fmtp:125 x-google-max-bitrate=800;x-google-min-bitrate=150;x-google-start-bitrate=300a=rtpmap:96 VP8/90000a=rtcp-fb:96 ccm fira=rtcp-fb:96 nacka=rtcp-fb:96 nack plia=rtcp-fb:96 goog-remb 2.RFC3264An Offer/Answer Model with the Session Description Protocol (SDP) 2.1情态动词定义在RFC2119: “MUST”，必须、一定要； “MUST NOT”，禁止； “REQUIRED”，需要； “SHALL”、”SHOULD”，应该； “SHALL NOT”、”SHOULD NOT”，不应该； “RECOMMENDED”，推荐； “MAY”，可以2.2术语 媒体流（Media Stream），或称为媒体类型（Media Type），即我们通常所说的音频流、视频流等，所有通信实体要进行媒体交互之前都必须进行媒体注的协商 媒体格式（Media Format），每种媒体流都有不同的编码格式，像音频有G711、G712编码，视频有H261、H264等，像现在所谓的高清视频采用是720P、1070P等。 单一会话（Unitcast Session） 多会话（Multicast Sessions） 单一媒体流（Unitcast Streams） 多媒体流（Multicast Streams）2.3offer/answerrfc3264协议[1]主要概述一个请求/响应模型（offer/answer，以下叙述采用英文），包括请求/响应的实体和不同阶段的操作行为，如初始协商过程和重协商过程，并简单介绍消息中各种参数的含义。具体各个参数的详细说明见rfc2327协议[2]2.3.1.实体,消息Offer/Answer模型包括两个实体，一个是请求主体Offerer，另外一个是响应实体Answerer，两个实体只是在逻辑上进行区分，在一定条件可以转换。例如，手机A发起媒体协商请求，那么A就是Offerer，反之如果A为接收请求则为Offerer。 Offerer发给Answerer的请求消息称为请求offer，内容包括媒体流类型、各个媒体流使用的编码集，以及将要用于接收媒体流的IP和端口。 Answerer收到offer之后，回复给Offerer的消息称为响应，内容包括要使用的媒体编码，是否接收该媒体流以及告诉Offerer其用于接收媒体流的IP和端口。2.3.2.SDP各个参数简单介绍下面示例摘自3264协议[1] v=0 o=carol 28908764872 28908764872 IN IP4 100.3.6.6 //会话ID号和版本 s=- //用于传递会话主题 t=0 0 //会话时间，一般由其它信令消息控制，因此填0 c=IN IP4 192.0.2.4 //描述本端将用于传输媒体流的IP m=audio 0 RTP/AVP 0 1 3 //媒体类型 端口号 本端媒体使用的编码标识（Payload）集 a=rtpmap:0 PCMU/8000 //rtpmap映射表，各种编码详细描述参数，包括使用带宽（bandwidth） a=rtpmap:1 1016/8000 a=rtpmap:3 GSM/8000 a=sendonly //说明本端媒体流的方向，取值包括sendonly/recvonly/sendrecv/inactive a=ptime:20 //说明媒体流打包时长 m=video 0 RTP/AVP 31 34 a=rtpmap:31 H261/90000 a=rtpmap:34 H263/900002.3.3.实体行为、操作过程2.3.3.1.初始协商的Offer请求实体A &lt;-&gt; 实体B，实体首先发起Offer请求，内容如2节所示，对于作何一个媒体流/媒体通道，这时实体A必须： 如果媒体流方向标为recvonly/sendrecv，即a=recvonly或a=sendrecv，则A必须（MUST）准备好在这个IP和端口上接收实体B发来的媒体流； 如果媒体流方向标为sendonly/inactive，即a=recvonly或a=sendrecv，则A不需要进行准备。2.3.3.1.Answer响应实体B收到A的请求offer后，根据自身支持的媒体类型和编码策略，回复响应。 如果实体B回复的响应中的媒体流数量和顺序必须（MUST）和请求offer一致，以便实体A进行甄别和决策。即m行的数量和顺序必须一致，B不能（MUST NOT）擅自增加或删除媒体流。如果B不支持某个媒体流，可以在对应的端口置0，但不能不带这个m行描述。 对于某种媒体，实体B必须（MUST）从请求offer中选出A支持且自己也支持的该媒体的编码标识集，并且可以（MAY）附带自己支持的其它类型编码。 对于响应消息中各个媒体的方向： 如果请求某媒体流的方向为sendonly，那么响应中对应媒体的方向必须为recvonly； 如果请求某媒体流的方向为recvonly，那么响应中对应媒体的方向必须为sendonly； 如果请求某媒体流的方向为sendrecv，那么响应中对应媒体的方向可以为sendrecv/sendonly/recvonly/inactive中的一种； 如果请求某媒体流的方向为inactive，那么响应中对应媒体的方向必须为inactive； 响应answer里提供IP和端口，指示Offerer本端期望用于接收媒体流的IP和端口，一旦响应发出之后，Offerer必须（MUST）准备好在这个IP和端口上接收实体A发来的媒体流。 如果请求offer中带了ptime（媒体流打包间隔）的a行或带宽的a行，则响应answer也应该（SHOULD）相应的携带。 实体B Offerer应该（SHOULD）使用实体A比较期望的编码生成媒体流发送。一般来说对于m行，如m=video 0 RTP/AVP 31 34，排充越靠前的编码表示该实体越希望以这个编码作为载体，这里示例31(H261)，34（H263）中H261为A更期望使用的编码类型。同理，当实体A收到响应answer后也是这样理解的。2.3.3.2.实体收到响应后的处理当实体A收到B回复的响应后，可以（MAY）开始发送媒体流，如果媒体流方向为sendonly/sendrecv， 必须（MUST）使用answer列举的媒体类型/编码生成媒体发送； 应该（SHOULD）使用answer中的ptime和bandwidth来打包发送媒体流； 可以（MAY）立即停止监听端口，该端口为offer支持answer不支持的媒体所使用的端口。 2.3.4.修改媒体流（会话）修改媒体流的offer-answer操作必须基于之前协商的媒体形式（音频、视频等），不能（MUST NOT）对已有媒体流进行删减。 2.3.4.1.删除媒体流如果实体认定新的会话不支持之前媒商的某个媒体，新的offer只须对这种媒体所在m行的端口置0，但不能不描述这种媒体，即不带对应m行。当answerer收到响应之后，处理同初始协商一样。 2.3.4.2.增加媒体流如果实体打算新增媒体流，在offer里只须加上描述即可或者占用之前端口被置0的媒体流，即用新的媒体描述m行替换旧的。当answerer收到offer请求后，发现有新增媒体描述，或者过于端口被置0的媒体行被新的媒体描述替换，即知道当前为新增媒体流，处理同初始协商。 2.3.4.3.修改媒体流修改媒体注主要是针对初始协商结果，如果有变更即进入修改流程处理，可能的变更包括IP地址、端口，媒体格式（编码），媒体类型（音、视频），媒体属性（ptime，bandwidth，媒体流方向变更等）。]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc之入门]]></title>
    <url>%2F2017%2F04%2F27%2Fwebrtc-introduce%2F</url>
    <content type="text"><![CDATA[webrtc developersThe WebRTC APIsThree main tasks Acquiring audio and video Communicating audio and video Communicating arbitrary data Three main JavaScript APIs MediaStream(aka getUserMedia) RTCPeerConnection RTCDataChannel MediaStream(Acquiring audio and video) MediaStream Pepresent a stream of audio and/or video Can contain multiple ‘tracks’ Obtain a MediaStream with navigator.getUserMedia() Constraints Controls the contents of the MediaStream Media type, resolution, frame rateRTCPeerConnection(Audio and video communication between peers)RTCPeerConnection does a lot Signal processing Codec handling Peer to peer communication Security Bandwidth managementWebRTC architecture RTCDataChannel(Bidirectional communication of arbitrary data between peers) RTCDataChannel Same API as WebSockets Ultra-low latency Unreliable or reliable Secure Servers and Protocols(Peer to peer — but we need servers :) Abstract Signaling Need to exchange ‘session description’ objects: What formats I support, what I want to send Network information for peer-to-peer setup Can use any messaging mechanism Can use any messaging protocol STUN and TRUN(P2P in the age of firewalls and NATs) An ideal world The real world STUN Tell me what my public IP address is Simple server, cheap to run Data flows peer-to-peer TURN Provide a cloud fallback if peer-to-peer communication fails Data is sent through server, uses server bandwidth Ensures the call works in almost all environments ICE ICE: a framework for connecting peers Tries to find the best path for each call Vast majority of calls can use STUN (webrtcstats.com): Deploying STUN/TURN stun.l.google.com:19302 WebRTC stunserver, turnserver rfc5766-turn-server restund SecuritySecurity throughout WebRTC Mandatory encryption for media and data Secure UI, explicit opt-in Sandboxed, no plugins WebRTC Security Architecture ArchitecturesPeer to Peer : one-to-one callclientA &lt;——–&gt; clientB Mesh: small N-way call123456clientA &lt;-------------&gt; clientB /|\ \ / /|\ | \ / | | / \ | \|/ / \|/clientC &lt;--------------&gt; clientD Star: medium N-way call123clientA &lt;---------&gt; clientBclientA &lt;---------&gt; clientCclientA &lt;---------&gt; clientD MCU: large N-way call1234567MCU &lt;--------------&gt;clientAMCU &lt;--------------&gt;clientBMCU &lt;--------------&gt;clientCMCU &lt;--------------&gt;clientDMCU &lt;--------------&gt;clientEMCU &lt;--------------&gt;clientFMCU &lt;--------------&gt;clientG]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc之源码管理工具gclient]]></title>
    <url>%2F2017%2F04%2F27%2Fwebrtc-gclient%2F</url>
    <content type="text"><![CDATA[google的chromium项目是用gclient来管理源码的checkout, update等。 gclient是google专门为这种多源项目编写的脚本，它可以将多个源码管理系统中的代码放在一起管理。甚至包括将Git和svn代码放在一起。 webrtc也是使用gclient管理代码. gclient的sync，update等命令密切相关的两类文件.gclient和DEPS。 .gclient文件是gclient的控制文件，该文件放在工作目录的最上层(webrtc环境下与src统计目录)。”.gclient”文件是一个Python的脚本，定义了一组”solutions”，格式类似如下 1234567891011solutions = [ &#123; &quot;name&quot; : &quot;src&quot;, &quot;url&quot; : &quot;svn://svnserver/component/trunk/src&quot;, &quot;custom_deps&quot; : &#123; # To use the trunk of a component instead of what&apos;s in DEPS: #&quot;component&quot;: &quot;https://svnserver/component/trunk/&quot;, # To exclude a component from your working copy: #&quot;data/really_large_component&quot;: None, &#125; &#125;, ] name:checkout出源码的名字 url:源码所在的目录,gclient希望checkout出的源码中包括一个DEPS的文件,这个文件包含了必须checkout到工作目录的源码信息; deps_file:这是一个文件名(不包括路径),指在工程目录中包含依赖列表的文件,该项可选,默认值为”DEPS” custom_deps:这是一个可选的字典对象,会覆盖工程的”DEPS”文件定义的条目.一般它用作本地目录中,那些不用checkout的代码,或者让本地目录从不同位置checkout一个新的代码出来,或者checkout不同的分支,版本等.也可以用于增加在DEPS中不存在的新的项目.如: 12345678&quot;custom_deps&quot;: &#123; &quot;src/content/test/data/layout_tests/LayoutTests&quot;: None, &quot;src/chrome/tools/test/reference_build/chrome_win&quot;: None, &quot;src/chrome_frame/tools/test/reference_build/chrome_win&quot;: None, &quot;src/chrome/tools/test/reference_build/chrome_linux&quot;: None, &quot;src/chrome/tools/test/reference_build/chrome_mac&quot;: None, &quot;src/third_party/hunspell_dictionaries&quot;: None, &#125;, target_os:这个可选的条目可以指出特殊的平台,根据平台类checkout出不同代码,如: 1target_os = [&apos;android&apos;] 如果target_os_only值为True的话,那么,仅仅checkout出对应的代码,如: 12target_os = [ &quot;ios&quot; ] target_os_only = True 在每个checkout出的工程中，gclient期望发现一个DEPS文件（由deps_file来给定），它定义了工程不同部分都是如何checkout出来。 “DEPS”也是一个python脚本，最简单的，如下： 12345deps = &#123; &quot;src/outside&quot; : &quot;http://outside-server/trunk@1234&quot;, &quot;src/component&quot; : &quot;svn://svnserver/component/trunk/src@77829&quot;, &quot;src/relative&quot; : &quot;/trunk/src@77829&quot;, &#125; deps的每个条目都包含一个key-value对，key是被checkout的本地目录，而value就是对应的远程URL。如果路径是以’/‘开头的，那么它是一个相对URL，相对与.gclient中URL地址。 URL通常包含一个版本号，以便锁定源码在特定版本上。当然，这是可选的。如果没有，那么它将获取指定分支上最新的版本。 DEPS还可以包含其他类型的数据，如vars, 12345678910111213vars = &#123; &apos;pymox&apos;: &apos;http://pymox.googlecode.com/svn&apos;, &apos;sfntly&apos;: &apos;http://sfntly.googlecode.com/svn&apos;, &apos;eyes-free&apos;: &apos;http://eyes-free.googlecode.com/svn&apos;, &apos;rlz&apos;: &apos;http://rlz.googlecode.com/svn&apos;, &apos;smhasher&apos;: &apos;http://smhasher.googlecode.com/svn&apos;,...&#125; vars定义了一组变量，在后面，可以通过Var(xxx)来访问。Var(xxx)返回一个字符串，故此，也可以进行操作，如 1234&apos;src/third_party/cros_dbus_cplusplus/source&apos;:Var(&quot;git.chromium.org&quot;) + &apos;/chromiumos/third_party/dbus-cplusplus.git@5e8f6d9db5c2abfb91d91f751184f25bb5cd0900&apos;,&apos;src/third_party/WebKit&apos;:Var(&quot;webkit_trunk&quot;)[:-6] + &apos;/branches/chromium/1548@153044&apos;, 第二个自立，Var(“webkit_trunk”)[:-6]是一个python表达式，表示取得”webkit_trunk”表示的字符串的最后6个 Hooks：DEPS包含可选的内容 hooks，也有重要的作用，它表示在sync, update或者recert后，执行一个hook操作。 如果使用 –nohooks选项（hook默认执行），那么在gclient sync或者其他操作后，不会执行hook。你可以通过gclient runhooks来单独执行； 如果有 gclient sync –force，那么，无论sync是否成功，都会执行hook。 hook在DEPS中的写法，一般是： 1234567hooks = [ &#123; &quot;pattern&quot;: &quot;\\.(gif|jpe?g|pr0n|png)$&quot;, &quot;action&quot;: [&quot;python&quot;, &quot;image_indexer.py&quot;, &quot;--all&quot;]&#125;, &#123; &quot;pattern&quot;: &quot;.&quot;, &quot;name&quot;: &quot;gyp&quot;, &quot;action&quot;: [&quot;python&quot;, &quot;src/build/gyp_chromium&quot;]&#125;,] hooks包含一组hook，每个hook有几个重要项: pattern 是一个正则表达式，用来匹配工程目录下的文件，一旦匹配成功，action项就会执行 action 描述一个根据特定参数运行的命令行。这个命令在每次gclient时，无论多少文件匹配，至多运行一次。这个命令和.gclient在同一目录下运行。如果第一个参数是”python”，那么，当前的python解释器将被使用。如果包含字符串 “$matching_files”，它将该字符串扩展为匹配出的文件列表。 name 可选，标记出hook所属的组，可以被用来覆盖和重新组织。 deps_os： DEPS中定义不同平台依赖关系的项目，如 1234567891011121314151617181920212223deps_os = &#123; &quot;win&quot;: &#123; &quot;src/chrome/tools/test/reference_build/chrome_win&quot;: &quot;/trunk/deps/reference_builds/chrome_win@197743&quot;, &quot;src/third_party/cygwin&quot;: &quot;/trunk/deps/third_party/cygwin@133786&quot;,..... &#125;, &quot;ios&quot;: &#123; &quot;src/third_party/GTM&quot;: (Var(&quot;googlecode_url&quot;) % &quot;google-toolbox-for-mac&quot;) + &quot;/trunk@&quot; + Var(&quot;gtm_revision&quot;), &quot;src/third_party/nss&quot;: &quot;/trunk/deps/third_party/nss@&quot; + Var(&quot;nss_revision&quot;),.... &#125;,...&#125; deps_os指定不同平台的依赖，它可以包含多种平台，和.gclient中的target_os对应。这种对应关系如下： 12345678910111213DEPS_OS_CHOICES = &#123; &quot;win32&quot;: &quot;win&quot;, &quot;win&quot;: &quot;win&quot;, &quot;cygwin&quot;: &quot;win&quot;, &quot;darwin&quot;: &quot;mac&quot;, &quot;mac&quot;: &quot;mac&quot;, &quot;unix&quot;: &quot;unix&quot;, &quot;linux&quot;: &quot;unix&quot;, &quot;linux2&quot;: &quot;unix&quot;, &quot;linux3&quot;: &quot;unix&quot;, &quot;android&quot;: &quot;android&quot;,&#125; 下载webrtc android代码的.gclient文件(与src同级目录): 12345678910solutions = [ &#123; &quot;url&quot;: &quot;https://chromium.googlesource.com/external/webrtc.git&quot;, &quot;managed&quot;: False, &quot;name&quot;: &quot;src&quot;, &quot;deps_file&quot;: &quot;DEPS&quot;, &quot;custom_deps&quot;: &#123;&#125;, &#125;,]target_os = [&quot;android&quot;, &quot;unix&quot;] src同级目录下.gclient_entries定义了各模块及对应地址 1234567891011121314151617181920212223242526272829303132333435363738394041424344entries = &#123; &apos;src&apos;: &apos;https://chromium.googlesource.com/external/webrtc.git&apos;, &apos;src/base&apos;: &apos;https://chromium.googlesource.com/chromium/src/base@413df39df4640665d7ee1e8c198be1e91cedb4d9&apos;, &apos;src/build&apos;: &apos;https://chromium.googlesource.com/chromium/src/build@98f2769027214c848094d0d58156474eada3bc1b&apos;, &apos;src/buildtools&apos;: &apos;https://chromium.googlesource.com/chromium/buildtools.git@98f00fa10dbad2cdbb2e297a66c3d6d5bc3994f3&apos;, &apos;src/testing&apos;: &apos;https://chromium.googlesource.com/chromium/src/testing@3eab1a4b0951ac1fcb2be8bf9cb24143b509ea52&apos;, &apos;src/testing/gmock&apos;: &apos;https://chromium.googlesource.com/external/googlemock.git@0421b6f358139f02e102c9c332ce19a33faf75be&apos;, &apos;src/testing/gtest&apos;: &apos;https://chromium.googlesource.com/external/github.com/google/googletest.git@6f8a66431cb592dad629028a50b3dd418a408c87&apos;, &apos;src/third_party&apos;: &apos;https://chromium.googlesource.com/chromium/src/third_party@939f3a2eae486dd7cf3b31eae38642d2bc243737&apos;, &apos;src/third_party/android_tools&apos;: &apos;https://chromium.googlesource.com/android_tools.git@b65c4776dac2cf1b80e969b3b2d4e081b9c84f29&apos;, &apos;src/third_party/boringssl/src&apos;: &apos;https://boringssl.googlesource.com/boringssl.git@777fdd6443d5f01420b67137118febdf56a1c8e4&apos;, &apos;src/third_party/catapult&apos;: &apos;https://chromium.googlesource.com/external/github.com/catapult-project/catapult.git@6939b1db033bf35f4adf1ee55824b6edb3e324d6&apos;, &apos;src/third_party/ced/src&apos;: &apos;https://chromium.googlesource.com/external/github.com/google/compact_enc_det.git@e21eb6aed10b9f6e2727f136c52420033214d458&apos;, &apos;src/third_party/colorama/src&apos;: &apos;https://chromium.googlesource.com/external/colorama.git@799604a1041e9b3bc5d2789ecbd7e8db2e18e6b8&apos;, &apos;src/third_party/ffmpeg&apos;: &apos;https://chromium.googlesource.com/chromium/third_party/ffmpeg.git@28a5cdde5c32bcf66715343c10f74e85713f7aaf&apos;, &apos;src/third_party/gflags&apos;: &apos;https://chromium.googlesource.com/external/webrtc/deps/third_party/gflags@892576179b45861b53e04a112996a738309cf364&apos;, &apos;src/third_party/gflags/src&apos;: &apos;https://chromium.googlesource.com/external/github.com/gflags/gflags@03bebcb065c83beff83d50ae025a55a4bf94dfca&apos;, &apos;src/third_party/gtest-parallel&apos;: &apos;https://chromium.googlesource.com/external/github.com/google/gtest-parallel@7eb02a6415979ea59e765c34fe9da6c792f53e26&apos;, &apos;src/third_party/icu&apos;: &apos;https://chromium.googlesource.com/chromium/deps/icu.git@b34251f8b762f8e2112a89c587855ca4297fed96&apos;, &apos;src/third_party/jsoncpp/source&apos;: &apos;https://chromium.googlesource.com/external/github.com/open-source-parsers/jsoncpp.git@f572e8e42e22cfcf5ab0aea26574f408943edfa4&apos;, &apos;src/third_party/jsr-305/src&apos;: &apos;https://chromium.googlesource.com/external/jsr-305.git@642c508235471f7220af6d5df2d3210e3bfc0919&apos;, &apos;src/third_party/junit/src&apos;: &apos;https://chromium.googlesource.com/external/junit.git@64155f8a9babcfcf4263cf4d08253a1556e75481&apos;, &apos;src/third_party/libFuzzer/src&apos;: &apos;https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer.git@16f5f743c188c836d32cdaf349d5d3effb8a3518&apos;, &apos;src/third_party/libjpeg_turbo&apos;: &apos;https://chromium.googlesource.com/chromium/deps/libjpeg_turbo.git@7260e4d8b8e1e40b17f03fafdf1cd83296900f76&apos;, &apos;src/third_party/libsrtp&apos;: &apos;https://chromium.googlesource.com/chromium/deps/libsrtp.git@ccf84786f8ef803cb9c75e919e5a3976b9f5a672&apos;, &apos;src/third_party/libvpx/source/libvpx&apos;: &apos;https://chromium.googlesource.com/webm/libvpx.git@f22b828d685adee4c7a561990302e2d21b5e0047&apos;, &apos;src/third_party/libyuv&apos;: &apos;https://chromium.googlesource.com/libyuv/libyuv.git@fc02cc3806a394a6b887979ba74aa49955f3199b&apos;, &apos;src/third_party/lss&apos;: &apos;https://chromium.googlesource.com/linux-syscall-support.git@63f24c8221a229f677d26ebe8f3d1528a9d787ac&apos;, &apos;src/third_party/mockito/src&apos;: &apos;https://chromium.googlesource.com/external/mockito/mockito.git@de83ad4598ad4cf5ea53c69a8a8053780b04b850&apos;, &apos;src/third_party/openh264/src&apos;: &apos;https://chromium.googlesource.com/external/github.com/cisco/openh264@0fd88df93c5dcaf858c57eb7892bd27763f0f0ac&apos;, &apos;src/third_party/openmax_dl&apos;: &apos;https://chromium.googlesource.com/external/webrtc/deps/third_party/openmax.git@7acede9c039ea5d14cf326f44aad1245b9e674a7&apos;, &apos;src/third_party/requests/src&apos;: &apos;https://chromium.googlesource.com/external/github.com/kennethreitz/requests.git@f172b30356d821d180fa4ecfa3e71c7274a32de4&apos;, &apos;src/third_party/robolectric/robolectric&apos;: &apos;https://chromium.googlesource.com/external/robolectric.git@2a0b6ba221c14f3371813a676ce06143353e448d&apos;, &apos;src/third_party/ub-uiautomator/lib&apos;: &apos;https://chromium.googlesource.com/chromium/third_party/ub-uiautomator.git@00270549ce3161ae72ceb24712618ea28b4f9434&apos;, &apos;src/third_party/usrsctp/usrsctplib&apos;: &apos;https://chromium.googlesource.com/external/github.com/sctplab/usrsctp@8679f2b0bf063ac894dc473debefd61dbbebf622&apos;, &apos;src/third_party/yasm/source/patched-yasm&apos;: &apos;https://chromium.googlesource.com/chromium/deps/yasm/patched-yasm.git@7da28c6c7c6a1387217352ce02b31754deb54d2a&apos;, &apos;src/tools&apos;: &apos;https://chromium.googlesource.com/chromium/src/tools@4718dd2b6d53fb68819b3fd23676b40935f4f31e&apos;, &apos;src/tools/gyp&apos;: &apos;https://chromium.googlesource.com/external/gyp.git@eb296f67da078ec01f5e3a9ea9cdc6d26d680161&apos;, &apos;src/tools/swarming_client&apos;: &apos;https://chromium.googlesource.com/external/swarming.client.git@11e31afa5d330756ff87aa12064bb5d032896cb5&apos;, &apos;src/buildtools/clang_format/script&apos;: &apos;https://chromium.googlesource.com/chromium/llvm-project/cfe/tools/clang-format.git@c09c8deeac31f05bd801995c475e7c8070f9ecda&apos;, &apos;src/buildtools/third_party/libc++/trunk&apos;: &apos;https://chromium.googlesource.com/chromium/llvm-project/libcxx.git@b1ece9c037d879843b0b0f5a2802e1e9d443b75a&apos;, &apos;src/buildtools/third_party/libc++abi/trunk&apos;: &apos;https://chromium.googlesource.com/chromium/llvm-project/libcxxabi.git@0edb61e2e581758fc4cd4cd09fc588b3fc91a653&apos;, &apos;src/third_party/android_tools/ndk&apos;: &apos;https://chromium.googlesource.com/android_ndk.git@26d93ec07f3ce2ec2cdfeae1b21ee6f12ff868d8&apos;,&#125;]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webrtc之信令交互流程]]></title>
    <url>%2F2017%2F04%2F27%2Fwebrtc-signalling%2F</url>
    <content type="text"><![CDATA[无论是使用前端JS的WebRTC API接口，还是在WebRTC源码上构建自己的对聊框架，都需要遵循以下执行流程： 上述序列中，WebRTC并不提供Stun服务器和Signal服务器，服务器端需要自己实现。Stun服务器可以用google提供的实现stun协议的测试服务器（stun:stun.l.google.com:19302），Signal服务器则完全需要自己实现了，它需要在ClientA和ClientB之间传送彼此的SDP信息和candidate信息，ClientA和ClientB通过这些信息建立P2P连接来传送音视频数据。 stun/turn、relay服务器的实现在WebRTC源码中都有示例。 上述序列中，标注的场景是ClientA向ClientB发起对聊请求，调用描述如下： ClientA首先创建PeerConnection对象，然后打开本地音视频设备，将音视频数据封装成MediaStream添加到PeerConnection中。 ClientA调用PeerConnection的CreateOffer方法创建一个用于offer的SDP对象，SDP对象中保存当前音视频的相关参数。ClientA通过PeerConnection的SetLocalDescription方法将该SDP对象保存起来，并通过Signal服务器发送给ClientB。 ClientB接收到ClientA发送过的offer SDP对象，通过PeerConnection的SetRemoteDescription方法将其保存起来，并调用PeerConnection的CreateAnswer方法创建一个应答的SDP对象，通过PeerConnection的SetLocalDescription的方法保存该应答SDP对象并将它通过Signal服务器发送给ClientA。 ClientA接收到ClientB发送过来的应答SDP对象，将其通过PeerConnection的SetRemoteDescription方法保存起来。 在SDP信息的offer/answer流程中，ClientA和ClientB已经根据SDP信息创建好相应的音频Channel和视频Channel并开启Candidate数据的收集，Candidate数据可以简单地理解成Client端的IP地址信息（本地IP地址、公网IP地址、Relay服务端分配的地址）。 当ClientA收集到Candidate信息后，PeerConnection会通过OnIceCandidate接口给ClientA发送通知，ClientA将收到的Candidate信息通过Signal服务器发送给ClientB，ClientB通过PeerConnection的AddIceCandidate方法保存起来。同样的操作ClientB对ClientA再来一次。 这样ClientA和ClientB就已经建立了音视频传输的P2P通道，ClientB接收到ClientA传送过来的音视频流，会通过PeerConnection的OnAddStream回调接口返回一个标识ClientA端音视频流的MediaStream对象，在ClientB端渲染出来即可。同样操作也适应ClientB到ClientA的音视频流的传输。]]></content>
      <categories>
        <category>webrtc</category>
      </categories>
      <tags>
        <tag>webrtc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git服务搭建]]></title>
    <url>%2F2017%2F04%2F25%2Fserver-git%2F</url>
    <content type="text"><![CDATA[纯git server软件安装环境:ubuntu16.0.4 安装Git-Core:sudo apt-get install python-setuptools 安装openssh-server和openssh-client:sudo apt-get install openssh-server openssh-client 安装python tool:sudo apt-get install python-setuptools 安装gitosis:12345git clone https://github.com/res0nat0r/gitosis.gitcd gitosis/sudo python setup.py install 添加管理账号12345678sudo adduser \ --system \ --shell /bin/sh \ --gecos &apos;git version control&apos; \ --group \ --disabled-password \ --home /home/git \ git 如果已有git账户,可以替换成gitmanager 创建链接映射由于gitosis默认状态下会将仓库放在用户的repositories目录下，例如gitmanager用户的仓库地址默认在 /home/gitmanager/repositories/目录下，这里我们需要创建一个链接映射。让他指向我们前面创建的专门用于存放项目的仓库目录/home/gitrepository。 1sudo ln -s /home/gitrepository /home/gitmanager/repositories 初始化管理用户 拷贝管理用户公钥到/tmp/下,如: 1scp ~/.ssh/id_rsa.pub gitmanager@192.168.0.68:/tmp/ 使用拷贝来的公钥初始化gitosis: 12sudo -H -u gitmanager gitosis-init &lt; /tmp/id_ras.pubsudo chmod 755 /home/gitmanager/repositories/gitosis-admin.git/hooks/post-update 配置账号 验证ssh1234ssh gitmanager@192.168.0.68TY allocation request failed on channel 0ERROR:gitosis.serve.main:Need SSH_ORIGINAL_COMMAND in environment. Connection to gitserver closed. 说明 Gitosis 认出了该用户的身份，但由于没有运行任何 Git 命令，所以它切断了连接。 克隆gitosis管理仓库:1git clone gitmanager@192.168.0.68:gitosis-admin.git 这会得到一个名为 gitosis-admin 的工作目录，主要由两部分组成： 12./gitosis.conf./keydir gitosis.conf 文件是用来设置用户、仓库和权限的控制文件。keydir 目录则是保存所有具有访问权限用户公钥的地方— 每人一个。在 keydir 里的文件名（比如上面的 qingkouwei.pub） 会自动从使用 gitosis-init 脚本导入的公钥尾部的描述中获取该名字。 看一下 gitosis.conf 文件的内容，它应该只包含与刚刚克隆的 gitosis-admin 相关的信息： 12345[gitosis][group gitosis-admin]members = qingkouweiwritable = gitosis-admin 要创建项目demo,在里面加入: 123[group demo]members = qingkouweiwritable = demo 要为demo项目添加用户user1: 123[group demo]members = qingkouwei user1writable = demo 并将用户user1的公钥计入到keydir,并且公钥名.pub和members里面的名字对应. 要添加对demo项目只读的用户: 1234567[group demo]members = qingkouwei user1writable = demo[group demo]members = user2readonly = demo 修改完配置文件和keydir,使用git push到gitosis-admin服务器.即可直接git add remote add suervename gitmanager@192.168.0.68:demo.git,然后直接将本地目录推送到demo仓库,不需要再服务器手动创建demo仓库,gitosis会帮忙自动创建. 常见问题 ERROR:gitosis.serve.main:Repository read access denied 原因: gitosis.conf中的members与keydir中的用户名不一致，如gitosis中的members = foo@bar，但keydir中的公密名却叫foo.pub 解决方法: 使keydir的名称与gitosis中members所指的名称一致。 改为members = foo 或 公密名称改为foo@bar.pub clone时报does not appear to be a git repository 原因: clone时不能用绝对路径，直接写gitosis-admin.git即可. 参考:https://git-scm.com/book/zh/v1/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84-Git-Gitosis gitlab服务搭建]]></content>
      <categories>
        <category>server</category>
      </categories>
      <tags>
        <tag>server</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware中Bridged,NAT,host-only三种网络连接模式的原理和区别]]></title>
    <url>%2F2017%2F04%2F22%2Fvmware-network-mode%2F</url>
    <content type="text"><![CDATA[不同虚拟交换机应用在不同的联网模式Bridged、NAT、host-only、custom四种模式，下面分别介绍其具体分配： VMnet0：这是VMware用于虚拟桥接网络下的虚拟交换机； VMnet1：这是VMware用于虚拟Host-Only网络下的虚拟交换机； VMnet8：这是VMware用于虚拟NAT网络下的虚拟交换机； VMnet2~VMnet7及VMnet9：是VMware用于虚拟自定义custom网络下的虚拟交换机； VMware Network Adapter VMnet1：这是宿主机用于与Host-Only虚拟网络进行通信的宿主机使用的虚拟网卡； VMware Network Adapter VMnet8：这是宿主机用于与NAT虚拟网络进行通信的宿主机使用的虚拟网卡； VMware Network Adapter VMnet1与VMware Network Adapter VMnet8可以在宿主机网络连接中看到. 1.Bridged桥接模式VMware在桥接模式下，虚拟机使用VMware为该虚拟机分配的虚拟网卡，宿主机使用自身的物理网卡（有线或无线都行），并且默认使用虚拟交换机VMnet0来连接虚拟机的虚拟网卡和宿主机的物理网卡。在此模式下没有局域网动态地址分配DHCP服务器，也没有网络地址转换ＮＡＴ服务器，虚拟交换机没有连接DHCP服务器和ＮＡＴ服务器。宿主机的网口（插网线的那个口）与宿主机物理网卡相连，同时也就和虚拟机的虚拟网卡相连，也就是和虚拟交换机相连，所以虚拟机相当于在宿主机所在局域网内的一个单独的主机，他的行为和宿主机是同等地位的，没有依存关系。所有桥接下的网卡与网卡都是交换模式的，相互可以访问而不干扰。在桥接模式下，虚拟机ip地址需要与主机在同一个网段，如果需要联网，则网关与DNS需要与主机网卡一致原理图如下： 配置虚拟机网卡,编辑/etc/sysconfig/network-scripts/ifcfg-eth0: 1234567891011DEVICE=eth0HWADDR=00:0C:29:DA:E9:99TYPE=EthernetUUID=0711466f-ae1f-aa83-825cb3dfb5f7ONBOOT=yesMM_CONTROLLED=yesBOOTPROTO=noneIPADDR=192.168.31.128 #设置虚拟机ip地址,与主机ip地址在同一网段NETMASK=255.255.255.0 #设置子网掩码GATEWAY=192.168.31.1#设置虚拟网关,与主机相同DNS1=192.168.31.1 #设置虚拟机DNS,与主机相同 执行/etc/init.d/network restart重启虚拟机网卡,ping内网与外网测试. 2.NAT网络地址转换模式： 注意：红色的方框是nat服务器，nat服务器有两个网卡一个是虚拟内网网卡，一个是宿主机的物理网卡。禁用VmNet8，虚拟机仍然可以上网，ping通主机，但是主机ping不通虚拟机的网卡。在NAT模式中，主机网卡直接与虚拟NAT设备相连，然后虚拟NAT设备与虚拟DHCP服务器一起连接在虚拟交换机VMnet8上，这样就实现了虚拟机联网。那么我们会觉得很奇怪，为什么需要虚拟网卡VMware Network Adapter VMnet8呢？原来我们的VMware Network Adapter VMnet8虚拟网卡主要是为了实现主机与虚拟机之间的通信。弥补了NAT协议中外网不能访问局域网的缺点。 具体配置: 123456789101112DEVICE=eth0HWADDR=00:0C:29:DA:E9:99TYPE=EthernetUUID=0711466f-ae1f-aa83-825cb3dfb5f7ONBOOT=yesMM_CONTROLLED=yesBOOTPROTO=dhcp #动态获取ip地址,如果此处设置为静态,则下面手动配置ip需要在DHCP地址范围内#NAT模式也可以设置静态ip,但需要在DHCP地址范围内IPADDR=192.168.31.128NETMASK=255.255.255.0GATEWAY=192.168.31.1DNS1=192.168.31.1 3.Host-Only方式 注意：上图中的VmNet8应该为VmNet1。其实跟nat模式的图片是类似的，只是少了nat服务。 所以host-only上不了外网，只能实现主机的VmNet1网卡和虚拟机的虚拟网卡通信。 NAT介绍NAT（Network Address Translation，网络地址转换）是1994年提出的。当在专用网内部的一些主机本来已经分配到了本地IP地址（即仅在本专用网内使用的专用地址），但现在又想和因特网上的主机通信（并不需要加密）时，可使用NAT方法。 这种方法需要在专用网连接到因特网的路由器上安装NAT软件。装有NAT软件的路由器叫做NAT路由器，它至少有一个有效的外部全球IP地址。这样，所有使用本地地址的主机在和外界通信时，都要在NAT路由器上将其本地地址转换成全球IP地址，才能和因特网连接。 另外，这种通过使用少量的公有IP 地址代表较多的私有IP 地址的方式，将有助于减缓可用的IP地址空间的枯竭。 功能NAT不仅能解决了lP地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。 宽带分享：这是 NAT 主机的最大功能。 安全防护：NAT 之内的 PC 联机到 Internet 上面时，他所显示的 IP 是 NAT 主机的公共 IP，所以 Client 端的 PC 当然就具有一定程度的安全了，外界在进行 portscan（端口扫描） 的时候，就侦测不到源Client 端的 PC 。 实现方式NAT的实现方式有三种，即静态转换Static Nat、动态转换Dynamic Nat和端口多路复用OverLoad。 静态转换是指将内部网络的私有IP地址转换为公有IP地址，IP地址对是一对一的，是一成不变的，某个私有IP地址只转换为某个公有IP地址。借助于静态转换，可以实现外部网络对内部网络中某些特定设备（如服务器）的访问。 动态转换是指将内部网络的私有IP地址转换为公用IP地址时，IP地址是不确定的，是随机的，所有被授权访问上Internet的私有IP地址可随机转换为任何指定的合法IP地址。也就是说，只要指定哪些内部地址可以进行转换，以及用哪些合法地址作为外部地址时，就可以进行动态转换。动态转换可以使用多个合法外部地址集。当ISP提供的合法IP地址略少于网络内部的计算机数量时。可以采用动态转换的方式。 端口多路复用（Port address Translation,PAT)是指改变外出数据包的源端口并进行端口转换，即端口地址转换（PAT，Port Address Translation).采用端口多路复用方式。内部网络的所有主机均可共享一个合法外部IP地址实现对Internet的访问，从而可以最大限度地节约IP地址资源。同时，又可隐藏网络内部的所有主机，有效避免来自internet的攻击。因此，目前网络中应用最多的就是端口多路复用方式。 ALG（Application Level Gateway），即应用程序级网关技术：传统的NAT技术只对IP层和传输层头部进行转换处理，但是一些应用层协议，在协议数据报文中包含了地址信息。为了使得这些应用也能透明地完成NAT转换，NAT使用一种称作ALG的技术，它能对这些应用程序在通信时所包含的地址信息也进行相应的NAT转换。例如：对于FTP协议的PORT/PASV命令、DNS协议的 “A” 和 “PTR” queries命令和部分ICMP消息类型等都需要相应的ALG来支持。 如果协议数据报文中不包含地址信息，则很容易利用传统的NAT技术来完成透明的地址转换功能，通常我们使用的如下应用就可以直接利用传统的NAT技术：HTTP、TELNET、FINGER、NTP、NFS、ARCHIE、RLOGIN、RSH、RCP等。 工作原理借助于NAT，私有（保留）地址的”内部”网络通过路由器发送数据包时，私有地址被转换成合法的IP地址，一个局域网只需使用少量IP地址（甚至是1个）即可实现私有地址网络内所有计算机与Internet的通信需求。 NAT将自动修改IP报文的源IP地址和目的IP地址，Ip地址校验则在NAT处理过程中自动完成。有些应用程序将源IP地址嵌入到IP报文的数据部分中，所以还需要同时对报文的数据部分进行修改，以匹配IP头中已经修改过的源IP地址。否则，在报文数据部分嵌入IP地址的应用程序就不能正常工作。 NAPTNAPT（Network Address Port Translation），即网络端口地址转换，可将多个内部地址映射为一个合法公网地址，但以不同的协议端口号与不同的内部地址相对应，也就是&lt;内部地址+内部端口&gt;与&lt;外部地址+外部端口&gt;之间的转换。NAPT普遍用于接入设备中，它可以将中小型的网络隐藏在一个合法的IP地址后面。NAPT也被称为“多对一”的NAT，或者叫PAT（Port Address Translations，端口地址转换）、地址超载（address overloading）。 NAPT与动态地址NAT不同，它将内部连接映射到外部网络中的一个单独的IP地址上，同时在该地址上加上一个由NAT设备选定的TCP端口号。NAPT算得上是一种较流行的NAT变体，通过转换TCP或UDP协议端口号以及地址来提供并发性。除了一对源和目的IP地址以外，这个表还包括一对源和目的协议端口号，以及NAT盒使用的一个协议端口号。 NAPT的主要优势在于，能够使用一个全球有效IP地址获得通用性。主要缺点在于其通信仅限于TCP或UDP。当所有通信都采用TCP或UDP，NAPT允许一台内部计算机访问多台外部计算机，并允许多台内部主机访问同一台外部计算机，相互之间不会发生冲突。 NAT穿透方法目前常用的针对UDP的NAT 穿透（NAT Traversal）方法主要有：STUN、TURN、ICE、uPnP等。其中ICE方式由于其结合了STUN和TURN的特点，所以使用最为广泛。针对TCP的NAT穿透技术目前仍为难点。实用的技术仍然不多。 配置在配置NAT(网络地址转换)之前，首先需要了解内部本地地址和内部全局地址的分配情况。根据不同的需求，执行以下不同的配置任务。 内部源地址NAT配置 内部源地址NAPT配置 重叠地址NAT配置 TCP负载均衡]]></content>
      <categories>
        <category>linux管理</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux网卡配置]]></title>
    <url>%2F2017%2F04%2F22%2Flinux-networkcard%2F</url>
    <content type="text"><![CDATA[linux网卡可以通过命令和配置文件配置,如果是桌面环境还可以通过图形化界面配置. 1.ifconfig(interfaces config)命令方式通常需要以root身份登录或使用sudo以便在Linux机器上使用ifconfig工具。依赖于ifconfig命令中使用一些选项属性，ifconfig工具不仅可以被用来简单地获取网络接口配置信息，还可以修改这些配置(用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在)。 1.1命令格式1ifconfig [网络设备] [参数] 1.2命令功能ifconfig 命令用来查看和配置网络设备。当网络环境发生改变时可通过此命令对网络进行相应的配置。 1.3命令参数 up 启动指定网络设备/网卡。 down 关闭指定网络设备/网卡。该参数可以有效地阻止通过指定接口的IP信息流，如果想永久地关闭一个接口，我们还需要从核心路由表中将该接口的路由信息全部删除。 arp 设置指定网卡是否支持ARP协议。 -promisc 设置是否支持网卡的promiscuous模式，如果选择此参数，网卡将接收网络中发给它所有的数据包 -allmulti 设置是否支持多播模式，如果选择此参数，网卡将接收网络中所有的多播数据包 -a 显示全部接口信息 -s 显示摘要信息（类似于 netstat -i） add 给指定网卡配置IPv6地址 del 删除指定网卡的IPv6地址 &lt;硬件地址&gt; 配置网卡最大的传输单元 mtu&lt;字节数&gt; 设置网卡的最大传输单元 (bytes) netmask&lt;子网掩码&gt; 设置网卡的子网掩码。掩码可以是有前缀0x的32位十六进制数，也可以是用点分开的4个十进制数。如果不打算将网络分成子网，可以不管这一选项；如果要使用子网，那么请记住，网络中每一个系统必须有相同子网掩码。 tunel 建立隧道 dstaddr 设定一个远端地址，建立点对点通信 -broadcast&lt;地址&gt; 为指定网卡设置广播协议 -pointtopoint&lt;地址&gt; 为网卡设置点对点通讯协议 multicast 为网卡设置组播标志 address 为网卡设置IPv4地址 txqueuelen&lt;长度&gt; 为网卡设置传输列队的长度 1.4使用实例1.4.1显示网络设备信息（激活状态的）命令:ifcofig 输出: 12345678910111213141516[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:20 inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0 TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:596390239 (568.7 MiB) TX bytes:2886956 (2.7 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:68 errors:0 dropped:0 overruns:0 frame:0 TX packets:68 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2856 (2.7 KiB) TX bytes:2856 (2.7 KiB) 说明 eth0 表示第一块网卡， 其中 HWaddr 表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是 00:50:56:BF:26:20 inet addr 用来表示网卡的IP地址，此网卡的 IP地址是 192.168.120.204，广播地址， Bcast:192.168.120.255，掩码地址Mask:255.255.255.0 lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 HTTPD服务器的指定到回坏地址，在浏览器输入 127.0.0.1 就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址） 第二行：网卡的IP地址、子网、掩码 第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节 第四、五行：接收、发送数据包情况统计 第七行：接收、发送数据字节数统计信息。 1.4.2启动关闭指定网卡命令： ifconfig eth0 up ifconfig eth0 down 输出： 说明： ifconfig eth0 up 为启动网卡eth0 ；ifconfig eth0 down 为关闭网卡eth0。ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。 1.4.3为网卡配置和删除IPv6地址命令： ifconfig eth0 add 33ffe:3240:800:1005::2/64 ifconfig eth0 del 33ffe:3240:800:1005::2/64 输出： 说明： ifconfig eth0 add 33ffe:3240:800:1005::2/64 为网卡eth0配置IPv6地址； ifconfig eth0 add 33ffe:3240:800:1005::2/64 为网卡eth0删除IPv6地址； 练习的时候，ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。 1.4.4用ifconfig修改MAC地址命令： ifconfig eth0 hw ether 00:AA:BB:CC:DD:EE 输出： 12345678910111213141516171819202122232425262728293031323334353637[root@localhost ~]# ifconfig eth0 down //关闭网卡[root@localhost ~]# ifconfig eth0 hw ether 00:AA:BB:CC:DD:EE //修改MAC地址[root@localhost ~]# ifconfig eth0 up //启动网卡[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:AA:BB:CC:DD:EE inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0 TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:596390239 (568.7 MiB) TX bytes:2886956 (2.7 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:68 errors:0 dropped:0 overruns:0 frame:0 TX packets:68 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2856 (2.7 KiB) TX bytes:2856 (2.7 KiB)[root@localhost ~]# ifconfig eth0 hw ether 00:50:56:BF:26:20 //关闭网卡并修改MAC地址[root@localhost ~]# ifconfig eth0 up //启动网卡[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:20 inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0 TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:596390239 (568.7 MiB) TX bytes:2886956 (2.7 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:68 errors:0 dropped:0 overruns:0 frame:0 TX packets:68 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2856 (2.7 KiB) TX bytes:2856 (2.7 KiB) 1.4.5配置IP地址输出: 123[root@localhost ~]# ifconfig eth0 192.168.120.56[root@localhost ~]# ifconfig eth0 192.168.120.56 netmask 255.255.255.0[root@localhost ~]# ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255 说明: ifconfig eth0 192.168.120.56 给eth0网卡配置IP地：192.168.120.56 ifconfig eth0 192.168.120.56 netmask 255.255.255.0 给eth0网卡配置IP地址：192.168.120.56 ，并加上子掩码：255.255.255.0 ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255 /给eth0网卡配置IP地址：192.168.120.56，加上子掩码：255.255.255.0，加上个广播地址： 192.168.120.255 1.4.6启用和关闭ARP协议命令： ifconfig eth0 arp ifconfig eth0 -arp 输出： 12[root@localhost ~]# ifconfig eth0 arp[root@localhost ~]# ifconfig eth0 -arp 说明 ifconfig eth0 arp 开启网卡eth0 的arp协议； ifconfig eth0 -arp 关闭网卡eth0 的arp协议； 1.4.7 设置最大传输单元命令： ifconfig eth0 mtu 1500 输出： 12345678910111213141516171819202122232425262728293031323334353637[root@localhost ~]# ifconfig eth0 mtu 1480[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:1F inet addr:192.168.120.203 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1480 Metric:1 RX packets:8712395 errors:0 dropped:0 overruns:0 frame:0 TX packets:36631 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:597062089 (569.4 MiB) TX bytes:2643973 (2.5 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:9973 errors:0 dropped:0 overruns:0 frame:0 TX packets:9973 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:518096 (505.9 KiB) TX bytes:518096 (505.9 KiB)[root@localhost ~]# ifconfig eth0 mtu 1500[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:1F inet addr:192.168.120.203 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8712548 errors:0 dropped:0 overruns:0 frame:0 TX packets:36685 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:597072333 (569.4 MiB) TX bytes:2650581 (2.5 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:9973 errors:0 dropped:0 overruns:0 frame:0 TX packets:9973 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:518096 (505.9 KiB) TX bytes:518096 (505.9 KiB)[root@localhost ~]# 说明: 设置能通过的最大数据包大小为1500bytes 2.配置文件方式ubuntu配置文件:/etc/network/interfaces 1234567auto loiface lo inet loopbackauto eth0 #配置静态ipiface eth0 inet staticaddress 192.168.1.100netmask 255.255.255.0gateway 192.168.1.1 centos配置文件:/etc/sysconfig/network-scripts/ifcfg-eth0 123456789101112DEVICE=eth0(默认)HWADDR=00:0C:29:2E:36:16(默认)TYPE=Ethernet(默认)UUID=XXXXXXX(默认)ONBOOT=yes(默认为no,修改为yes意为每次reboot后 ifup eth0)MM_CONTROLLED=yes(默认)#BOOTPROTO=dhcp(dhcp为自动分配ip地址,我们把他注释了，在下面另外加)BOOTPROTO=static(新添加)IPV6INIT=no(新添加)USERCTL=no(新添加)IPADDR=192.168.164.100(新添加)NETMASK=255.255.255.0(新添加) service network restart重启网卡服务 3.图形界面方式添加虚拟网卡一台服务器需要设置多个ip,但又不想添加多块网卡,那就需要设置虚拟网卡.这里介绍几种方式在linux服务器上添加虚拟网卡. 比如向eth0中添加一块虚拟网卡: 1.快速创建删除虚拟网卡sudo ifconfig eth0: 192.168.10.10 up 以上的命令就可以在eth0网卡上创建一个叫eth0:0的虚拟网卡,他的地址是:192.168.1.63 如果不想要这个虚拟网卡了,可以使用如下命令删除: 1sudo ifconfig eth0:0 down 重启服务器或者网络后,虚拟网卡就没有了. 2.修改网卡配置文件在ubuntu下,网卡的配置文件是/etc/network/interfaces,所以我们修改它: sudo vim /etc/network/interfaces 在这个文件中增加如下内容并保存: 123456auto eth0:0iface eth0:0 inet staticaddress 192.168.10.10netmask 255.255.255.0#network 192.168.10.1#broadcast 192.168.1.255 保存后,我们需要重启网卡(重新加载配置文件)才会生效,使用如下命令重启:sudo /etc/init.d/networking restart 他的优点是重启服务器或者网卡配置不会丢失。 3.创建tag前两种方法都有一个特点，创建的网卡可有不同的ip地址，但是Mac地址相同。无法用来创建虚拟机。 添加虚拟网卡tap 1tunctl -b 其他配置命令: 显示网桥信息:brctl show 添加网桥:brctl addbr virbr0 激活网桥:ip link set virbr0 up 添加虚拟网卡tap:tunctl -b tap0 ——-&gt; 执行上面使命就会生成一个tap,后缀从0，1，2依次递增激活创建的tap:ip link set tap0 up 将tap0虚拟网卡添加到指定网桥上:brctl addif br0 tap0给网桥配制ip地址:ifconfig virbr1 169.254.251.4 up 将virbr1网桥上绑定的网卡eth5解除: brctl delif virb1 eth5 给virbr1网桥添加网卡eth6:brctl addif virbr1 eth6]]></content>
      <categories>
        <category>linux管理</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android声音播放与录制]]></title>
    <url>%2F2017%2F04%2F19%2Fissues-android-audio%2F</url>
    <content type="text"><![CDATA[AudioTrackAudioTrack类说明:123456789101112131415161718192021222324252627282930313233343536373839/** * The AudioTrack class manages and plays a single audio resource for Java applications. * It allows streaming of PCM audio buffers to the audio sink for playback. This is * achieved by &quot;pushing&quot; the data to the AudioTrack object using one of the * &#123;@link #write(byte[], int, int)&#125;, &#123;@link #write(short[], int, int)&#125;, * and &#123;@link #write(float[], int, int, int)&#125; methods. * * &lt;p&gt;An AudioTrack instance can operate under two modes: static or streaming.&lt;br&gt; * In Streaming mode, the application writes a continuous stream of data to the AudioTrack, using * one of the &#123;@code write()&#125; methods. These are blocking and return when the data has been * transferred from the Java layer to the native layer and queued for playback. The streaming * mode is most useful when playing blocks of audio data that for instance are: * * &lt;ul&gt; * &lt;li&gt;too big to fit in memory because of the duration of the sound to play,&lt;/li&gt; * &lt;li&gt;too big to fit in memory because of the characteristics of the audio data * (high sampling rate, bits per sample ...)&lt;/li&gt; * &lt;li&gt;received or generated while previously queued audio is playing.&lt;/li&gt; * &lt;/ul&gt; * * The static mode should be chosen when dealing with short sounds that fit in memory and * that need to be played with the smallest latency possible. The static mode will * therefore be preferred for UI and game sounds that are played often, and with the * smallest overhead possible. * * &lt;p&gt;Upon creation, an AudioTrack object initializes its associated audio buffer. * The size of this buffer, specified during the construction, determines how long an AudioTrack * can play before running out of data.&lt;br&gt; * For an AudioTrack using the static mode, this size is the maximum size of the sound that can * be played from it.&lt;br&gt; * For the streaming mode, data will be written to the audio sink in chunks of * sizes less than or equal to the total buffer size. * * AudioTrack is not final and thus permits subclasses, but such use is not recommended. */public class AudioTrack extends PlayerBase implements AudioRouting&#123; &#125; 构造方法说明1234567891011121314151617//根据采样率，采样精度，单双声道来得到frame的大小。int bufsize = AudioTrack.getMinBufferSize(8000,//每秒8K个点 AudioFormat.CHANNEL_CONFIGURATION_STEREO,//双声道AudioFormat.ENCODING_PCM_16BIT);//一个采样点16比特-2个字节//注意，按照数字音频的知识，这个算出来的是一秒钟buffer的大小。//创建AudioTrackAudioTrack trackplayer = new AudioTrack(AudioManager.STREAM_MUSIC, 8000, AudioFormat.CHANNEL_CONFIGURATION_ STEREO, AudioFormat.ENCODING_PCM_16BIT, bufsize,AudioTrack.MODE_STREAM);// trackplayer.play() ;//开始trackplayer.write(bytes_pkg, 0, bytes_pkg.length) ;//往track中写数据….trackplayer.stop();//停止播放trackplayer.release();//释放底层资源。 参数说明1. AudioTrack.MODE_STREAM的意思：AudioTrack中有MODE_STATIC和MODE_STREAM两种分类。 STREAM的意思是由用户在应用程序通过write方式把数据一次一次得写到audiotrack中。这个和我们在socket中发送数据一样，应用层从某个地方获取数据，例如通过编解码得到PCM数据，然后write到audiotrack。这种方式的坏处就是总是在JAVA层和Native层交互，效率损失较大。 而STATIC的意思是一开始创建的时候，就把音频数据放到一个固定的buffer，然后直接传给audiotrack，后续就不用一次次得write了。AudioTrack会自己播放这个buffer中的数据。 这种方法对于铃声等内存占用较小，延时要求较高的声音来说很适用。 2. StreamType这个在构造AudioTrack的第一个参数中使用。这个参数和Android中的AudioManager有关系，涉及到手机上的音频管理策略。 Android将系统的声音分为以下几类常见的（定义在AudioManager)： 1234567891011121314151617181920/** The audio stream for phone calls */public static final int STREAM_VOICE_CALL = AudioSystem.STREAM_VOICE_CALL;/** The audio stream for system sounds */public static final int STREAM_SYSTEM = AudioSystem.STREAM_SYSTEM;/** The audio stream for the phone ring */public static final int STREAM_RING = AudioSystem.STREAM_RING;/** The audio stream for music playback */public static final int STREAM_MUSIC = AudioSystem.STREAM_MUSIC;/** The audio stream for alarms */public static final int STREAM_ALARM = AudioSystem.STREAM_ALARM;/** The audio stream for notifications */public static final int STREAM_NOTIFICATION = AudioSystem.STREAM_NOTIFICATION;/** @hide The audio stream for phone calls when connected to bluetooth */public static final int STREAM_BLUETOOTH_SCO = AudioSystem.STREAM_BLUETOOTH_SCO;/** @hide The audio stream for enforced system sounds in certain countries (e.g camera in Japan) */public static final int STREAM_SYSTEM_ENFORCED = AudioSystem.STREAM_SYSTEM_ENFORCED;/** The audio stream for DTMF Tones */public static final int STREAM_DTMF = AudioSystem.STREAM_DTMF;/** @hide The audio stream for text to speech (TTS) */public static final int STREAM_TTS = AudioSystem.STREAM_TTS; 常用说明: STREAM_ALARM：警告声 STREAM_MUSCI：音乐声，例如music等 STREAM_RING：铃声 STREAM_SYSTEM：系统声音 STREAM_VOCIE_CALL：电话声音 为什么要分这么多呢？以前在台式机上开发的时候很少知道有这么多的声音类型，不过仔细思考下，发现这样做是有道理的。例如你在听music的时候接到电话，这个时候music播放肯定会停止，此时你只能听到电话，如果你调节音量的话，这个调节肯定只对电话起作用。当电话打完了，再回到music，你肯定不用再调节音量了。 其实系统将这几种声音的数据分开管理，所以，这个参数对AudioTrack来说，它的含义就是告诉系统，我现在想使用的是哪种类型的声音，这样系统就可以对应管理他们了。 AudioRecordAudioRecord说明The AudioRecord class manages the audio resources for Java applications to record audio from the audio input hardware of the platform. This is achieved by “pulling” (reading) the data from the AudioRecord object. The application is responsible for polling the AudioRecord object in time using one of the following three methods: read(byte[], int, int), read(short[], int, int) or read(ByteBuffer, int). The choice of which method to use will be based on the audio data storage format that is the most convenient for the user of AudioRecord. Upon creation, an AudioRecord object initializes its associated audio buffer that it will fill with the new audio data. The size of this buffer, specified during the construction, determines how long an AudioRecord can record before “over-running” data that has not been read yet. Data should be read from the audio hardware in chunks of sizes inferior to the total recording buffer size. audiosource类型定义在MediaRecorder中 1234567891011121314151617181920212223242526272829/** Default audio source **/ public static final int DEFAULT = 0; /** Microphone audio source */ public static final int MIC = 1; /** Voice call uplink (Tx) audio source */ public static final int VOICE_UPLINK = 2; /** Voice call downlink (Rx) audio source */ public static final int VOICE_DOWNLINK = 3; /** Voice call uplink + downlink audio source */ public static final int VOICE_CALL = 4; /** Microphone audio source with same orientation as camera if available, the main * device microphone otherwise */ public static final int CAMCORDER = 5; /** Microphone audio source tuned for voice recognition if available, behaves like * &#123;@link #DEFAULT&#125; otherwise. */ public static final int VOICE_RECOGNITION = 6; /** Microphone audio source tuned for voice communications such as VoIP. It * will for instance take advantage of echo cancellation or automatic gain control * if available. It otherwise behaves like &#123;@link #DEFAULT&#125; if no voice processing * is applied. */ public static final int VOICE_COMMUNICATION = 7; AudioManager获取系统音量代码 1234567891011121314151617181920212223242526272829//初始化AudioManager:AudioManager mAudioManager = (AudioManager) getSystemService(Context.AUDIO_SERVICE);//通话音量int max = mAudioManager.getStreamMaxVolume( AudioManager.STREAM_VOICE_CALL );int current = mAudioManager.getStreamVolume( AudioManager.STREAM_VOICE_CALL );Log.d(“VIOCE_CALL”, “max : ” + max + ” current : ” + current);//系统音量max = mAudioManager.getStreamMaxVolume( AudioManager.STREAM_SYSTEM );current = mAudioManager.getStreamVolume( AudioManager.STREAM_SYSTEM );Log.d(“SYSTEM”, “max : ” + max + ” current : ” + current);//铃声音量max = mAudioManager.getStreamMaxVolume( AudioManager.STREAM_RING );current = mAudioManager.getStreamVolume( AudioManager.STREAM_RING );Log.d(“RING”, “max : ” + max + ” current : ” + current);//音乐音量max = mAudioManager.getStreamMaxVolume( AudioManager.STREAM_MUSIC );current = mAudioManager.getStreamVolume( AudioManager.STREAM_MUSIC );Log.d(“MUSIC”, “max : ” + max + ” current : ” + current);//提示声音音量max = mAudioManager.getStreamMaxVolume( AudioManager.STREAM_ALARM );current = mAudioManager.getStreamVolume( AudioManager.STREAM_ALARM );Log.d(“ALARM”, “max : ” + max + ” current : ” + current); ps： 游戏过程中只允许调整多媒体音量，而不允许调整通话音量。 1setVolumeControlStream(AudioManager.STREAM_MUSIC); 控制音量AudioManager提供了设置音量的方法： 1public void setStreamVolume(intstreamType,intindex,intflags) 其中streamType有内置的常量，去文档里面就可以看到。 使用示例: 12345678910111213141516171819202122232425//音量控制,初始化定义 AudioManager mAudioManager = (AudioManager) getSystemService(Context.AUDIO_SERVICE); //最大音量 int maxVolume = mAudioManager.getStreamMaxVolume(AudioManager.STREAM_MUSIC); //当前音量 int currentVolume = mAudioManager.getStreamVolume(AudioManager.STREAM_MUSIC);//直接控制音量if(isSilent)&#123; mAudioManager.setStreamVolume(AudioManager.STREAM_MUSIC, 0, 0); &#125;else&#123; mAudioManager.setStreamVolume(AudioManager.STREAM_MUSIC, tempVolume, 0); //tempVolume:音量绝对值 &#125; //以一步步长控制音量的增减，并弹出系统默认音量控制条：//降低音量，调出系统音量控制 if(flag == 0)&#123; mAudioManager.adjustStreamVolume(AudioManager.STREAM_MUSIC,AudioManager.ADJUST_LOWER, AudioManager.FX_FOCUS_NAVIGATION_UP); &#125; //增加音量，调出系统音量控制 else if(flag == 1)&#123; mAudioManager.adjustStreamVolume(AudioManager.STREAM_MUSIC,AudioManager.ADJUST_RAISE, AudioManager.FX_FOCUS_NAVIGATION_UP); &#125; 监听按键手动控制音量 123456789101112131415161718192021AudioManager audio = (AudioManager) getSystemService(Service.AUDIO_SERVICE);@Overridepublic boolean onKeyDown(int keyCode, KeyEvent event) &#123; switch (keyCode) &#123; case KeyEvent.KEYCODE_VOLUME_UP: audio.adjustStreamVolume( AudioManager.STREAM_MUSIC, AudioManager.ADJUST_RAISE, AudioManager.FLAG_PLAY_SOUND | AudioManager.FLAG_SHOW_UI); return true; case KeyEvent.KEYCODE_VOLUME_DOWN: audio.adjustStreamVolume( AudioManager.STREAM_MUSIC, AudioManager.ADJUST_LOWER, AudioManager.FLAG_PLAY_SOUND | AudioManager.FLAG_SHOW_UI); return true; default: break; &#125; return super.onKeyDown(keyCode, event); 插入耳机状态仍使用扬声器外放音乐插入耳机的时候也可以选择使用扬声器播放音乐，来电铃声就是这么用的。但是只能用MediaPlayer，播放音频文件。 使用AudioTrack.write播放是行不通的(有待验证)。按理说AudioRecord、AudioTrack类相对于MediaRecorder mediaPlayer来说，更加接近底层，应该也行得通的。 插入耳机，选择外放的代码如下(兼容性验证)： 123456789AudioManager audioManager = (AudioManager) this.getSystemService(Context.AUDIO_SERVICE); audioManager.setMicrophoneMute(false); audioManager.setSpeakerphoneOn(true);//使用扬声器外放，即使已经插入耳机 setVolumeControlStream(AudioManager.STREAM_MUSIC);//控制声音的大小 audioManager.setMode(AudioManager.STREAM_MUSIC); //播放一段声音，查看效果 MediaPlayer playerSound = MediaPlayer.create(this, Uri.parse(&quot;file:///system/media/audio/ui/camera_click.ogg&quot;)); playerSound.start(); 使用STREAM_VOCIE_CALL播放声音与耳机冲突使用STREAM_VOCIE_CALL播放声音在某些手机,比如魅蓝等上面会导致声音仍外放,耳机没声音现象. WebRtc使用STREAM_VOCIE_CALL播放声音,导致某些手机声音低,没法使用音量键调节,插入耳机声音仍外放等问题.]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>Audio</tag>
        <tag>issue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MarkDown中嵌入LaTex]]></title>
    <url>%2F2017%2F04%2F16%2Flatex%2F</url>
    <content type="text"><![CDATA[MarkDown中使用标识符$$和$$$$即可引入LaTeX语法,前者使用时不换行,即在所使用位置使用LaTeX的格式,后者会换行后居中 部分希腊字母 命令 显示 命令 显示 \alpha α A A \beta β B B \gamma γ \Gamma \varGamma Γ Γ delta δ \Delta \varDelta Δ Δ \epsilon ϵ E E \eta η H H \theta θ \Theta \varTheta Θ Θ \kappa κ K K \lambda λ \Lambda \varLambda Λ Λ \mu μ M M \nu ν N N \pi π \Pi \varPi Π Π \rho ρ P P \sigma σ \Sigma \varSigma Σ Σ \tau τ T T \phi \varphi ϕ φ \Phi \varPhi Φ Φ \omega ω \Omega \varOmega Ω Ω 全部24个字母: 名称 大写 Tex 小写 Tex alpha A A α \alpha beta B B β\ beta gamma Γ \Gamma γ \gamma delta Δ \Delta δ \delta epsilon E E ϵ \epsilon zeta Z Z ζ \zeta eta H H η \eta theta Θ \Theta θ \theta iota I I ι \iota kappa K K κ \kappa lambda Λ \Lambda λ \lambda mu M M μ \mu nu N N ν \nu xi Ξ \Xi ξ \xi omicron O O ο \omicron pi Π \Pi π \pi rho P P ρ \rho sigma Σ \Sigma σ \sigma tau T T τ \tau upsilon Υ \Upsilon υ \upsilon phi Φ \Phi ϕ \phi chi X X χ \chi psi Ψ \Psi ψ \psi omega Ω \Omega ω \omega 部分运算符 命令 显示 命令 显示 \pm ± \mp ∓ \times × \div ÷ \circ ∘ \bullet ∙ \cdot ⋅ \cup ∪ \cap ∩ \subset ⊂ \supset ⊃ \subseteq ⊆ \supseteq ⊇ \leq ≤ \geq ≥ \propto ∝ 其他符号 命令 显示 命令 显示 \cdotp ⋅ \cdots ⋯ \ddots ⋱ \infty ∞ \partial ∂ \bot ⊥ \hat{a} â \tilde{a} ã \bar{a} a¯ \vec{a} a⃗ \dot{a} a˙ \sqrt{a} a‾‾√ \sqrt[3]{2} a‾‾√3 a^{3} a3 \frac{1}{a} 1a \lim_{x \to 0} lima→0 集合关系符号 说明 命令 集合的大括号 { … }\ 集合中的竖线$\mid$ \mid 属于 \in 不属于 \not\in A包含于B A\subset B A真包含于B A\subsetneqq B A包含B A\supset B A真包含B A\supsetneqq B A不包含于B A\not\subset B A交B A\cap B A并B A\cup B A的闭包 \overline{A} A减去B A\setminus B 实数集合 \mathbb{R} 空集 \emptyset 表格中竖线用&amp;#124; 括号总结 功能 语法 显示 不好看 ( \frac{1}{2} ) $(\frac{1}{2})$ 好一点 \left( \frac{1}{2} \right) $\left ( \frac{1}{2} \right )$ 可以使用\left和\right来显示不同的括号： 功能 语法 显示 圆括号，小括号 \left( \frac{a}{b} \right) $\left( \frac{a}{b} \right)$ 方括号，中括号 \left[ \frac{a}{b} \right] $\left[ \frac{a}{b} \right]$ 花括号，大括号 \left\{ \frac{a}{b} \right\} $ \left{ \frac{a}{b} \right}$ 角括号 \left \langle \frac{a}{b} \right \rangle $\left\langle \frac{a}{b} \right \rangle$ 单竖线，绝对值 \left 竖线 \frac{a}{b} \right 竖线 双竖线，范 \left \ 竖线 \frac{a}{b} \right \ 竖线 取整函数 （Floor function） \left \lfloor \frac{a}{b} \right \rfloor $ \left \lfloor \frac{a}{b} \right \rfloor$ 取顶函数 （Ceiling function) \left \lceil \frac{c}{d} \right \rceil $ \left \lceil \frac{c}{d} \right \rceil$ 斜线与反斜线 \left / \frac{a}{b} \right \backslash $ \left / \frac{a}{b} \right \backslash$ 上下箭头 \left \uparrow \frac{a}{b} \right \downarrow \left \Uparrow \frac{a}{b} \right \Downarrow \left \updownarrow \frac{a}{b} \right \Updownarrow $\left \uparrow \frac{a}{b} \right \downarrow$ $\left \Uparrow \frac{a}{b} \right \Downarrow$ $\left \updownarrow \frac{a}{b} \right \Updownarrow$ 混合括号 \left [ 0,1 \right ) \left \langle \psi ) $\left [ 0,1 \right )$ $ \left \langle \psi \right)$ 单左括号 \left \{ \frac{a}{b} \right . $\left { \frac{a}{b} \right .$ 单右括号 \left . \frac{a}{b} \right \} $\left . \frac{a}{b} \right }$ 备注： 可以使用\big, \Big, \bigg, \Bigg控制括号的大小，比如代码\Bigg ( \bigg [ \Big \{ \big \langle \left | \| \frac{a}{b} \| \right | \big \rangle \Big \} \bigg ] \Bigg )显示 $$\Bigg ( \bigg [ \Big { \big \langle \left | | x | \right | \big \rangle \Big } \bigg ] \Bigg )$$ 矩阵基本用法使用$$\begin{matrix}…\end{matrix}$$这样的形式来表示矩阵，在\begin与\end之间加入矩阵中的元素即可。矩阵的行之间使用\\分隔，列之间使用&amp;分隔。 1234567$$ \begin&#123;matrix&#125; 1 &amp; x &amp; x^2 \\ 1 &amp; y &amp; y^2 \\ 1 &amp; z &amp; z^2 \\ \end&#123;matrix&#125;$$ $$ \begin{matrix} 1 &amp; x &amp; x^2 \ 1 &amp; y &amp; y^2 \ 1 &amp; z &amp; z^2 \ \end{matrix} $$ 加括号如果要对矩阵加括号，可以像上文中提到的一样，使用\left与\right配合表示括号符号。也可以使用特殊的matrix。即替换\begin{matrix}…\end{matrix}中的matrix为pmatrix，bmatrix，Bmatrix，vmatrix,Vmatrix. 省略元素可以使用\cdots ⋯ \ddots ⋱ \vdots ⋮ 来省略矩阵中的元素 增广矩阵增广矩阵需要使用前面的array来实现 1234567$$ \left[ \begin&#123;array&#125;&#123;cc|c&#125; 1&amp;2&amp;3\\ 4&amp;5&amp;6 \end&#123;array&#125; \right]$$ $$ \left[ \begin{array}{cc|c} 1&amp;2&amp;3\ 4&amp;5&amp;6 \end{array} \right] $$ 表格使用$$\begin{array}{列样式}…\end{array}$$这样的形式来创建表格，列样式可以是clr表示居中，左，右对齐，还可以使用|表示一条竖线。表格中 各行使用\\分隔，各列使用&amp;分隔。使用\hline在本行前加入一条直线。 例如， 123456789$$\begin&#123;array&#125;&#123;c|lcr&#125;n &amp; \text&#123;Left&#125; &amp; \text&#123;Center&#125; &amp; \text&#123;Right&#125; \\\hline1 &amp; 0.24 &amp; 1 &amp; 125 \\2 &amp; -1 &amp; 189 &amp; -8 \\3 &amp; -20 &amp; 2000 &amp; 1+10i \\\end&#123;array&#125;$$ 上标与下标上标和下标分别使用^与_，例如x_i^2：$x_i^2$ 。默认情况下，上下标符号仅仅对下一个组起作用。一个组即单个字符或者使用{..}包裹起来的内容。也就是说，如果使用10^10，会得到$10^10$ ，而10^{10}才是$10^{10}$ 。同时，大括号还能消除二义性，如x^5^6将得到一个错误，必须使用大括号来界定^的结合性，如{x^5}^6：${x^5}^6$ 或者 x^{5^6}：$x^{5^6}$ 。 对齐的公式分类表达式定义函数的时候经常需要分情况给出表达式，可使用\begin{cases}…\end{cases}。其中，使用\来分类，使用&amp;指示需要对齐的位置。如： 多重积分连分数方程组颜色公式标记与引用求和与积分分式与根式特殊函数与符号空间顶部符号 参考 http://mlworks.cn/posts/introduction-to-mathjax-and-latex-expression/]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高数1.函数与极限]]></title>
    <url>%2F2017%2F04%2F13%2Fam-function-and-limitaion%2F</url>
    <content type="text"><![CDATA[1.映射与函数1.1 集合1.1.1 集合的概念集合(集)是指具有某种特定性质的事物的总体,组成这个集合的事物成为该集合的元素(简称元) 表示:用大写拉丁字母A,B,C…表示集合,小写拉丁字母表示集合的元素 分类: 有限集 无限集 表示数集的字母的右上角标*表示该数集内排除0的集,标上+来表示数集内排除0和负数的集 常用表示 N={0, 1, 2, 3…};全体非负整数即自然数的集合 N+={1,2,3,…n,….};全体正整数的集合 Z={…,-n,…-3, -2,-1, 0, 1, 2, 3,…,n…};全体整数的集合 $Q=\lbrace \frac{p}{q}|p \in Z,q \in N^{+} \rbrace$;全体有理数集 全体实数记做R,R*为排除0的实数集,R+为全体正实数集. 子集概念: 子集 真子集 集合相等:互为子集 空集 $\emptyset$1.1.2 集合的运算 1.1.3 区间和领域]]></content>
      <categories>
        <category>高数</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android线程使用总结]]></title>
    <url>%2F2017%2F04%2F10%2Ftips-android-thread%2F</url>
    <content type="text"><![CDATA[1. Threading Performance在程序开发的实践当中，为了让程序表现得更加流畅，我们肯定会需要使用到多线程来提升程序的并发执行性能。但是编写多线程并发的代码一直以来都是一个相对棘手的问题，所以想要获得更佳的程序性能，我们非常有必要掌握多线程并发编程的基础技能。 众所周知，Android 程序的大多数代码操作都必须执行在主线程，例如系统事件(例如设备屏幕发生旋转)，输入事件(例如用户点击滑动等)，程序回调服务，UI 绘制以及闹钟事件等等。那么我们在上述事件或者方法中插入的代码也将执行在主线程。 一旦我们在主线程里面添加了操作复杂的代码，这些代码就很可能阻碍主线程去响应点击/滑动事件，阻碍主线程的 UI 绘制等等。我们知道，为了让屏幕的刷新帧率达到 60fps，我们需要确保 16ms 内完成单次刷新的操作。一旦我们在主线程里面执行的任务过于繁重就可能导致接收到刷新信号的时候因为资源被占用而无法完成这次刷新操作，这样就会产生掉帧的现象，刷新帧率自然也就跟着下降了(一旦刷新帧率降到 20fps 左右，用户就可以明显感知到卡顿不流畅了)。 为了避免上面提到的掉帧问题，我们需要使用多线程的技术方案，把那些操作复杂的任务移动到其他线程当中执行，这样就不容易阻塞主线程的操作，也就减小了出现掉帧的可能性。 为主线程减轻负的多线程方案有哪些呢？这些方案分别适合在什么场景下使用？Android 系统为我们提供了若干组工具类来帮助解决这个问题。 AsyncTask: 为 UI 线程与工作线程之间进行快速的切换提供一种简单便捷的机制。适用于当下立即需要启动，但是异步执行的生命周期短暂的使用场景。 HandlerThread: 为某些回调方法或者等待某些任务的执行设置一个专属的线程，并提供线程任务的调度机制。 ThreadPool: 把任务分解成不同的单元，分发到各个不同的线程上，进行同时并发处理。 IntentService: 适合于执行由 UI 触发的后台 Service 任务，并可以把后台任务执行的情况通过一定的机制反馈给 UI。 了解这些系统提供的多线程工具类分别适合在什么场景下，可以帮助我们选择合适的解决方案，避免出现不可预期的麻烦。虽然使用多线程可以提高程序的并发量，但是我们需要特别注意因为引入多线程而可能伴随而来的内存问题。举个例子，在 Activity 内部定义的一个 AsyncTask，它属于一个内部类，该类本身和外面的 Activity 是有引用关系的，如果 Activity 要销毁的时候，AsyncTask 还仍然在运行，这会导致 Activity 没有办法完全释放，从而引发内存泄漏。所以说，多线程是提升程序性能的有效手段之一，但是使用多线程却需要十分谨慎小心，如果不了解背后的执行机制以及使用的注意事项，很可能引起严重的问题。 2. Understanding Android Threading通常来说，一个线程需要经历三个生命阶段：开始，执行，结束。线程会在任务执行完毕之后结束，那么为了确保线程的存活，我们会在执行阶段给线程赋予不同的任务，然后在里面添加退出的条件从而确保任务能够执行完毕后退出。 在很多时候，线程不仅仅是线性执行一系列的任务就结束那么简单的，我们会需要增加一个任务队列，让线程不断的从任务队列中获取任务去进行执行，另外我们还可能在线程执行的任务过程中与其他的线程进行协作。如果这些细节都交给我们自己来处理，这将会是件极其繁琐又容易出错的事情。 所幸的是，Android 系统为我们提供了 Looper，Handler，MessageQueue 来帮助实现上面的线程任务模型： Looper: 能够确保线程持续存活并且可以不断的从任务队列中获取任务并进行执行。 Handler: 能够帮助实现队列任务的管理，不仅仅能够把任务插入到队列的头部，尾部，还可以按照一定的时间延迟来确保任务从队列中能够来得及被取消掉。 MessageQueue: 使用 Intent，Message，Runnable 作为任务的载体在不同的线程之间进行传递。 把上面三个组件打包到一起进行协作，这就是 HandlerThread 我们知道，当程序被启动，系统会帮忙创建进程以及相应的主线程，而这个主线程其实就是一个 HandlerThread。这个主线程会需要处理系统事件，输入事件，系统回调的任务，UI绘制等等任务，为了避免主线程任务过重，我们就会需要不断的开启新的工作线程来处理那些子任务。 3. Memory &amp; Threading增加并发的线程数会导致内存消耗的增加，平衡好这两者的关系是非常重要的。我们知道，多线程并发访问同一块内存区域有可能带来很多问题，例如读写的权限争夺问题，ABA 问题等等。为了解决这些问题，我们会需要引入锁的概念。 在 Android 系统中也无法避免因为多线程的引入而导致出现诸如上文提到的种种问题。Android UI 对象的创建，更新，销毁等等操作都默认是执行在主线程，但是如果我们在非主线程对UI对象进行操作，程序将可能出现异常甚至是崩溃。 另外，在非 UI 线程中直接持有 UI 对象的引用也很可能出现问题。例如Work线程中持有某个 UI 对象的引用，在 Work 线程执行完毕之前，UI 对象在主线程中被从 ViewHierarchy 中移除了，这个时候 UI 对象的任何属性都已经不再可用了，另外对这个 UI 对象的更新操作也都没有任何意义了，因为它已经从 ViewHierarchy 中被移除，不再绘制到画面上了。 不仅如此，View 对象本身对所属的 Activity 是有引用关系的，如果工作线程持续保有 View 的引用，这就可能导致 Activity 无法完全释放。除了直接显式的引用关系可能导致内存泄露之外，我们还需要特别留意隐式的引用关系也可能导致泄露。例如通常我们会看到在 Activity 里面定义的一个 AsyncTask，这种类型的 AsyncTask 与外部的 Activity 是存在隐式引用关系的，只要 Task 没有结束，引用关系就会一直存在，这很容易导致 Activity 的泄漏。更糟糕的情况是，它不仅仅发生了内存泄漏，还可能导致程序异常或者崩溃。 为了解决上面的问题，我们需要谨记的原则就是：不要在任何非 UI 线程里面去持有 UI 对象的引用。系统为了确保所有的 UI 对象都只会被 UI 线程所进行创建，更新，销毁的操作，特地设计了对应的工作机制(当 Activity 被销毁的时候，由该 Activity 所触发的非 UI 线程都将无法对UI对象进行操作，否者就会抛出程序执行异常的错误)来防止 UI 对象被错误的使用。 4. Good AsyncTask HuntingAsyncTask 是一个让人既爱又恨的组件，它提供了一种简便的异步处理机制，但是它又同时引入了一些令人厌恶的麻烦。一旦对 AsyncTask 使用不当，很可能对程序的性能带来负面影响，同时还可能导致内存泄露。 举个例子，常遇到的一个典型的使用场景：用户切换到某个界面，触发了界面上的图片的加载操作，因为图片的加载相对来说耗时比较长，我们需要在子线程中处理图片的加载，当图片在子线程中处理完成之后，再把处理好的图片返回给主线程，交给 UI 更新到画面上。 AsyncTask 的出现就是为了快速的实现上面的使用场景，AsyncTask 把在主线程里面的准备工作放到 onPreExecute()方法里面进行执行，doInBackground()方法执行在工作线程中，用来处理那些繁重的任务，一旦任务执行完毕，就会调用 onPostExecute()方法返回到主线程。 使用 AsyncTask 需要注意的问题有哪些呢？请关注以下几点： 首先，默认情况下，所有的 AsyncTask 任务都是被线性调度执行的，他们处在同一个任务队列当中，按顺序逐个执行。假设你按照顺序启动20个 AsyncTask，一旦其中的某个 AsyncTask 执行时间过长，队列中的其他剩余 AsyncTask 都处于阻塞状态，必须等到该任务执行完毕之后才能够有机会执行下一个任务。 为了解决上面提到的线性队列等待的问题，我们可以使用 AsyncTask.executeOnExecutor()强制指定 AsyncTask 使用线程池并发调度任务。 其次，如何才能够真正的取消一个 AsyncTask 的执行呢？我们知道 AsyncTaks 有提供 cancel()的方法，但是这个方法实际上做了什么事情呢？线程本身并不具备中止正在执行的代码的能力，为了能够让一个线程更早的被销毁，我们需要在 doInBackground()的代码中不断的添加程序是否被中止的判断逻辑. 一旦任务被成功中止，AsyncTask 就不会继续调用 onPostExecute()，而是通过调用 onCancelled()的回调方法反馈任务执行取消的结果。我们可以根据任务回调到哪个方法（是 onPostExecute 还是 onCancelled）来决定是对 UI 进行正常的更新还是把对应的任务所占用的内存进行销毁等。 最后，使用 AsyncTask 很容易导致内存泄漏，一旦把 AsyncTask 写成 Activity 的内部类的形式就很容易因为 AsyncTask 生命周期的不确定而导致 Activity 发生泄漏。 综上所述，AsyncTask 虽然提供了一种简单便捷的异步机制，但是我们还是很有必要特别关注到他的缺点，避免出现因为使用错误而导致的严重系统性能问题。 5. Getting a HandlerThread大多数情况下，AsyncTask 都能够满足多线程并发的场景需要（在工作线程执行任务并返回结果到主线程），但是它并不是万能的。例如打开相机之后的预览帧数据是通过 onPreviewFrame()的方法进行回调的，onPreviewFrame()和 open()相机的方法是执行在同一个线程的。 如果这个回调方法执行在 UI 线程，那么在 onPreviewFrame()里面将要执行的数据转换操作将和主线程的界面绘制，事件传递等操作争抢系统资源，这就有可能影响到主界面的表现性能。 我们需要确保 onPreviewFrame()执行在工作线程。如果使用 AsyncTask，会因为 AsyncTask 默认的线性执行的特性(即使换成并发执行)会导致因为无法把任务及时传递给工作线程而导致任务在主线程中被延迟，直到工作线程空闲，才可以把任务切换到工作线程中进行执行。 所以我们需要的是一个执行在工作线程，同时又能够处理队列中的复杂任务的功能，而 HandlerThread 的出现就是为了实现这个功能的，它组合了 Handler，MessageQueue，Looper 实现了一个长时间运行的线程，不断的从队列中获取任务进行执行的功能。 回到刚才的处理相机回调数据的例子，使用 HandlerThread 我们可以把 open()操作与 onPreviewFrame()的操作执行在同一个线程，同时还避免了 AsyncTask 的弊端。如果需要在 onPreviewFrame()里面更新 UI，只需要调用 runOnUiThread()方法把任务回调给主线程就够了。 HandlerThread 比较合适处理那些在工作线程执行，需要花费时间偏长的任务。我们只需要把任务发送给 HandlerThread，然后就只需要等待任务执行结束的时候通知返回到主线程就好了。 另外很重要的一点是，一旦我们使用了 HandlerThread，需要特别注意给 HandlerThread 设置不同的线程优先级，CPU 会根据设置的不同线程优先级对所有的线程进行调度优化。 掌握 HandlerThread 与 AsyncTask 之间的优缺点，可以帮助我们选择合适的方案。 6. Swimming in Threadpools线程池适合用在把任务进行分解，并发进行执行的场景。通常来说，系统里面会针对不同的任务设置一个单独的守护线程用来专门处理这项任务。例如使用 Networking Thread 用来专门处理网络请求的操作，使用 IO Thread 用来专门处理系统的 I\O 操作。针对那些场景，这样设计是没有问题的，因为对应的任务单次执行的时间并不长而且可以是顺序执行的。但是这种专属的单线程并不能满足所有的情况，例如我们需要一次性 decode 40张图片，每个线程需要执行 4ms 的时间，如果我们使用专属单线程的方案，所有图片执行完毕会需要花费 160ms(40*4)，但是如果我们创建10个线程，每个线程执行4个任务，那么我们就只需要16ms就能够把所有的图片处理完毕。 为了能够实现上面的线程池模型，系统为我们提供了 ThreadPoolExecutor 帮助类来简化实现，剩下需要做的就只是对任务进行分解就好了。 使用线程池需要特别注意同时并发线程数量的控制，理论上来说，我们可以设置任意你想要的并发数量，但是这样做非常的不好。因为 CPU 只能同时执行固定数量的线程数，一旦同时并发的线程数量超过 CPU 能够同时执行的阈值，CPU 就需要花费精力来判断到底哪些线程的优先级比较高，需要在不同的线程之间进行调度切换。 一旦同时并发的线程数量达到一定的量级，这个时候 CPU 在不同线程之间进行调度的时间就可能过长，反而导致性能严重下降。另外需要关注的一点是，每开一个新的线程，都会耗费至少 64K+ 的内存。为了能够方便的对线程数量进行控制，ThreadPoolExecutor 为我们提供了初始化的并发线程数量，以及最大的并发数量进行设置。 另外需要关注的一个问题是：Runtime.getRuntime().availableProcesser()方法并不可靠，他返回的值并不是真实的 CPU 核心数，因为 CPU 会在某些情况下选择对部分核心进行睡眠处理，在这种情况下，返回的数量就只能是激活的 CPU 核心数。 7. The Zen of IntentService默认的 Service 是执行在主线程的，可是通常情况下，这很容易影响到程序的绘制性能(抢占了主线程的资源)。除了前面介绍过的 AsyncTask 与 HandlerThread，我们还可以选择使用 IntentService 来实现异步操作。IntentService 继承自普通 Service 同时又在内部创建了一个 HandlerThread，在 onHandlerIntent()的回调里面处理扔到 IntentService 的任务。所以 IntentService 就不仅仅具备了异步线程的特性，还同时保留了 Service 不受主页面生命周期影响的特点。 如此一来，我们可以在 IntentService 里面通过设置闹钟间隔性的触发异步任务，例如刷新数据，更新缓存的图片或者是分析用户操作行为等等，当然处理这些任务需要小心谨慎。 使用 IntentService 需要特别留意以下几点： 首先，因为 IntentService 内置的是 HandlerThread 作为异步线程，所以每一个交给 IntentService 的任务都将以队列的方式逐个被执行到，一旦队列中有某个任务执行时间过长，那么就会导致后续的任务都会被延迟处理。 其次，通常使用到 IntentService 的时候，我们会结合使用 BroadcastReceiver 把工作线程的任务执行结果返回给主 UI 线程。使用广播容易引起性能问题，我们可以使用 LocalBroadcastManager 来发送只在程序内部传递的广播，从而提升广播的性能。我们也可以使用 runOnUiThread() 快速回调到主 UI 线程。 最后，包含正在运行的 IntentService 的程序相比起纯粹的后台程序更不容易被系统杀死，该程序的优先级是介于前台程序与纯后台程序之间的。 8. Threading and Loaders当启动工作线程的 Activity 被销毁的时候，我们应该做点什么呢？为了方便的控制工作线程的启动与结束，Android 为我们引入了 Loader 来解决这个问题。我们知道 Activity 有可能因为用户的主动切换而频繁的被创建与销毁，也有可能是因为类似屏幕发生旋转等被动原因而销毁再重建。在 Activity 不停的创建与销毁的过程当中，很有可能因为工作线程持有 Activity 的 View 而导致内存泄漏(因为工作线程很可能持有 View 的强引用，另外工作线程的生命周期还无法保证和 Activity 的生命周期一致，这样就容易发生内存泄漏了)。除了可能引起内存泄漏之外，在 Activity 被销毁之后，工作线程还继续更新视图是没有意义的，因为此时视图已经不在界面上显示了。 Loader 的出现就是为了确保工作线程能够和 Activity 的生命周期保持一致，同时避免出现前面提到的问题。 LoaderManager 会对查询的操作进行缓存，只要对应 Cursor 上的数据源没有发生变化，在配置信息发生改变的时候(例如屏幕的旋转)，Loader 可以直接把缓存的数据回调到 onLoadFinished()，从而避免重新查询数据。另外系统会在 Loader 不再需要使用到的时候(例如使用 Back 按钮退出当前页面)回调 onLoaderReset()方法，我们可以在这里做数据的清除等等操作。 在 Activity 或者 Fragment 中使用 Loader 可以方便的实现异步加载的框架，Loader 有诸多优点。但是实现 Loader 的这套代码还是稍微有点点复杂，Android 官方为我们提供了使用 Loader 的示例代码进行参考学习。 9. The Importance of Thread Priority理论上来说，我们的程序可以创建出非常多的子线程一起并发执行的，可是基于 CPU 时间片轮转调度的机制，不可能所有的线程都可以同时被调度执行，CPU 需要根据线程的优先级赋予不同的时间片。 Android 系统会根据当前运行的可见的程序和不可见的后台程序对线程进行归类，划分为 forground 的那部分线程会大致占用掉 CPU 的90%左右的时间片，background 的那部分线程就总共只能分享到5%-10%左右的时间片。之所以设计成这样是因为 forground 的程序本身的优先级就更高，理应得到更多的执行时间。 默认情况下，新创建的线程的优先级默认和创建它的母线程保持一致。如果主 UI 线程创建出了几十个工作线程，这些工作线程的优先级就默认和主线程保持一致了，为了不让新创建的工作线程和主线程抢占 CPU 资源，需要把这些线程的优先级进行降低处理，这样才能给帮组 CPU 识别主次，提高主线程所能得到的系统资源。 在 Android 系统里面，我们可以通过 android.os.Process.setThreadPriority(int) 设置线程的优先级，参数范围从-20到19，数值越小优先级越高。Android 系统还为我们提供了以下的一些预设值，我们可以通过给不同的工作线程设置不同数值的优先级来达到更细粒度的控制。 大多数情况下，新创建的线程优先级会被设置为默认的0，主线程设置为0的时候，新创建的线程还可以利用 THREAD_PRIORITY_LESS_FAVORABLE 或者 THREAD_PRIORITY_MORE_FAVORABLE 来控制线程的优先级。 Android 系统里面的 AsyncTask 与 IntentService已经默认帮助我们设置线程的优先级，但是对于那些非官方提供的多线程工具类，我们需要特别留意根据需要自己手动来设置线程的优先级。 10. Profile GPU Rendering : M Update从 Android M 系统开始，系统更新了 GPU Profiling 的工具来帮助我们定位 UI 的渲染性能问题。早期的 CPU Profiling 工具只能粗略的显示出 Process，Execute，Update 三大步骤的时间耗费情况。 但是仅仅显示三大步骤的时间耗费情况，还是不太能够清晰帮助我们定位具体的程序代码问题，所以在 Android M 版本开始，GPU Profiling 工具把渲染操作拆解成如下8个详细的步骤进行显示。 旧版本中提到的 Proces，Execute，Update 还是继续得到了保留，他们的对应关系如下： 接下去我们看下其他五个步骤分别代表了什么含义： Sync &amp; Upload：通常表示的是准备当前界面上有待绘制的图片所耗费的时间，为了减少该段区域的执行时间，我们可以减少屏幕上的图片数量或者是缩小图片本身的大小。 Measure &amp; Layout：这里表示的是布局的 onMeasure 与 onLayout 所花费的时间，一旦时间过长，就需要仔细检查自己的布局是不是存在严重的性能问题。 Animation：表示的是计算执行动画所需要花费的时间，包含的动画有 ObjectAnimator，ViewPropertyAnimator，Transition 等等。一旦这里的执行时间过长，就需要检查是不是使用了非官方的动画工具或者是检查动画执行的过程中是不是触发了读写操作等等。 Input Handling：表示的是系统处理输入事件所耗费的时间，粗略等于对于的事件处理方法所执行的时间。一旦执行时间过长，意味着在处理用户的输入事件的地方执行了复杂的操作。 Misc/Vsync Delay：如果稍加注意，我们可以在开发应用的 Log 日志里面看到这样一行提示：I/Choreographer(691): Skipped XXX frames! The application may be doing too much work on its main thread。这意味着我们在主线程执行了太多的任务，导致 UI 渲染跟不上 vSync 的信号而出现掉帧的情况。 上面八种不同的颜色区分了不同的操作所耗费的时间，为了便于我们迅速找出那些有问题的步骤，GPU Profiling 工具会显示 16ms 的阈值线，这样就很容易找出那些不合理的性能问题，再仔细看对应具体哪个步骤相对来说耗费时间比例更大，结合上面介绍的细化步骤，从而快速定位问题，修复问题。 http://bugly.qq.com/bbs/forum.php?mod=viewthread&amp;tid=1022]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>tips</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MP4格式解析]]></title>
    <url>%2F2017%2F04%2F09%2Fm-f-mp4%2F</url>
    <content type="text"><![CDATA[目前MP4的概念被炒得很火，也很乱。最开始MP4指的是音频（MP3的升级版），即MPEG-2 AAC标准。随后MP4概念被转移到视频上，对应的是MPEG-4标准。而现在我们流行的叫法，多半是指能播放MPEG-4标准编码格式视频的播放器。但是这篇文章介绍的内容跟上面这些都无关，我们要讨论的是MP4文件封装格式，对应的标准为ISO/IEC 14496-12，即信息技术 视听对象编码的第12部分：ISO 基本媒体文件格式（Information technology Coding of audio-visual objects Part 12: ISO base media file format）。ISO/IEC组织指定的标准一般用数字表示，ISO/IEC 14496即MPEG-4标准。 MP4视频文件封装格式是基于QuickTime容器格式定义的，因此参考QuickTime的格式定义对理解MP4文件格式很有帮助。MP4文件格式是一个十分开放的容器，几乎可以用来描述所有的媒体结构，MP4文件中的媒体描述与媒体数据是分开的，并且媒体数据的组织也很自由，不一定要按照时间顺序排列，甚至媒体数据可以直接引用其他文件。同时，MP4也支持流媒体。MP4目前被广泛用于封装h.264视频和AAC音频，是高清视频的代表。MP4格式的官方文件后缀名是“.mp4”，还有其他的以mp4为基础进行的扩展或者是缩水版本的格式，包括：M4V, 3GP, F4V等。 1.概述MP4文件中的所有数据都装在box（QuickTime中为atom）中，也就是说MP4文件由若干个box组成，每个box有类型和长度，可以将box理解为一个数据对象块。box中可以包含另一个box，这种box称为container box。一个MP4文件首先会有且只有一个“ftyp”类型的box，作为MP4格式的标志并包含关于文件的一些信息；之后会有且只有一个“moov”类型的box（Movie Box），它是一种container box，子box包含了媒体的metadata信息；MP4文件的媒体数据包含在“mdat”类型的box（Midia Data Box）中，该类型的box也是container box，可以有多个，也可以没有（当媒体数据全部引用其他文件时），媒体数据的结构由metadata进行描述。 下面是一些概念： track 表示一些sample的集合，对于媒体数据来说，track表示一个视频或音频序列。 hint track 这个特殊的track并不包含媒体数据，而是包含了一些将其他数据track打包成流媒体的指示信息。 sample 对于非hint track来说，video sample即为一帧视频，或一组连续视频帧，audio sample即为一段连续的压缩音频，它们统称sample。对于hint track，sample定义一个或多个流媒体包的格式。 sample table 指明sampe时序和物理布局的表。 chunk 一个track的几个sample组成的单元。 不讨论涉及hint的内容，只关注包含媒体数据的本地MP4文件。下图为一个典型的MP4文件的结构树。 2.Boxbox中的字节序为网络字节序，也就是大端字节序（Big-Endian），简单的说，就是一个32位的4字节整数存储方式为高位字节在内存的低端。Box由header和body组成，其中header统一指明box的大小和类型，body根据类型有不同的意义和格式。 标准的box开头的4个字节（32位）为box size，该大小包括box header和box body整个box的大小，这样我们就可以在文件中定位各个box。如果size为1，则表示这个box的大小为large size，真正的size值要在largesize域上得到。（实际上只有“mdat”类型的box才有可能用到large size。）如果size为0，表示该box为文件的最后一个box，文件结尾即为该box结尾。（同样只存在于“mdat”类型的box中。）size后面紧跟的32位为box type，一般是4个字符，如“ftyp”、“moov”等，这些box type都是已经预定义好的，分别表示固定的意义。如果是“uuid”，表示该box为用户扩展类型。如果box type是未定义的，应该将其忽略。 3.File Type Box(ftyp)该box有且只有1个，并且只能被包含在文件层，而不能被其他box包含。该box应该被放在文件的最开始，指示该MP4文件应用的相关信息。 “ftyp” body依次包括1个32位的major brand（4个字符），1个32位的minor version（整数）和1个以32位（4个字符）为单位元素的数组compatible brands。这些都是用来指示文件应用级别的信息。该box的字节实例如下： 1200000000h: 00 00 00 18 66 74 79 70 6D 70 34 32 00 00 00 01 ; ....ftypmp42....00000010h: 6D 70 34 32 6D 70 34 31 00 00 5A EB 6D 6F 6F 76 ; mp42mp41..Zmoov 4.Movie Box(moov)该box包含了文件媒体的metadata信息，“moov”是一个container box，具体内容信息由子box诠释。同File Type Box一样，该box有且只有一个，且只被包含在文件层。一般情况下，“moov”会紧随“ftyp”出现。 一般情况下（限于篇幅，本文只讲解常见的MP4文件结构），“moov”中会包含1个“mvhd”和若干个“trak”。其中“mvhd”为header box，一般作为“moov”的第一个子box出现（对于其他container box来说，header box都应作为首个子box出现）。“trak”包含了一个track的相关信息，是一个container box。下图为部分“moov”的字节实例，其中红色部分为box header，绿色为“mvhd”，黄色为一部分“trak”。 4.1 Movie Header Box(mvhd)“mvhd”接口如下表: 字段 字节数 意义 box size 4 box大小 box type 4 box类型 version 1 box版本，0或1，一般为0。（以下字节数均按version=0） flags 3 creation time 4 创建时间（相对于UTC时间1904-01-01零点的秒数） modification time 4 修改时间 time scale 4 文件媒体在1秒时间内的刻度值，可以理解为1秒长度的时间单元数 duration 4 该track的时间长度，用duration和time scale值可以计算track时长，比如audio track的time scale = 8000, duration = 560128，时长为70.016，video track的time scale = 600, duration = 42000，时长为70 rate 4 推荐播放速率，高16位和低16位分别为小数点整数部分和小数部分，即[16.16] 格式，该值为1.0（0x00010000）表示正常前向播放 volume 2 与rate类似，[8.8] 格式，1.0（0x0100）表示最大音量 reserved 10 保留位 matrix 36 视频变换矩阵 pre-defined 24 next track id 4 下一个track使用的id号 4.2Track Box(trak)“trak”也是一个container box，其子box包含了该track的媒体数据引用和描述（hint track除外）。一个MP4文件中的媒体可以包含多个track，且至少有一个track，这些track之间彼此独立，有自己的时间和空间信息。“trak”必须包含一个“tkhd”和一个“mdia”，此外还有很多可选的box（略）。其中“tkhd”为track header box，“mdia”为media box，该box是一个包含一些track媒体数据信息box的container box。 box类型说明 ftypefile type,说明文件类型 moovmetadata container,存放媒体信息的地方 mvhdmovie header,文件的总体信息,如时长,创建时间等 mvhdmovie header,文件的总体信息,如时长,创建时间等 traktrack or stream container,存放视频/音频流的容器 tkhdtrack header,track的总体信息,如时长,宽高等 mediatrak media information container mdhdmedia header,定义TimeScale,trak需要通过TimeScale转换成真实时间 hdlrhandler,表明本trak类型,指明是video/audio/还是hint minfmedia information container,数据在子box中 stblsample table box,存放时间/偏移的映射关系表,数据在子box中 stsdsample descriptions stts(decoding)time-to-sample,”时戳-sample序号”的映射表 stscsample-to-chunk,sample和chunk的映射表,这里的算法比较巧妙 stszsample size,每个sample的大小 stz2sample size,另一种sample size的存储算法,更节省空间 stsssync sample table,可随机访问的sample列表(关键帧列表) stcochunk offset,每个chunk的偏移,sample的偏移可根据其他box推算出来 co6464-bit chunk offset mdatmedia data container,具体的媒体数据 Mdat Box引用 http://xhelmboyx.tripod.com/formats/mp4-layout.txt]]></content>
      <categories>
        <category>音视频封装</category>
      </categories>
      <tags>
        <tag>多媒体</tag>
        <tag>音视频</tag>
        <tag>format</tag>
      </tags>
  </entry>
</search>